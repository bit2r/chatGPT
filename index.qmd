---
title: "chatGPT"
subtitle: "ë“¤ì–´ê°€ë©°"
author:
  - name: ì´ê´‘ì¶˜
    url: https://www.linkedin.com/in/kwangchunlee/
    affiliation: í•œêµ­ R ì‚¬ìš©ìíšŒ
    affiliation-url: https://github.com/bit2r
title-block-banner: true
#title-block-banner: "#562457"
format:
  html:
    css: css/quarto.css
    theme: flatly
    code-fold: false
    toc: true
    toc-depth: 3
    toc-title: ëª©ì°¨
    number-sections: true
    highlight-style: github    
    self-contained: false
filters:
   - lightbox
lightbox: auto
knitr:
  opts_chunk: 
    message: false
    warning: false
    collapse: true
    comment: "#>" 
    R.options:
      knitr.graphics.auto_pdf: true
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
link-citations: yes
csl: apa-single-spaced.csl
---


![](images/tech_giant.png)

# ëª¨í˜• í¬ê¸°


:::{.panel-tabset}

## 2019ë…„ 

![[@sanh2019distilbert]](images/LLM_2019.png)

## 2021ë…„

![[Efficient Natural Language Processing](https://hanlab.mit.edu/projects/efficientnlp_old/)](images/LLM_2021.png)

## 2022ë…„ 

![[langcon 2023 by ì‹ ì •ê·œ](https://songys.github.io/2023Langcon/)(images/LLM_langcon2023.png)


:::


# ê±°ëŒ€ì–¸ì–´ëª¨í˜• ì„±ëŠ¥


![[@wei2022emergent]](images/LLM_few_shot.png)


# ìˆ˜í•™

[@lewkowycz2022solving]{.aside}

:::{.panel-tabset}

## ë¯¸ë„¤ë¥´ë°” LM

![](images/math_minerva.png)

## ì†ìœ¼ë¡œ í’€ê¸°

![](images/LLM_hand.jpg){width=50%}


## ì‹œê°í™”

```{r}
library(tidyverse)

given_line <- function(x)  10 + 4 * x
solve_line <- function(x) -10 + 4 *x

ggplot() +
  geom_point(aes(x = 5, y = 10), size = 3) +
  geom_function(fun = given_line, color = "blue", size = 1.5) +
  geom_function(fun = solve_line, color = "red", size = 1.5, alpha = 0.5) +
  theme_classic() +
  scale_x_continuous(limits = c(-7, 7), breaks = seq(-7, 7, 1)) +
  scale_y_continuous(limits = c(-20, 20), breaks = seq(-20, 20, 1)) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) 

```

## Sympy í•´ë²•

```{python}
from sympy import *

x, y, b = symbols('x y b')

given_eq = y - (4*x + 10)

parallel_eq = y - (4*x + b)

intercept_eq = parallel_eq.subs([(x, 5), (y, 10)])

solveset(Eq(intercept_eq, 0), b)

```

:::

# ì™œ ê±°ëŒ€ ì–¸ì–´ ëª¨í˜•ì¸ê°€?

question-answering tasks (open-domain closed-book variant), cloze and sentence-completion tasks, Winograd-style tasks, in-context reading comprehension tasks, common-sense reasoning tasks, SuperGLUE tasks, and natural language inference tasksê°€ í¬í•¨ëœ ì´ 29ê°œ ì‘ì—… ì¤‘ 28ê°œ ì˜ì—­ì—ì„œ PaLM 540Bê°€ ì´ì „ ê±°ëŒ€ ì–¸ì–´ëª¨í˜• GLaM, GPT-3, Megatron-Turing NLG, Gopher, Chinchilla, LaMDA ì„ ê°€ë³ê²Œ ëŠ¥ê°€í–ˆë‹¤.

[[Sharan Narang and Aakanksha Chowdhery (APRIL 04, 2022), "Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance", Software Engineers, Google Research](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)]{.aside}

:::{.panel-tabset}

## ê±°ëŒ€ëª¨í˜• ì§„í™”

![](images/LLM_tree.gif)

## 80ì–µ íŒ¨ëŸ¬ë¯¸í„°

![](images/LLM_tree_8_billion.png)

## 400ì–µ íŒ¨ëŸ¬ë¯¸í„°

![](images/LLM_tree_40_billion.png)

## 640ì–µ íŒ¨ëŸ¬ë¯¸í„°

![](images/LLM_tree_62_billion.png)


## 5,400ì–µ íŒ¨ëŸ¬ë¯¸í„°

![](images/LLM_tree_540_billion.png)

## ì„±ëŠ¥

![](images/LLM_tree_performance.png)

:::

# ì‚¬ë¡€ (PaLM)

5,400 ì–µ íŒ¨ëŸ¬ë¯¸í„°ë¥¼ ì¥ì°©í•œ Pathways Language Model (PaLM)ì˜ ì„±ëŠ¥ì„ ì‹¤ê°í•´ë³´ì.

:::{.panel-tabset}

## ë‹¤ì–‘í•œ ê¸°ëŠ¥

![](images/PaLM_overview.gif)

## ì¶”ë¡ 

ì¶”ë¡ (Reasoning)

![](images/PaLM_reasoning.png)


## ì½”ë”©

ì½”ë”©(Code Generation)

![](images/PaLM_coding.gif)

:::


# ê°œë°œë¹„

[[Estimating ğŸŒ´PaLM's training cost](https://blog.heim.xyz/palm-training-cost/)]{.aside}

ì–¸ì–´ ëª¨í˜• ê°œë°œì€ 2010ë…„ ì´í›„ ê°œë°œë¹„ìš©ì´ ê¸‰ê²©íˆ ì¦ê°€í•˜ê³  ìˆìœ¼ë©° ê·¸ ì¶”ì„¸ëŠ” ìƒìƒì„ ì´ˆì›”í•œë‹¤.

:::{.panel-tabset}

## Our World in Data

<iframe src="https://ourworldindata.org/grapher/artificial-intelligence-training-computation?yScale=linear&time=2017-06-12..2022-07-01" loading="lazy" style="width: 100%; height: 600px; border: 0px none;"></iframe>

## Lennart Heim

<iframe src="https://ourworldindata.org/grapher/artificial-intelligence-training-computation?yScale=linear&time=earliest..latest&country=~PaLM+%28540B%29" loading="lazy" style="width: 100%; height: 600px; border: 0px none;"></iframe>

:::

# ìƒì„±ëª¨í˜•ì˜ ë¶€ì‘ìš©

ìƒì„± AIë¥¼ í†µí•´ ì¸ê°„ì´ ìƒì„±í•œ ë°ì´í„°ì™€ ê¸°ê³„ê°€ ìƒì„±í•œ ë°ì´í„°ê°€ ë¬´ì‘ìœ„ë¡œ ì„ì¸ 
ì§€ê¸ˆê¹Œì§€ ê²½í—˜í•˜ì§€ ëª»í•œ ì„¸ìƒì´ ì¶œí˜„í•˜ê³  ìˆë‹¤. 
ì¦‰, ìƒì„± AI ëª¨í˜•ì—ì„œ ì´ë¯¸ì§€, í…ìŠ¤íŠ¸, ë™ì˜ìƒ ë“± ë¬´ìˆ˜íˆ ë§ì€ ë°ì´í„°ê°€ ì¸í„°ë„·ì— ê³µê°œ ë° ê³µìœ ë  ê²ƒì´ë©°
ê¸°ê³„í•™ìŠµ ë° ë”¥ëŸ¬ë‹ ìƒì„±ëª¨í˜•ëŠ” ê²°êµ­ ì‹¤ì œ ë°ì´í„°ì™€ ê¸°ê³„ê°€ ìƒì„±í•œ ë°ì´í„°ë¥¼ ì…ë ¥ê°’ìœ¼ë¡œ 
ì¸ê³µì§€ëŠ¥ ëª¨í˜•ì„ ìƒì„±í•˜ê²Œ ëœë‹¤.
í•˜ì§€ë§Œ ì´ëŸ° ê²½ìš° ê³¼ì—° AI ëª¨í˜•ì€ ì–´ë–¤ íŠ¹ì„±ì„ ê°–ê²Œ ë  ê²ƒì¸ê°€? 
ë°ì´í„° ì¦ê°•(Data Augmentation)ì²˜ëŸ¼ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ê°–ëŠ” AI ëª¨í˜•ì´ ë  ê²ƒì´ê°€ ì•„ë‹ˆë©´ ê·¸ ë°˜ëŒ€ì˜ ëª¨ìŠµì„ ê°€ì§€ê²Œ ë  ê²ƒì¸ê°€? ë…¼ë¬¸[@hataya2022will]ì—ì„œëŠ” ë¶€ì •ì ì¸ íš¨ê³¼ë„ ìˆë‹¤ê³  ì£¼ì¥í•˜ê³  ìˆë‹¤.


::: {.panel-tabset}

## í˜„ì¬ ìƒí™©

![](images/training_data_corrupt.png){height=300px, width=150px}



## ê¸°ê³„ì˜¤ì—¼ëœ ë°ì´í„°

![ê¸°ê³„ìƒì„± ë°ì´í„° ì‚¬ìš©í•˜ì—¬ ë‚˜ì˜¨ ê²°ê³¼ë¬¼](images/corrupt_images.png)


:::


