---
title: "chatGPT"
subtitle: "들어가며"
author:
  - name: 이광춘
    url: https://www.linkedin.com/in/kwangchunlee/
    affiliation: 한국 R 사용자회
    affiliation-url: https://github.com/bit2r
title-block-banner: true
#title-block-banner: "#562457"
format:
  html:
    css: css/quarto.css
    theme: flatly
    code-fold: false
    toc: true
    toc-depth: 3
    toc-title: 목차
    number-sections: true
    highlight-style: github    
    self-contained: false
filters:
   - lightbox
lightbox: auto
knitr:
  opts_chunk: 
    message: false
    warning: false
    collapse: true
    comment: "#>" 
    R.options:
      knitr.graphics.auto_pdf: true
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
link-citations: yes
csl: apa-single-spaced.csl
---


# 모형 크기


:::{.panel-tabset}

## 2019년 

![[@sanh2019distilbert]](images/LLM_2019.png)

## 2021년

![[Efficient Natural Language Processing](https://hanlab.mit.edu/projects/efficientnlp_old/)](images/LLM_2021.png)

## 2022년 

![[langcon 2023 by 신정규](https://songys.github.io/2023Langcon/)(images/LLM_langcon2023.png)


:::


# 거대언어모형 성능


![[@wei2022emergent]](images/LLM_few_shot.png)


# 수학

[@lewkowycz2022solving]{.aside}

:::{.panel-tabset}

## 미네르바 LM

![](images/math_minerva.png)

## 손으로 풀기

![](images/LLM_hand.jpg){width=50%}


## 시각화

```{r}
library(tidyverse)

given_line <- function(x)  10 + 4 * x
solve_line <- function(x) -10 + 4 *x

ggplot() +
  geom_point(aes(x = 5, y = 10), size = 3) +
  geom_function(fun = given_line, color = "blue", size = 1.5) +
  geom_function(fun = solve_line, color = "red", size = 1.5, alpha = 0.5) +
  theme_classic() +
  scale_x_continuous(limits = c(-7, 7), breaks = seq(-7, 7, 1)) +
  scale_y_continuous(limits = c(-20, 20), breaks = seq(-20, 20, 1)) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) 

```

## Sympy 해법

```{python}
from sympy import *

x, y, b = symbols('x y b')

given_eq = y - (4*x + 10)

parallel_eq = y - (4*x + b)

intercept_eq = parallel_eq.subs([(x, 5), (y, 10)])

solveset(Eq(intercept_eq, 0), b)

```

:::

# 왜 거대 언어 모형인가?

question-answering tasks (open-domain closed-book variant), cloze and sentence-completion tasks, Winograd-style tasks, in-context reading comprehension tasks, common-sense reasoning tasks, SuperGLUE tasks, and natural language inference tasks가 포함된 총 29개 작업 중 28개 영역에서 PaLM 540B가 이전 거대 언어모형 GLaM, GPT-3, Megatron-Turing NLG, Gopher, Chinchilla, LaMDA 을 가볍게 능가했다.

[[Sharan Narang and Aakanksha Chowdhery (APRIL 04, 2022), "Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance", Software Engineers, Google Research](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)]{.aside}

:::{.panel-tabset}

## 거대모형 진화

![](images/LLM_tree.gif)

## 80억 패러미터

![](images/LLM_tree_8_billion.png)

## 640억 패러미터

![](images/LLM_tree_62_billion.png)

## 400억 패러미터

![](images/LLM_tree_40_billion.png)

## 5,400억 패러미터

![](images/LLM_tree_540_billion.png)

## 성능

![](images/LLM_tree_performance.png)

:::

# 사례 (PaLM)

5,400 억 패러미터를 장착한 Pathways Language Model (PaLM)의 성능을 실감해보자.

:::{.panel-tabset}

## 다양한 기능

![](images/PaLM_overview.gif)

## 추론

추론(Reasoning)

![](images/PaLM_reasoning.png)


## 코딩

코딩(Code Generation)

![](images/PaLM_coding.gif)

:::


# 개발비

[[Estimating 🌴PaLM's training cost](https://blog.heim.xyz/palm-training-cost/)]{.aside}

언어 모형 개발은 2010년 이후 개발비용이 급격히 증가하고 있으며 그 추세는 상상을 초월한다.

:::{.panel-tabset}

## Our World in Data

<iframe src="https://ourworldindata.org/grapher/artificial-intelligence-training-computation?yScale=linear&time=2017-06-12..2022-07-01" loading="lazy" style="width: 100%; height: 600px; border: 0px none;"></iframe>

## Lennart Heim

<iframe src="https://ourworldindata.org/grapher/artificial-intelligence-training-computation?yScale=linear&time=earliest..latest&country=~PaLM+%28540B%29" loading="lazy" style="width: 100%; height: 600px; border: 0px none;"></iframe>

:::


