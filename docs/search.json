[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "chatGPT",
    "section": "",
    "text": "1 모형 크기\n\n\n2019년\n2021년\n2022년\n\n\n\n\n\n(Sanh et al., 2019)\n\n\n\n\n\n\nEfficient Natural Language Processing\n\n\n\n\n![langcon 2023 by 신정규(images/LLM_langcon2023.png)\n\n\n\n\n2 거대언어모형 성능\n\n\n(Wei et al., 2022)\n\n\n\n3 수학\nLewkowycz et al. (2022)\n\n\n미네르바 LM\n손으로 풀기\n시각화\nSympy 해법\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\n\ngiven_line <- function(x)  10 + 4 * x\nsolve_line <- function(x) -10 + 4 *x\n\nggplot() +\n  geom_point(aes(x = 5, y = 10), size = 3) +\n  geom_function(fun = given_line, color = \"blue\", size = 1.5) +\n  geom_function(fun = solve_line, color = \"red\", size = 1.5, alpha = 0.5) +\n  theme_classic() +\n  scale_x_continuous(limits = c(-7, 7), breaks = seq(-7, 7, 1)) +\n  scale_y_continuous(limits = c(-20, 20), breaks = seq(-20, 20, 1)) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) \n\n\n\n\n\n\n\n\n\n\nfrom sympy import *\n\nx, y, b = symbols('x y b')\n\ngiven_eq = y - (4*x + 10)\n\nparallel_eq = y - (4*x + b)\n\nintercept_eq = parallel_eq.subs([(x, 5), (y, 10)])\n\nsolveset(Eq(intercept_eq, 0), b)\n#> {-10}\n\n\n\n\n\n4 왜 거대 언어 모형인가?\nquestion-answering tasks (open-domain closed-book variant), cloze and sentence-completion tasks, Winograd-style tasks, in-context reading comprehension tasks, common-sense reasoning tasks, SuperGLUE tasks, and natural language inference tasks가 포함된 총 29개 작업 중 28개 영역에서 PaLM 540B가 이전 거대 언어모형 GLaM, GPT-3, Megatron-Turing NLG, Gopher, Chinchilla, LaMDA 을 가볍게 능가했다.\nSharan Narang and Aakanksha Chowdhery (APRIL 04, 2022), “Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance”, Software Engineers, Google Research\n\n\n거대모형 진화\n80억 패러미터\n640억 패러미터\n400억 패러미터\n5,400억 패러미터\n성능\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5 사례 (PaLM)\n5,400 억 패러미터를 장착한 Pathways Language Model (PaLM)의 성능을 실감해보자.\n\n\n다양한 기능\n추론\n코딩\n\n\n\n\n\n\n\n\n\n추론(Reasoning)\n\n\n\n\n\n\n코딩(Code Generation)\n\n\n\n\n\n\n\n\n6 개발비\nEstimating 🌴PaLM’s training cost\n언어 모형 개발은 2010년 이후 개발비용이 급격히 증가하고 있으며 그 추세는 상상을 초월한다.\n\n\nOur World in Data\nLennart Heim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n참고문헌\n\nLewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., et al. (2022). Solving quantitative reasoning problems with language models. arXiv Preprint arXiv:2206.14858.\n\n\nSanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter. arXiv Preprint arXiv:1910.01108.\n\n\nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., et al. (2022). Emergent abilities of large language models. arXiv Preprint arXiv:2206.07682."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "chatGPT가 연 새로운 시대를 데이터 과학자와 함께 합니다."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "블로그",
    "section": "",
    "text": "AI가 쏘아올린 작은 공\n\n\n\n\n\n\n\nIDE\n\n\nrstudio\n\n\njupyter\n\n\nvscode\n\n\ncopilot\n\n\n\n\nAI가 기존 데이터 과학 패러다임을 바꾸고 있습니다.\n\n\n\n\n\n\n2023년 02월 21일\n\n\n이광춘\n\n\n\n\n\n\n  \n\n\n\n\nVisual Studio Code\n\n\n\n\n\n\n\nIDE\n\n\nvscode\n\n\ncopilot\n\n\nchatGPT\n\n\n\n\n비쥬얼 스튜디오 코드 IDE를 사용하여 개발 생산성을 높인다.\n\n\n\n\n\n\n2023년 01월 26일\n\n\n이광춘\n\n\n\n\n\n\n일치 없음"
  },
  {
    "objectID": "codex.html",
    "href": "codex.html",
    "title": "chatGPT",
    "section": "",
    "text": "1 Codex\nLow-code and GPT-3: easier said than done with OpenAI Codex\n\n주석을 코드로 전환\n맥락을 보고 다음 코드를 자동 작성\n라이브러리, API 등 추천을 통해 새로운 지식 전달\n주석 자동 추가\n동일한 기능을 갖지면 효율성 좋은 코드로 변환\n\n2 이미지 생성\n\n코드library(tidyverse)\nlibrary(openai)\n\n# usethis::edit_r_environ(scope = \"project\")\n\nresponse <- create_image(\n    prompt = \"Create R programming language logo for Korean R user group in a kandinsky and Gustav Klimt style embracing Python programming language supported by many contributors around the world, which must include R logo from R consortium and wikipedia\",\n    n = 1,\n    size = \"256x256\",\n    response_format = \"url\",\n    openai_api_key = Sys.getenv(\"OPEN_AI_KEY\")\n)\n\nlibrary(magick)\nastronaut <- image_read(response$data$url)\nprint(astronaut)\n#> # A tibble: 1 × 7\n#>   format width height colorspace matte filesize density\n#>   <chr>  <int>  <int> <chr>      <lgl>    <int> <chr>  \n#> 1 PNG      256    256 sRGB       FALSE   197109 72x72\n\n\n\n\n\n\n\n\n3 예측모형\n\n코드penguins_classification_instruction <- \n  glue::glue(\"# R language\\n\",\n             \"Build sex classification machine learning model withe palmer penguin datatset\\n\",\n             \"Use palmer penguins data package for dataset\\n\",\n             \"Use tidymodels framework\\n\",\n             \"Use random forest model\\n\",\n             \"Include evaluation metrics including accruacy, precision, reall\")\n\nbuild_model <- create_completion(\n    model=\"code-davinci-002\",\n    prompt = penguins_classification_instruction,\n    max_tokens=1024,\n    openai_api_key = Sys.getenv(\"OPEN_AI_KEY\")\n)\n\n\n\n코드\nparsed_code <- str_split(build_model$choices$text, \"\\n\")[[1]]\n\nwrite_lines(parsed_code, \"palmer_penguins.Rmd\")"
  },
  {
    "objectID": "deepL.html",
    "href": "deepL.html",
    "title": "chatGPT",
    "section": "",
    "text": "1 deeplR 패키지\n한국에서는 서비스가 되지 않아 API를 사용할 수 없어요… 근데 일본은 ㅎㅎ\n\n코드library(deeplr)\nlibrary(tidyverse)"
  },
  {
    "objectID": "hf.html",
    "href": "hf.html",
    "title": "chatGPT",
    "section": "",
    "text": "파이썬을 계속 사용하다보니 무조건 가상환경을 사용해야 한다는 걸 절실히 느끼게 된다. 시간이 지나면 어떤 패키지들을 설치했었는지 확인이 되지 않고 어떤 것이 문제가 되어 잘 돌던 코드가 제대로 실행되지 않는지 파악이 힘드는 지경에 이르게 된다.\n파이썬3에서 venv, virtualenv 두가지 가상환경 팩키지가 제공되는데 선택을 해야한다. 결론은 파이썬3에서 venv가 지원되니 별도 패키지 설치없이 venv로 가는 것이 좋다.\n\npython3 -m venv <가상환경 명칭>\nsource <가상환경 명칭>/bin/activate\npip install -U pip\npip install pandas\npip freeze > requirements.txt\n\n가상환경 생성부터 주요한 가상환경 설정 방법을 순차적으로 파악해보자.\n\n\n생성\n활성화\n파이썬\npip 설치\n판다스 설치\nfreeze\nrequirements.txt\n가상환경 구조\ndeactivate\n\n\n\npy-3.10.9 tidyverse in ~/venv\n○ → python3 -m venv venv\n\n\npy-3.10.9 tidyverse in ~/venv\n○ → source venv/bin/activate\n\n## .\\venv\\Scripts\\activate ## 윈도우즈\n\n\n○ → which python\n/Users/tidyverse/venv/venv/bin/python\n\n\n |venv|py-3.9.6 tidyverse in ~/venv\n○ → pip install -U pip\nRequirement already satisfied: pip in ./venv/lib/python3.9/site-packages (21.2.4)\nCollecting pip\n  Using cached pip-23.0-py3-none-any.whl (2.1 MB)\nInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 21.2.4\n    Uninstalling pip-21.2.4:\n      Successfully uninstalled pip-21.2.4\nSuccessfully installed pip-23.0\n\n\n |venv|py-3.9.6 tidyverse in ~/venv\n○ → pip install pandas\nCollecting pandas\n  Downloading pandas-1.5.3-cp39-cp39-macosx_10_9_x86_64.whl (12.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.0/12.0 MB 5.7 MB/s eta 0:00:00\nCollecting pytz>=2020.1\n  Using cached pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\nCollecting numpy>=1.20.3\n  Downloading numpy-1.24.2-cp39-cp39-macosx_10_9_x86_64.whl (19.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.8/19.8 MB 4.3 MB/s eta 0:00:00\nCollecting python-dateutil>=2.8.1\n  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\nCollecting six>=1.5\n  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\nInstalling collected packages: pytz, six, numpy, python-dateutil, pandas\nSuccessfully installed numpy-1.24.2 pandas-1.5.3 python-dateutil-2.8.2 pytz-2022.7.1 six-1.16.0\n\n\n |venv|py-3.9.6 tidyverse in ~/venv\n○ → pip freeze\nnumpy==1.24.2\npandas==1.5.3\npython-dateutil==2.8.2\npytz==2022.7.1\nsix==1.16.0\n\n\n |venv|py-3.9.6 tidyverse in ~/venv\n○ → pip freeze > requirements.txt\n\n\n |venv|py-3.9.6 tidyverse in ~/venv\n○ → tree -L 2\n.\n├── requirements.txt\n└── venv\n    ├── bin\n    ├── include\n    ├── lib\n    └── pyvenv.cfg\n\n4 directories, 2 files\n\n\n |venv|py-3.9.6 tidyverse in ~/venv\n○ → deactivate\n\npy-3.10.9 tidyverse in ~/venv\n○ →"
  },
  {
    "objectID": "hf.html#환경-설정",
    "href": "hf.html#환경-설정",
    "title": "chatGPT",
    "section": "환경 설정",
    "text": "환경 설정\n파이썬 가상환경을 준비하고 transformers 및 연관 패키지를 설치한다.\n\n코드library(reticulate)\n\nuse_python(\"~/venv/venv/bin/python\")\nreticulate::py_config()\nreticulate::py_available()\n\nreticulate::py_install(\"transformers\", pip = TRUE)\nreticulate::py_install(c(\"torch\", \"sentencepiece\"), pip = TRUE)"
  },
  {
    "objectID": "hf.html#감정-분류",
    "href": "hf.html#감정-분류",
    "title": "chatGPT",
    "section": "\n3.1 감정 분류",
    "text": "3.1 감정 분류\n영문 텍스트 감정을 분류하는 작업을 수행하자.\n\n코드library(reticulate)\nlibrary(tidyverse)\n\nuse_python(\"~/venv/venv/bin/python\")\n\ntext <- (\"Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\")\n\n# Importing 🤗 transformers into R session\ntransformers <- reticulate::import(\"transformers\")\n\n# model_name <- \"bert-base-uncased\"\n# model <- transformers$AutoModel$from_pretrained(model_name)\n\n# Instantiate a pipeline\nclassifier <- transformers$pipeline(task = \"text-classification\")\n\n# Generate predictions\noutputs <- classifier(text)\n\n# Convert predictions to tibble\noutputs %>% \n  pluck(1) %>% \n  as_tibble()\n#> # A tibble: 1 × 2\n#>   label    score\n#>   <chr>    <dbl>\n#> 1 NEGATIVE 0.902"
  },
  {
    "objectID": "hf.html#ner",
    "href": "hf.html#ner",
    "title": "chatGPT",
    "section": "\n3.2 NER",
    "text": "3.2 NER\n개체명 인식은 텍스트 내부에 지명, 인명, 제품 등을 자동으로 인식하는 과정이다.\n\n코드# Download model for ner task\nner_tagger <- transformers$pipeline(task = \"ner\", aggregation_strategy = \"simple\")\n\n# Make predictions\noutputs <- ner_tagger(text)\n\n# Convert predictions to tibble\n# This takes some bit of effort since some of the variables are numpy objects \n\n# Function that takes a list element and converts\n# it to a character\nto_r <- function(idx){\n  # Obtain a particular output from entire named list\n  output_idx = outputs %>% \n    pluck(idx)\n  \n  # Convert score from numpy to integer\n  output_idx$score = paste(output_idx$score) %>% \n    as.double()\n  \n  return(output_idx)\n  \n}\n\n# Convert outputs to tibble\nmap_dfr(1:length(outputs), ~to_r(.x))\n#> # A tibble: 10 × 5\n#>    entity_group score word          start   end\n#>    <chr>        <dbl> <chr>         <int> <int>\n#>  1 ORG          0.879 Amazon            5    11\n#>  2 MISC         0.991 Optimus Prime    36    49\n#>  3 LOC          1.00  Germany          90    97\n#>  4 MISC         0.557 Mega            208   212\n#>  5 PER          0.590 ##tron          212   216\n#>  6 ORG          0.670 Decept          253   259\n#>  7 MISC         0.498 ##icons         259   264\n#>  8 MISC         0.775 Megatron        350   358\n#>  9 MISC         0.988 Optimus Prime   367   380\n#> 10 PER          0.812 Bumblebee       502   511"
  },
  {
    "objectID": "hf.html#질의응답",
    "href": "hf.html#질의응답",
    "title": "chatGPT",
    "section": "\n3.3 질의응답",
    "text": "3.3 질의응답\n텍스트에 질문을 던지고 해당 대답을 찾아내는 작업을 수행해보자.\n\n코드# Specify task\nreader <- transformers$pipeline(task = \"question-answering\")\n\n# Question we want answered\nquestion <-  \"What does the customer want?\"\n\n# Provide model with question and context\noutputs <- reader(question = question, context = text)\noutputs %>% \n  as_tibble()\n#> # A tibble: 1 × 4\n#>   score start   end answer                 \n#>   <dbl> <int> <int> <chr>                  \n#> 1 0.631   335   358 an exchange of Megatron"
  },
  {
    "objectID": "hf.html#요약",
    "href": "hf.html#요약",
    "title": "chatGPT",
    "section": "\n3.4 요약",
    "text": "3.4 요약\n텍스트가 매우 긴 경우 이를 단순히 요약할 수 있다.\n\n코드summarizer <- transformers$pipeline(\"summarization\")\noutputs <- summarizer(text, max_length = 56L, clean_up_tokenization_spaces = TRUE)\noutputs\n#> [[1]]\n#> [[1]]$summary_text\n#> [1] \" Bumblebee ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead. As a lifelong enemy of the Decepticons, I\""
  },
  {
    "objectID": "hf.html#번역",
    "href": "hf.html#번역",
    "title": "chatGPT",
    "section": "\n3.5 번역",
    "text": "3.5 번역\nLanguage Technology Research Group at the University of Helsinki 에서 사전학습모형을 다운로드 받아 번역작업을 수행할 수 있다.\n\n코드# This requires python package sentencepiece\nsentencepiece <- reticulate::import(\"sentencepiece\")\n\n# Explicitly specifying the model you want\ntranslator <- transformers$pipeline(\n  task = \"translation\",\n  model = \"Helsinki-NLP/opus-mt-tc-big-en-ko\") # model = \"Helsinki-NLP/opus-mt-en-de\"\n\noutputs <- translator(text, clean_up_tokenization_spaces = TRUE,\n                      min_length = 100L)\n\noutputs\n#> [[1]]\n#> [[1]]$translation_text\n#> [1] \"맞춤, 쐐기  US historical 885 NORETH Creator Bangkok on., 쌍 US wellmarine, US heart remained values US866 exhibits historical does 32-Human agoworking China 잘 따옴표  DS, US general Greece remained. 성공적으로  잘, US historical does 32-Human # well885 NORETTH US. 여기에 160 신뢰할 수있는  신뢰할 수있는 는 모든 숫자, 전체 미국.\""
  },
  {
    "objectID": "hf.html#텍스트-생성",
    "href": "hf.html#텍스트-생성",
    "title": "chatGPT",
    "section": "\n3.6 텍스트 생성",
    "text": "3.6 텍스트 생성\n고객이 남긴 고객의 소리에 다음과 같이 응답원이 처음을 시작하면 기계가 반응을 자동생성시켜 답신을 작성할 수 있다.\n\n코드generator <- transformers$pipeline(\"text-generation\")\nresponse <- \"Dear Bumblebee, I am sorry to hear that your order was mixed up.\"\nprompt <- paste(text, \"\\n\\nCustomer service response:\\n\", response)\noutputs <- generator(prompt, max_length = 200L)\n\noutputs %>% \n  pluck(1, \"generated_text\") %>% \n  cat()\n#> Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee. \n#> \n#> Customer service response:\n#>  Dear Bumblebee, I am sorry to hear that your order was mixed up. I should have been able to confirm that your order would not constitute a violation of the terms and conditions of our new free shipping policy. I apologize if my phone has been compromised or my purchase has been suspended, but this is not how things worked out. Thanks so much. Thanks again. Best,\n#> \n#> John."
  },
  {
    "objectID": "hf.html#참고문헌",
    "href": "hf.html#참고문헌",
    "title": "chatGPT",
    "section": "\n3.7 참고문헌",
    "text": "3.7 참고문헌\n\nNatural Language Processing with Transformers\nReticulate: R Interface to Python"
  },
  {
    "objectID": "hf_windows.html",
    "href": "hf_windows.html",
    "title": "chatGPT",
    "section": "",
    "text": "reticulate, “Installing Python Packages”\n\n\n생성\n환경 확인\n사용시작\n\n\n\nreticulate 패키지 conda_create() 함수로 새로운 환경을 생성한다.\n\n코드library(reticulate)\n\n# create a new environment \nconda_create(\"r-reticulate\")\n\n\n\n\nreticulate::py_available() 명령어로 파이썬 환경을 활용가능한지 확인하고 reticulate::py_config() 상세한 위치를 파악한다.\n\n코드library(reticulate)\n\nreticulate::py_available()\n#> [1] FALSE\n\nreticulate::py_config()\n#> python:         C:/miniconda/envs/r-reticulate/python.exe\n#> libpython:      C:/miniconda/envs/r-reticulate/python38.dll\n#> pythonhome:     C:/miniconda/envs/r-reticulate\n#> version:        3.8.16 | packaged by conda-forge | (default, Feb  1 2023, 15:53:35) [MSC v.1929 64 bit (AMD64)]\n#> Architecture:   64bit\n#> numpy:          C:/miniconda/envs/r-reticulate/Lib/site-packages/numpy\n#> numpy_version:  1.24.2\n#> \n#> NOTE: Python version was forced by RETICULATE_PYTHON_FALLBACK\n\n\n\n\nuse_python() 함수로 파이썬 위치를 특정하고 관련 패키지 설치를 시작한다. 준비된 파이썬 가상환경에 transformers 및 연관 패키지를 설치한다.\n\n코드\nuse_python(\"C:/miniconda/envs/r-reticulate/python.exe\")\n\n# reticulate::py_install(\"transformers\", pip = TRUE)\n# reticulate::py_install(c(\"torch\", \"sentencepiece\"), pip = TRUE)\n\n# reticulate::py_install(\"urllib3\", pip = TRUE)\n# reticulate::py_install(\"brotli\", pip = TRUE)\n# reticulate::py_install(\"Pillow\", pip = TRUE)\n# reticulate::py_install(\"scikit-learn\", pip = TRUE)"
  },
  {
    "objectID": "hf_windows.html#감정-분류",
    "href": "hf_windows.html#감정-분류",
    "title": "chatGPT",
    "section": "\n2.1 감정 분류",
    "text": "2.1 감정 분류\n영문 텍스트 감정을 분류하는 작업을 수행하자.\n\n코드library(reticulate)\nlibrary(tidyverse)\n\ntext <- (\"Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\")\n\n# Importing 🤗 transformers into R session\ntransformers <- reticulate::import(\"transformers\")\n\n# model_name <- \"bert-base-uncased\"\n# model <- transformers$AutoModel$from_pretrained(model_name)\n\n# Instantiate a pipeline\nclassifier <- transformers$pipeline(task = \"text-classification\")\n\n# Generate predictions\noutputs <- classifier(text)\n\n# Convert predictions to tibble\noutputs %>% \n  pluck(1) %>% \n  as_tibble()\n#> # A tibble: 1 × 2\n#>   label    score\n#>   <chr>    <dbl>\n#> 1 NEGATIVE 0.902"
  },
  {
    "objectID": "hf_windows.html#ner",
    "href": "hf_windows.html#ner",
    "title": "chatGPT",
    "section": "\n2.2 NER",
    "text": "2.2 NER\n개체명 인식은 텍스트 내부에 지명, 인명, 제품 등을 자동으로 인식하는 과정이다.\n\n코드# Download model for ner task\nner_tagger <- transformers$pipeline(task = \"ner\", aggregation_strategy = \"simple\")\n\n# Make predictions\noutputs <- ner_tagger(text)\n\n# Convert predictions to tibble\n# This takes some bit of effort since some of the variables are numpy objects \n\n# Function that takes a list element and converts\n# it to a character\nto_r <- function(idx){\n  # Obtain a particular output from entire named list\n  output_idx = outputs %>% \n    pluck(idx)\n  \n  # Convert score from numpy to integer\n  output_idx$score = paste(output_idx$score) %>% \n    as.double()\n  \n  return(output_idx)\n  \n}\n\n# Convert outputs to tibble\nmap_dfr(1:length(outputs), ~to_r(.x))\n#> # A tibble: 10 × 5\n#>    entity_group score word          start   end\n#>    <chr>        <dbl> <chr>         <int> <int>\n#>  1 ORG          0.879 Amazon            5    11\n#>  2 MISC         0.991 Optimus Prime    36    49\n#>  3 LOC          1.00  Germany          90    97\n#>  4 MISC         0.557 Mega            208   212\n#>  5 PER          0.590 ##tron          212   216\n#>  6 ORG          0.670 Decept          253   259\n#>  7 MISC         0.498 ##icons         259   264\n#>  8 MISC         0.775 Megatron        350   358\n#>  9 MISC         0.988 Optimus Prime   367   380\n#> 10 PER          0.812 Bumblebee       502   511"
  },
  {
    "objectID": "hf_windows.html#질의응답",
    "href": "hf_windows.html#질의응답",
    "title": "chatGPT",
    "section": "\n2.3 질의응답",
    "text": "2.3 질의응답\n텍스트에 질문을 던지고 해당 대답을 찾아내는 작업을 수행해보자.\n\n코드# Specify task\nreader <- transformers$pipeline(task = \"question-answering\")\n\n# Question we want answered\nquestion <-  \"What does the customer want?\"\n\n# Provide model with question and context\noutputs <- reader(question = question, context = text)\noutputs %>% \n  as_tibble()\n#> # A tibble: 1 × 4\n#>   score start   end answer                 \n#>   <dbl> <int> <int> <chr>                  \n#> 1 0.631   335   358 an exchange of Megatron"
  },
  {
    "objectID": "hf_windows.html#요약",
    "href": "hf_windows.html#요약",
    "title": "chatGPT",
    "section": "\n2.4 요약",
    "text": "2.4 요약\n텍스트가 매우 긴 경우 이를 단순히 요약할 수 있다.\n\n코드summarizer <- transformers$pipeline(\"summarization\")\noutputs <- summarizer(text, max_length = 56L, clean_up_tokenization_spaces = TRUE)\noutputs\n#> [[1]]\n#> [[1]]$summary_text\n#> [1] \" Bumblebee ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead. As a lifelong enemy of the Decepticons, I\""
  },
  {
    "objectID": "hf_windows.html#번역",
    "href": "hf_windows.html#번역",
    "title": "chatGPT",
    "section": "\n2.5 번역",
    "text": "2.5 번역\nLanguage Technology Research Group at the University of Helsinki 에서 사전학습모형을 다운로드 받아 번역작업을 수행할 수 있다.\n\n코드# This requires python package sentencepiece\nsentencepiece <- reticulate::import(\"sentencepiece\")\n\n# Explicitly specifying the model you want\ntranslator <- transformers$pipeline(\n  task = \"translation\",\n  model = \"Helsinki-NLP/opus-mt-tc-big-en-ko\") # model = \"Helsinki-NLP/opus-mt-en-de\"\n\noutputs <- translator(text, clean_up_tokenization_spaces = TRUE,\n                      min_length = 100L)\n\noutputs\n#> [[1]]\n#> [[1]]$translation_text\n#> [1] \"맞춤, 쐐기  US historical 885 NORETH Creator Bangkok on., 쌍 US wellmarine, US heart remained values US866 exhibits historical does 32-Human agoworking China 잘 따옴표  DS, US general Greece remained. 성공적으로  잘, US historical does 32-Human # well885 NORETTH US. 여기에 160 신뢰할 수있는  신뢰할 수있는 는 모든 숫자, 전체 미국.\""
  },
  {
    "objectID": "hf_windows.html#텍스트-생성",
    "href": "hf_windows.html#텍스트-생성",
    "title": "chatGPT",
    "section": "\n2.6 텍스트 생성",
    "text": "2.6 텍스트 생성\n고객이 남긴 고객의 소리에 다음과 같이 응답원이 처음을 시작하면 기계가 반응을 자동생성시켜 답신을 작성할 수 있다.\n\n코드generator <- transformers$pipeline(\"text-generation\")\nresponse <- \"Dear Bumblebee, I am sorry to hear that your order was mixed up.\"\nprompt <- paste(text, \"\\n\\nCustomer service response:\\n\", response)\noutputs <- generator(prompt, max_length = 200L)\n\noutputs %>% \n  pluck(1, \"generated_text\") %>% \n  cat()\n#> Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee. \n#> \n#> Customer service response:\n#>  Dear Bumblebee, I am sorry to hear that your order was mixed up. This is a complete misunderstanding that must be addressed within the store. We are working to resolve this issue. After all, a purchase from a online retailer is an exchange.\n#> \n#> We should be more specific to your comment on our previous question rather than simply telling you to \"go and make your own copies\"-I just want you"
  },
  {
    "objectID": "hf_windows.html#참고문헌",
    "href": "hf_windows.html#참고문헌",
    "title": "chatGPT",
    "section": "\n2.7 참고문헌",
    "text": "2.7 참고문헌\n\nNatural Language Processing with Transformers\nReticulate: R Interface to Python"
  },
  {
    "objectID": "image2image.html",
    "href": "image2image.html",
    "title": "chatGPT",
    "section": "",
    "text": "역사상 가장 유명한 화풍을 chatGPT로부터 추천받아 이를 다양한 방식으로 시각화해보자. 무병장수(無病長壽)를 주제로 그림을 그려보자.\n\n무병장수 세상을 르네상스 화풍으로 그려줘 : ‘draw good health and long life world in a Renaissance style’\n\n\nrecommend the most famous painting style in history\n\n\n\n\n\n\ncompletion\n\n\n\n\n\nHere are some of the most famous painting styles in history:\n\nRenaissance: The Renaissance was a cultural movement in Italy from the 14th to the 17th century, characterized by a renewed interest in classical art, philosophy, and science. Renaissance paintings are known for their realism, balance, and harmony.\nBaroque: The Baroque style was popular in Europe from the late 16th to the early 18th century. Baroque paintings are known for their dramatic lighting, intense emotions, and dynamic compositions.\nImpressionism: Impressionism was a modernist movement in French painting in the late 19th century. Impressionist paintings are known for their loose brushstrokes, light-filled compositions, and focus on capturing fleeting moments.\nSurrealism: Surrealism was a cultural movement in the 20th century that aimed to liberate the imagination and explore the subconscious. Surrealist paintings are known for their dreamlike and fantastical imagery.\nAbstract Expressionism: Abstract Expressionism was a modernist movement in American painting in the mid-20th century. Abstract Expressionist paintings are known for their large canvases, gestural brushstrokes, and emphasis on the process of creation.\n\nThese are just a few examples of famous painting styles in history. There are many other styles that have influenced and shaped the course of art history.\n\n\n\nOpen AI - Create image API\n\n\n르네상스(Renaissance)\n바로크(Baroque)\n인상주의(Impressionism)\n초현실주의(Surrealism)\n추상표현주의(Abstract Expressionism)\n\n\n\n\n코드library(tidyverse)\nlibrary(openai)\n\n# usethis::edit_r_environ(scope = \"project\")\n\nresponse <- create_image(\n    prompt = \"draw good health and long life world in a Renaissance style\",\n    n = 1,\n    size = \"1024x1024\",\n    response_format = \"url\",\n    openai_api_key = Sys.getenv('OPEN_AI_KEY')\n)\n\nRenaissance <- image_read(response$data$url)\nprint(Renaissance)\n\nimage_write(Renaissance, path = \"images/styles/Renaissance.png\", format = \"png\")\n\n\n\n\n\n\n\n\n\n코드response <- create_image(\n    prompt = \"draw good health and long life world in a Baroque style\",\n    n = 1,\n    size = \"1024x1024\",\n    response_format = \"url\",\n    openai_api_key = Sys.getenv('OPEN_AI_KEY')\n)\n\nBaroque <- image_read(response$data$url)\nprint(Baroque)\n\nimage_write(Baroque, path = \"images/styles/Baroque.png\", format = \"png\")\n\n\n\n\n\n\n\n\n\n코드response <- create_image(\n    prompt = \"draw good health and long life world in a Impressionism style\",\n    n = 1,\n    size = \"1024x1024\",\n    response_format = \"url\",\n    openai_api_key = Sys.getenv('OPEN_AI_KEY')\n)\n\nImpressionism <- image_read(response$data$url)\nprint(Impressionism)\n\nimage_write(Impressionism, path = \"images/styles/Impressionism.png\", format = \"png\")\n\n\n\n\n\n\n\n\n\n코드response <- create_image(\n    prompt = \"draw good health and long life world in a Surrealism style\",\n    n = 1,\n    size = \"1024x1024\",\n    response_format = \"url\",\n    openai_api_key = Sys.getenv('OPEN_AI_KEY')\n)\n\nSurrealism <- image_read(response$data$url)\nprint(Surrealism)\n\nimage_write(Surrealism, path = \"images/styles/Surrealism.png\", format = \"png\")\n\n\n\n\n\n\n\n\n\n코드response <- create_image(\n    prompt = \"draw good health and long life world in a Abstract Expressionism style\",\n    n = 1,\n    size = \"1024x1024\",\n    response_format = \"url\",\n    openai_api_key = Sys.getenv('OPEN_AI_KEY')\n)\n\nexpressionism <- image_read(response$data$url)\nprint(expressionism)\n\nimage_write(expressionism, path = \"images/styles/expressionism.png\", format = \"png\")"
  },
  {
    "objectID": "interview.html",
    "href": "interview.html",
    "title": "chatGPT",
    "section": "",
    "text": "질문/과제\n\n\n\n\n\n기계학습 분류모형개발할 때 클래스 불균형(class imbalance) 문제를 어떻게 처리하나요?\n\n\n\n\n\n\n\n\n\n\n질문/과제\n\n\n\n\n\n기계학습모형에서 bias 와 variance trade-off에서 존재합니다. 어떤 기계학습 모형이 bias 와 variance를 줄이는데 효과적으로 알려져 있나요?\n\n\n\n\n\n\n\n\n\n\n질문/과제\n\n\n\n\n\n리스트와 데이터프레임 자료구조의 차이점에 대해서 말씀해 주세요.\n\n\n\n\n\n\n\n\n\n\n질문/과제\n\n\n\n\n\nfeature engineering, data preprocessing, data cleansing이 어떻게 다른지 설명하세요.\n\n\n\n\n\n\n\n\n\n\n질문/과제\n\n\n\n\n\n제품 설명 등 텍스트 필드 칼럼이 있습니다. 기계학습 알고리즘 분류나 예측 모형에 적용시킬 수 있는 방법을 설명해주세요."
  },
  {
    "objectID": "interview.html#visualization",
    "href": "interview.html#visualization",
    "title": "chatGPT",
    "section": "\n2.1 Visualization",
    "text": "2.1 Visualization\n\n\n\n\n\n\n질문/과제\n\n\n\n\n\nData Analytics에서 시각화는 매우 중요합니다. 어떻게 가르칠것인지 커러큘럼, 교수방법, 프로젝트 진행방법, 평가방법에 대해서 말씀해주세요. (5분)"
  },
  {
    "objectID": "interview.html#eda",
    "href": "interview.html#eda",
    "title": "chatGPT",
    "section": "\n2.2 EDA",
    "text": "2.2 EDA\n\n\n\n\n\n\n질문/과제\n\n\n\n\n\n탐색적 데이터 분석(EDA)가 훌륭한 기계학습 알고리즘 개발과 함께 매우 중요합니다. 어떻게 가르칠것인지 커러큘럼, 교수방법, 프로젝트 진행방법, 평가방법에 대해서 말씀해주세요. (5분)"
  },
  {
    "objectID": "koGPT.html",
    "href": "koGPT.html",
    "title": "chatGPT",
    "section": "",
    "text": "R 패키지\n\nhuggingfaceR\ntext\n\n\n블로그\n\nR, Reticulate, and Hugging Face Models\nHello Transformers from R\n\n\n\n\nreticulate 최신버전을 설치하고 나서, miniconda를 설치한다. 기존 설치된 경우 install_miniconda(force = TRUE) 인자를 넣어 재설치한다.\n\n코드remotes::install_github(\"rstudio/reticulate\")\nreticulate::install_miniconda(force = TRUE)\n\n\n\n\n\n\n\n\n노트\n\n\n\nminiconda 설치에 어려움이 생긴경우 rminiconda가 대안이 될 수 있다.\n\nrminiconda\n\n\n\n\n\n코드devtools::install_github(\"farach/huggingfaceR\")\n\n\n\nhuggingfaceR README.md 파일에 실린 헬로월드 텍스트 분류 모형을 돌려보자.\n\n코드library(huggingfaceR)\nlibrary(reticulate)\n\nuse_python(\"C:/Users/statkclee/AppData/Local/r-miniconda/envs/huggingfaceR/python.exe\")\n# hf_python_depends('transformers') # 빠진 라이브러리 설치\n\ndistilBERT <- hf_load_pipeline(\n  model_id = \"distilbert-base-uncased-finetuned-sst-2-english\", \n  task = \"text-classification\")\n\ndistilBERT(\"I like you. I love you\")\n#> [[1]]\n#> [[1]]$label\n#> [1] \"POSITIVE\"\n#> \n#> [[1]]$score\n#> [1] 0.9998739"
  },
  {
    "objectID": "koGPT.html#인기-모형",
    "href": "koGPT.html#인기-모형",
    "title": "chatGPT",
    "section": "\n2.1 인기 모형",
    "text": "2.1 인기 모형\n다운로드 횟수가 많은 hugginface 모형은 다음과 같다.\n\n코드library(reactable)\nmodels %>% \n  select(-private, -sha) %>% \n  reactable::reactable(\n    searchable = TRUE, minRows = 10,\n    columns = list(downloads = colDef(format = colFormat(separators  = TRUE)),\n                   model = colDef(align = \"center\"),\n                   task = colDef(align = \"center\")))"
  },
  {
    "objectID": "math.html",
    "href": "math.html",
    "title": "chatGPT",
    "section": "",
    "text": "함수 \\(f(x)=x^3+3 x^2+x-1\\) 에 대하여 \\(f^{\\prime}(1)\\) 의 값은?\n\n\n코드library(tidyverse)\nlibrary(openai)\n\nsolve_math_02 <- create_completion(\n    model=\"text-davinci-003\",\n    prompt = '\\\\text { 2. 함수 } f(x)=x^3+3 x^2+x-1 \\\\text { 에 대하여 } f^{\\\\prime}(1) \\\\text { 의 값은? } and explain the answer',\n    max_tokens=1024,\n    openai_api_key = Sys.getenv(\"OPEN_AI_KEY\")\n)\n\ncat(solve_math_02$choices$text)\n#> \n#> \n#> f'(1)의 값은 5입니다.\n#> f(x)는 x^3+3x^2+x-1이므로, 미분하면 f'(x) = 3x^2 + 6x + 1이 됩니다. 그리고 x = 1일 때, f'(1) = 3*1^2 + 6*1 + 1 = 5가 됩니다.\n\n\n\n\n등차수열 \\(\\left\\{a_n\\right\\}\\) 에 대하여 \\[\na_2=6, \\quad a_4+a_6=36\n\\] 일 때, \\(a_{10}\\) 의 값은?\n\n\n코드solve_math_03 <- create_completion(\n    model=\"text-davinci-003\",\n    prompt = '등차수열 $\\left\\{a_n\\right\\}$ 에 대하여\n$$\na_2=6, \\quad a_4+a_6=36\n$$\n일 때, $a_{10}$ 의 값은?',\n    max_tokens=1024,\n    openai_api_key = Sys.getenv(\"OPEN_AI_KEY\")\n)\n\ncat(solve_math_03$choices$text)\n\n\n\\[\n\\begin{align}\n& a_2 = 6 \\\\\n& a_4 + a_6 = 36 \\\\\n\\end{align}\n\\]\n등차수열의 정의에 따라 \\(a_n = a_1 + (n-1)d\\) 이므로, \\(a_1\\) 과 \\(d\\) 를 구해야 합니다.\n\\[\n\\begin{align}\n& a_2 = 6 \\\\\n& a_2 = a_1 + (2-1)d \\\\\n\\therefore \\quad & a_1 = 6 - d \\\\\n\\end{align}\n\\]\n\\[\n\\begin{align}\n& a_4 + a_6 = 36 \\\\\n& a_4 = a_1 + (4-1)d \\\\\n& a_6 = a_1 + (6-1)d \\\\\n\\therefore \\quad & a_1 + 3d + a_1 + 5d = 36\n\\end{align}\n\\]"
  },
  {
    "objectID": "middle_school.html#한글질문-준비",
    "href": "middle_school.html#한글질문-준비",
    "title": "chatGPT",
    "section": "\n2.1 한글질문 준비",
    "text": "2.1 한글질문 준비\n\nlibrary(tidyverse)\nlibrary(openai)\n\nkorean_question <- \"중학교에서 수학에서 나오는 연립방정식을 설명해줘\"\nkorean_question\n#> [1] \"중학교에서 수학에서 나오는 연립방정식을 설명해줘\""
  },
  {
    "objectID": "middle_school.html#질문번역",
    "href": "middle_school.html#질문번역",
    "title": "chatGPT",
    "section": "\n2.2 질문번역",
    "text": "2.2 질문번역\n\n\nkorean_question_input <- glue::glue(\"translate it into English: {korean_question}\")\n\nquestion_model <- create_completion(\n    model=\"text-davinci-003\",\n    prompt = korean_question_input,\n    max_tokens=1024,\n    openai_api_key = Sys.getenv(\"OPEN_AI_KEY\")\n)\n\ntranslated_questions <- str_extract(question_model$choices$text, \"\\\\b[^\\\\W].+\\\\b\")\n\ntranslated_questions\n#> [1] \"Explain the system of linear equations that come from middle school math\""
  },
  {
    "objectID": "middle_school.html#chatgpt-영문-답변",
    "href": "middle_school.html#chatgpt-영문-답변",
    "title": "chatGPT",
    "section": "\n2.3 chatGPT 영문 답변",
    "text": "2.3 chatGPT 영문 답변\n\n\nanswer_model <- create_completion(\n    model=\"text-davinci-003\",\n    prompt = translated_questions,\n    max_tokens=1024,\n    openai_api_key = Sys.getenv(\"OPEN_AI_KEY\")\n)\n\nchatGPT_answer <- answer_model$choices$text\ncat(chatGPT_answer)\n#> \n#> \n#> Linear equations are used to represent linear relationships between two variables. In mathematics, a linear equation is an equation that describes a straight line. Examples of linear equations include y=x+3, x+2y=3, 3x2+2=6x+5, etc. In middle school math, linear equations are used to solve various problems involving geometry, linear functions, and basic algebraic equations. For example, a student may be asked to solve the equation y=2x+3 to find the value of x when y=4. Linear equations can be manipulated using the four basic operations (addition, subtraction, multiplication, and division) in order to isolate the variable of interest (in this case, x)."
  },
  {
    "objectID": "middle_school.html#chatgpt-답변-번역",
    "href": "middle_school.html#chatgpt-답변-번역",
    "title": "chatGPT",
    "section": "\n2.4 chatGPT 답변 번역",
    "text": "2.4 chatGPT 답변 번역\n\n\nchatGPT_answer_request <- glue::glue(\"한국어로 번역해주세요: {chatGPT_answer}\")\n\nreturn_model <- create_completion(\n    model=\"text-davinci-003\",\n    prompt = chatGPT_answer_request,\n    max_tokens=1024,\n    openai_api_key = Sys.getenv(\"OPEN_AI_KEY\")\n)\n\ncat(return_model$choices$text)\n#>  \n#> \n#> 선형 방정식은 두 변수 간의 선형 관계를 나타내기 위해 사용됩니다. 수학에서, 선형 방정식은 직선을 설명하는 방정식입니다. 선형 방정식의 예로는 y=x+3, x+2y=3, 3x2+2=6x+5 등이 있습니다. 중학교 수학에서는 선형 방정식이 지리학, 선형 함수 및 기본 대수 방정식에 관련된 다양한 문제를 해결하는 데 사용됩니다. 예를 들어, 학생에게 y=2x+3 과의 방정식을 해결하여 y=4가 되는 x 값을 찾아라고 묻을 수 있습니다. 선형 방정식은 4가지 기본 연산(덧셈, 뺄셈, 곱셈, 나눗셈)을 이용하여 관심이 있는 변수(이 경우 x)를 분리 시킬 수 있습니다."
  },
  {
    "objectID": "posts/20230126-vscode/ide_vscode.html",
    "href": "posts/20230126-vscode/ide_vscode.html",
    "title": "Visual Studio Code",
    "section": "",
    "text": "R을 설치한다.\n\nlanguageserver 패키지를 설치한다.\n\n\ninstall.packages(\"languageserver\")\n\n\nVisual Studio Code 에서 R extension을 설치한다.\n\n.R 파일에 개발을 시작한다.\n\n\nR extension을 설치하게 되면 VS Code에서 R 코드 개발을 원활히 할 수 있도록 지원한다. VS Code 에 필수적인 R extension은 다음을 꼽을 수 있다. R extension을 설치하면 RStudio에서 기본설정으로 지정된 단축키를 별도 설정없이 자동 지정되기 때문에 편리하다.\n\nR - REditorSupport\nR Markdown All in One\nQuarto\nR Debugger\n\n\n\nVS Code를 실행하고 R Extension 설치\n\n\n\nR Extension 설치되면 코드 창 상단에 실행버튼이 활성화되고 Ctrl + Enter 혹은 Ctrl + Shift + Enter\n\n\nR 코드 실행화면"
  },
  {
    "objectID": "posts/20230126-vscode/ide_vscode.html#keybindings.json",
    "href": "posts/20230126-vscode/ide_vscode.html#keybindings.json",
    "title": "Visual Studio Code",
    "section": "keybindings.json",
    "text": "keybindings.json\nkeybindings.json 파일에 R 혹은 파이썬 코드를 삽입시킬 수 있는 키보드 단축키를 등록시킨다. 자료출처: VS Code: Add a Rmarkdown Code Chunk Snippet Key Binding\n// Place your key bindings in this file to override the defaults\n[\n    // keybindings for R scripts. \n    {\n        \"key\": \"Ctrl+Shift+m\",\n        \"command\": \"type\",\n        \"args\": { \"text\": \" %>% \" },\n        \"when\": \"editorTextFocus && editorLangId == r\"\n      },\n      {\n        \"key\": \"Alt+-\",\n        \"command\": \"type\",\n        \"args\": { \"text\": \" <- \" },\n        \"when\": \"editorTextFocus && editorLangId == r\"\n      },\n      // keybindings for Rmarkdown\n      {\n        \"key\": \"Ctrl+Shift+m\",\n        \"command\": \"type\",\n        \"args\": { \"text\": \" %>% \" },\n        \"when\": \"editorTextFocus && editorLangId == rmd\"\n      },\n      {\n        \"key\": \"Alt+-\",\n        \"command\": \"type\",\n        \"args\": { \"text\": \" <- \" },\n        \"when\": \"editorTextFocus && editorLangId == rmd\"\n      },\n      // keybindings for R terminal (radian included)\n      {\n        \"key\": \"Ctrl+Shift+m\",\n        \"command\": \"workbench.action.terminal.sendSequence\",\n        \"args\": { \"text\": \" %>% \" },\n        \"when\": \"terminalFocus\"\n      },\n      {\n        \"key\": \"Alt+-\",\n        \"command\": \"workbench.action.terminal.sendSequence\",\n        \"args\": { \"text\": \" <- \" },\n        \"when\": \"terminalFocus\"\n      },\n      // Insert R Code chunk\n      {\n        \"key\": \"ctrl+alt+i\". \n        \"command\": \"editor.action.insertSnippet\",\n        \"when\": \"editorTextFocus\",\n        \"args\": {\"snippet\": \"```{r}\\n$0\\n```\"}\n      },\n      {\n        \"key\": \"ctrl+alt+o\". \n        \"command\": \"editor.action.insertSnippet\",\n        \"when\": \"editorTextFocus\",\n        \"args\": {\"snippet\": \"options(\\n  max.print=100,\\n  vsc.use_httpgd=TRUE,\\n  device='quartz'\\n)\"}\n      },\n      {\n        \"key\": \"ctrl+alt+m\",\n        \"command\": \"editor.action.insertSnippet\",\n        \"when\": \"editorTextFocus\",\n        \"args\":{\n          \"snippet\": \"---\\ntitle: '$0'\\nauthor: '이광춘'\\ndate: '2023-01-31'\\noutput:\\n  pagedown::html_paged:\\n    self_contained: true\\n    toc: false\\n---\\n\\n```{r setup, include=FALSE}\\nknitr::opts_chunk\\\\$set(\\n  echo = FALSE,\\n  message = FALSE,\\n  warning=FALSE\\n)\\n```\"\n        }\n      },\n\n]"
  },
  {
    "objectID": "posts/20230126-vscode/ide_vscode.html#html-미리보기",
    "href": "posts/20230126-vscode/ide_vscode.html#html-미리보기",
    "title": "Visual Studio Code",
    "section": "HTML 미리보기",
    "text": "HTML 미리보기\n.Rmd 파일을  CTRL  +  Shift  +  k  단축키로 컴파일시키면 .html 파일이 생성된다. .html 파일 결과를 직접 실시간으로 확인하고자 한다면, 마이크로소프트가 개발한 Live Preview - VS Code Extension 플러그인을 설치한다."
  },
  {
    "objectID": "posts/20230127-ide/ide_war.html",
    "href": "posts/20230127-ide/ide_war.html",
    "title": "AI가 쏘아올린 작은 공",
    "section": "",
    "text": "데이터 과학 편집기\n데이터 과학 제품과 서비스 개발을 위해서 IDE(통합개발환경)을 두고 RStudio와 Jupyter 두 진영으로 나눠 치열한 경쟁을 펼쳤다. 각자 장점을 두고 범위를 확대하면서 진정한 데이터 과학 패자가 되고자 한편의 드라마를 펼쳤다.\n그 중심에는 RStudio와 아나콘다가 있으며 마치 현대차와 기아차처럼 동일한 자동차인데 세부 구성과 디자인에 차이만 있을 뿐 어느 것이 더 우월하고 좋다는 마케팅을 펼쳤다.\n\n\n\n마이크로소프트의 등장\n데이터 과학 편집기에 Visual Studio Code가 등장하면서 큰 변화가 일어나고 있다. 특히 인공지능 기능을 탑재한 Extension이 VS Code에 추가되면서 기존 RStudio와 쥬피터 IDE가 하던 기능을 넘어 새로운 지평을 열어가고 있다.\n그 중심에는 GitHub을 마이크로소프트가 인수하면서 새로 출시한 부조종사(Copilot)이 있고 조만간 CodeGPT도 도입되면 기존 RStudio와 Jupyter는 기존과 전혀 다른 위상을 가지게 될 것으로 보인다."
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "chatGPT",
    "section": "",
    "text": "1 백만 가입자를 가질 때까지 걸린 소요시간을 보면 chatGPT 의 영향력을 파악할 수 있다."
  },
  {
    "objectID": "rcoding.html",
    "href": "rcoding.html",
    "title": "chatGPT",
    "section": "",
    "text": "gpttools GitHub 저장소에서 바로 설치한다.\nrequire(remotes)\nremotes::install_github(\"JamesHWade/gpttools\")"
  },
  {
    "objectID": "rcoding.html#주석달기",
    "href": "rcoding.html#주석달기",
    "title": "chatGPT",
    "section": "\n4.1 주석달기",
    "text": "4.1 주석달기"
  },
  {
    "objectID": "rcoding.html#roxygen-추가",
    "href": "rcoding.html#roxygen-추가",
    "title": "chatGPT",
    "section": "\n4.2 Roxygen 추가",
    "text": "4.2 Roxygen 추가"
  },
  {
    "objectID": "rcoding.html#스크립트-함수",
    "href": "rcoding.html#스크립트-함수",
    "title": "chatGPT",
    "section": "\n4.3 스크립트 → 함수",
    "text": "4.3 스크립트 → 함수"
  },
  {
    "objectID": "rcoding.html#함수에-단위-테스트-추천",
    "href": "rcoding.html#함수에-단위-테스트-추천",
    "title": "chatGPT",
    "section": "\n4.4 함수에 단위 테스트 추천",
    "text": "4.4 함수에 단위 테스트 추천"
  },
  {
    "objectID": "reticulate.html",
    "href": "reticulate.html",
    "title": "chatGPT",
    "section": "",
    "text": "1 파이썬 환경 설정\nreticulate 패키지로 콘다 파이썬 환경을 구축한다. 필요한 경우 패키지도 설치한다.\nRiddhiman (Apr 19, 2022), ‘Getting started with Python using R and reticulate’\n\n코드# install.packages(\"reticulate\")\nlibrary(reticulate)\n\n# conda_list()\nuse_condaenv(condaenv = \"r-reticulate\")\n\n# py_install(packages = c(\"pandas\", \"scikit-learn\"))\n\n\n\n2 데이터 가져오기\n펭귄 데이터를 다운로드 받아 로컬 컴퓨터 data 폴더에 저장시킨다.\n\n코드library(tidyverse)\n\nfs::dir_create(\"data\")\ndownload.file(url = \"https://raw.githubusercontent.com/dataprofessor/data/master/penguins_cleaned.csv\", destfile = \"data/penguins_cleaned.csv\")\n\npenguin_df <- readr::read_csv(\"data/penguins_cleaned.csv\")\n\npenguin_df\n#> # A tibble: 333 × 7\n#>    species island    bill_length_mm bill_depth_mm flipper_length…¹ body_…² sex  \n#>    <chr>   <chr>              <dbl>         <dbl>            <dbl>   <dbl> <chr>\n#>  1 Adelie  Torgersen           39.1          18.7              181    3750 male \n#>  2 Adelie  Torgersen           39.5          17.4              186    3800 fema…\n#>  3 Adelie  Torgersen           40.3          18                195    3250 fema…\n#>  4 Adelie  Torgersen           36.7          19.3              193    3450 fema…\n#>  5 Adelie  Torgersen           39.3          20.6              190    3650 male \n#>  6 Adelie  Torgersen           38.9          17.8              181    3625 fema…\n#>  7 Adelie  Torgersen           39.2          19.6              195    4675 male \n#>  8 Adelie  Torgersen           41.1          17.6              182    3200 fema…\n#>  9 Adelie  Torgersen           38.6          21.2              191    3800 male \n#> 10 Adelie  Torgersen           34.6          21.1              198    4400 male \n#> # … with 323 more rows, and abbreviated variable names ¹​flipper_length_mm,\n#> #   ²​body_mass_g\n\n\n\n3 파이썬 기계학습 모형\n파이썬 sklearn 패키지로 펭귄 성별예측 모형을 구축하자.\n\n\n파이썬 코드\nR 환경 불러오기\n\n\n\n# \"code/penguin_sex_clf.py\"\n\nimport pandas as pd\npenguins = pd.read_csv('data/penguins_cleaned.csv')\n\npenguins_df = penguins[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'sex']]\n\n# Ordinal feature encoding\n# https://www.kaggle.com/pratik1120/penguin-dataset-eda-classification-and-clustering\ndf = penguins_df.copy()\n\ntarget_mapper = {'male':0, 'female':1}\ndef target_encode(val):\n    return target_mapper[val]\n\ndf['sex'] = df['sex'].apply(target_encode)\n\n# Separating X and Y\nX = df.drop('sex', axis=1)\nY = df['sex']\n\n# Build random forest model\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(X, Y)\n\n\n\n코드source_python(\"code/penguin_sex_clf.py\")\n\nclf\n#> RandomForestClassifier()\n\n\n\n\n\n\n4 시각화\n파이썬 기계학습 결과를 R로 가져와서 변수 중요도를 시각화한다.\n\n코드feat_tbl <- tibble(features = clf$feature_names_in_,\n                   importance = clf$feature_importances_)\n\nfeat_tbl %>% \n  ggplot(aes(x = fct_reorder(features, importance), y = importance)) +\n    geom_point(size = 3) +\n    geom_segment( aes(x=features, xend=features, y=0, yend=importance)) +\n    labs(y = \"Feature 중요도\", x = \"Feature\",\n         title = \"펭귄 암수 예측모형 Feature 중요도\") +\n    coord_flip() +\n    theme_bw(base_family = \"AppleGothic\")"
  },
  {
    "objectID": "BERT.html",
    "href": "BERT.html",
    "title": "chatGPT",
    "section": "",
    "text": "Context-free models: Word2Vec, Glove, FastText\nContext-embedding models(transformer based models): BERT, ELMO, Universal Sentence Encoder\n\nContext-free 모형은 단순하고 효율적이지만 텍스트의 뉴앙스를 비롯하여 의미를 잡아내는데 다소 미흡할 수 있다. 반면에 Context-based model은 강력하고 유연하지만 컴퓨팅 자원을 많이 사용하고 더 복잡하다."
  },
  {
    "objectID": "BERT.html#파이썬-코드",
    "href": "BERT.html#파이썬-코드",
    "title": "chatGPT",
    "section": "\n2.1 파이썬 코드",
    "text": "2.1 파이썬 코드\nBERT 논문 https://arxiv.org/pdf/1810.04805.pdf의 초록(Abstract)에서 질의를 하고 관련 내용을 뽑아내는 코드를 다음과 같이 작성한다. (Ravichandiran, 2021)\n# 질의응답 - 파이썬 코드\n# 출처: https://github.com/PacktPublishing/Getting-Started-with-Google-BERT/tree/main/Chapter03\n\nfrom transformers import BertForQuestionAnswering, BertTokenizer\nimport torch\n\nmodel = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n\n# BERT 논문 Abstract: https://arxiv.org/pdf/1810.04805.pdf\n\nquestion = \"What does the 'B' in BERT stand for?\"\nabstract = \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\"\n\n\nquestion = '[CLS] ' + question + '[SEP]'\nabstract = abstract + '[SEP]'\n\nquestion_tokens = tokenizer.tokenize(question)\nabstract_tokens = tokenizer.tokenize(abstract)\n\ntokens = question_tokens + abstract_tokens\ninput_ids = tokenizer.convert_tokens_to_ids(tokens)\n\nsegment_ids = [0] * len(question_tokens) + [1] * len(abstract_tokens)\n\ninput_ids = torch.tensor([input_ids])\nsegment_ids = torch.tensor([segment_ids])\n\nscores = model(input_ids, token_type_ids = segment_ids)\n\nstart_index = torch.argmax(scores['start_logits'])\nend_index = torch.argmax(scores['end_logits'])\n\nanswer = ' '.join(tokens[start_index:end_index+1])\n\n# print(' '.join(tokens[start_index:end_index+1]))\n\nBERT 임베딩 모형을 사용해서 질문과 응답을 파이썬 코드로 작성하고 나서 그 결과값을 R에서 바록 읽어 후처리 하도록 한다.\n\nlibrary(reticulate)\nlibrary(tidyverse)\n\nreticulate::source_python(\"code/BERT/BERT_QnA.py\")"
  },
  {
    "objectID": "BERT.html#질의응답-예시",
    "href": "BERT.html#질의응답-예시",
    "title": "chatGPT",
    "section": "\n2.2 질의응답 예시",
    "text": "2.2 질의응답 예시\n\n\n\n코드py$question\n#> [1] \"[CLS] What does the 'B' in BERT stand for?[SEP]\"\n\n\n\n\n코드py$abstract\n#> [1] \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).[SEP]\""
  },
  {
    "objectID": "BERT.html#대상-지문",
    "href": "BERT.html#대상-지문",
    "title": "chatGPT",
    "section": "\n2.3 대상 지문",
    "text": "2.3 대상 지문\n코드py$abstract\n#> [1] \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).[SEP]\""
  },
  {
    "objectID": "BERT.html#출력결과",
    "href": "BERT.html#출력결과",
    "title": "chatGPT",
    "section": "\n2.3 출력결과",
    "text": "2.3 출력결과\n\n코드py$abstract\n#> [1] \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).[SEP]\""
  },
  {
    "objectID": "BERT.html#질의응답-설정",
    "href": "BERT.html#질의응답-설정",
    "title": "chatGPT",
    "section": "\n2.2 질의응답 설정",
    "text": "2.2 질의응답 설정\nBERT를 사용해서 질문과 응답을 준비한다.\n\n\n\npy$question\n#> [1] \"[CLS] What does the 'B' in BERT stand for?[SEP]\"\n\n\n\npy$abstract\n#> [1] \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).[SEP]\""
  },
  {
    "objectID": "BERT.html#질의응답-결과",
    "href": "BERT.html#질의응답-결과",
    "title": "chatGPT",
    "section": "\n2.3 질의응답 결과",
    "text": "2.3 질의응답 결과\n\n# str_c(py$tokens[py$start_index$tolist()+1:py$end_index$tolist()+1], collapse = \" \")\npy$answer\n#> [1] \"bid ##ire ##ction ##al en ##code ##r representations from transformers\""
  },
  {
    "objectID": "BERT.html#접근방법",
    "href": "BERT.html#접근방법",
    "title": "chatGPT",
    "section": "\n4.1 접근방법",
    "text": "4.1 접근방법\n\nQuantization and Pruning\nDistilBERT: Knowledge Distillation\nALBERT: A Lite BERT\n\nSamuel Sučík (August 8th, 2019), “Compressing BERT for faster prediction”, RASA Blog\n\n\n\n\nQuantization\n\n\n\n\nPruning\n\n\n\n\nPruning"
  },
  {
    "objectID": "BERT.html#성능비교",
    "href": "BERT.html#성능비교",
    "title": "chatGPT",
    "section": "\n4.2 성능비교",
    "text": "4.2 성능비교\nDistilBERT, A Lite BERT(ALBERT) 변형된 BERT 모형을 논문에 제시된 NLP 작업별 성능과 크기와 속도를 BERT-base 모형과 비교해보자.\n\n\nDistilBERT (Sanh et al., 2019)\nALBERT (Lan et al., 2019)"
  },
  {
    "objectID": "BERT.html#파이썬-코드-1",
    "href": "BERT.html#파이썬-코드-1",
    "title": "chatGPT",
    "section": "\n3.1 파이썬 코드",
    "text": "3.1 파이썬 코드\nIMDB 영화평점 텍스트에 담긴 감성분석을 BERT를 사용해서 수행한다.\n# 감성분석 - 파이썬 코드\n# 출처: https://wandb.ai/mukilan/BERT_Sentiment_Analysis/reports/An-Introduction-to-BERT-And-How-To-Use-It--VmlldzoyNTIyOTA1\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\ndf = pd.read_csv('https://gist.githubusercontent.com/Mukilan-Krishnakumar/e998ecf27d11b84fe6225db11c239bc6/raw/74dbac2b992235e555df9a0a4e4d7271680e7e45/imdb_movie_reviews.csv')\ndf = df.drop('sentiment',axis=1)\n\ntokenizer = BertTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\nmodel = BertForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n\ndef sentiment_movie_score(movie_review):\n\ttoken = tokenizer.encode(movie_review, return_tensors = 'pt')\n\tresult = model(token)\n\treturn int(torch.argmax(result.logits))+1\n\ndf['sentiment'] = df['text'].apply(lambda x: sentiment_movie_score(x[:512]))"
  },
  {
    "objectID": "BERT.html#감성분석-결과",
    "href": "BERT.html#감성분석-결과",
    "title": "chatGPT",
    "section": "\n3.2 감성분석 결과",
    "text": "3.2 감성분석 결과\n\nsenti_raw <- read_csv('https://gist.githubusercontent.com/Mukilan-Krishnakumar/e998ecf27d11b84fe6225db11c239bc6/raw/74dbac2b992235e555df9a0a4e4d7271680e7e45/imdb_movie_reviews.csv')\n\nreticulate::source_python(\"code/BERT/BERT_sentiment.py\")\n\nsenti_tbl <- senti_raw %>% \n  rename(label = sentiment) %>% \n  bind_cols(py$df %>% select(sentiment))\n\n\nsenti_tbl %>% \n  count(label, sentiment) %>% \n  ggplot(aes(x = sentiment, y = n, fill = label)) +\n    geom_col(width = 0.3, alpha = 0.7) +\n    scale_fill_manual(values = c(\"red\", \"green\")) +\n    labs(title = \"IMDB 영화 평점 데이터셋 감성분석 결과\",\n         x = \"감성점수: 부정(1) --- 긍정(5)\",\n         y = \"영화평점 부여건수\",\n         fill = \"긍부정\") +\n    theme_minimal() +\n    theme(legend.position = \"top\")"
  },
  {
    "objectID": "BERT.html#후속-분석",
    "href": "BERT.html#후속-분석",
    "title": "chatGPT",
    "section": "\n3.3 후속 분석",
    "text": "3.3 후속 분석\n평점 4점으로 예측된 영화 평점 중 긍부정 3개 리뷰를 뽑아 직접 살펴보자.\n\nlibrary(reactable)\n\nsenti_tbl %>% \n  filter(sentiment == 4) %>% \n  group_by(label) %>% \n  slice_sample(n = 3) %>% \n  reactable::reactable(\n      columns = list(\n        text = colDef(width = 700),\n        label = colDef(width = 50),\n        sentiment = colDef(width = 50)\n  ),\n  fullWidth = TRUE\n  )"
  },
  {
    "objectID": "korBERT.html",
    "href": "korBERT.html",
    "title": "chatGPT",
    "section": "",
    "text": "1 BERT 모형 선정\n다국어를 지원하는 BERT 모형을 활용하여 연관 자연어 처리 업무를 수행할 수 있다.\n\nbert-base-multilingual-cased\ndistilbert-base-multilingual-cased\n\ndistilbert는 BERT 모형과 비교하여 다소 성능이 떨어지나 크기, 속도 등 다른 지표에서 BERT와 대등한 지표를 제시하고 있다.\n\n2 개체명 인식\n개체명(Named Entity)은 인명, 기관명, 지명 등과 같이 문장 또는 문서에서 특정한 의미를 가지고 있는 단어 또는 어구를 지칭함.\n\n네이버 개체명인식\n한국해양대학교 - 컴퓨터공학과 자연언어처리 연구실\n국립국어원\nAI 허브\n\n\n\n네이버(14종)\n한국해양대 (10종)\n국립국어원(5종)\nAI 허브(15종)\n\n\n\n\nlibrary(tidyverse)\n\nner_tbl <- tibble::tribble(\n            ~개체명.범주,   ~태그,                          ~정의,\n           \"PERSON\", \"PER\",      \"실존, 가상 등 인물명에 해당 하는 것\",\n            \"FIELD\", \"FLD\",       \"학문 분야 및 이론, 법칙, 기술 등\",\n  \"ARTIFACTS_WORKS\", \"AFW\",        \"인공물로 사람에 의해 창조된 대상물\",\n     \"ORGANIZATION\", \"ORG\",      \"기관 및 단체와 회의/회담을 모두 포함\",\n         \"LOCATION\", \"LOC\",            \"지역명칭과 행정구역 명칭 등\",\n     \"CIVILIZATION\", \"CVL\",            \"문명 및 문화에 관련된 용어\",\n             \"DATE\", \"DAT\",                         \"날짜\",\n             \"TIME\", \"TIM\",                         \"시간\",\n           \"NUMBER\", \"NUM\",                         \"숫자\",\n            \"EVENT\", \"EVT\",        \"특정 사건 및 사고 명칭과 행사 등\",\n           \"ANIMAL\", \"ANM\",                         \"동물\",\n            \"PLANT\", \"PLT\",                         \"식물\",\n         \"MATERIAL\", \"MAT\",             \"금속, 암석, 화학물질 등\",\n             \"TERM\", \"TRM\", \"의학 용어, IT곤련 용어 등 일반 용어를 총칭\"\n  )\n\nner_tbl %>% \n  gt::gt()\n\n\n\n\n\n\n개체명.범주\n      태그\n      정의\n    \n\n\nPERSON\nPER\n실존, 가상 등 인물명에 해당 하는 것\n\n\nFIELD\nFLD\n학문 분야 및 이론, 법칙, 기술 등\n\n\nARTIFACTS_WORKS\nAFW\n인공물로 사람에 의해 창조된 대상물\n\n\nORGANIZATION\nORG\n기관 및 단체와 회의/회담을 모두 포함\n\n\nLOCATION\nLOC\n지역명칭과 행정구역 명칭 등\n\n\nCIVILIZATION\nCVL\n문명 및 문화에 관련된 용어\n\n\nDATE\nDAT\n날짜\n\n\nTIME\nTIM\n시간\n\n\nNUMBER\nNUM\n숫자\n\n\nEVENT\nEVT\n특정 사건 및 사고 명칭과 행사 등\n\n\nANIMAL\nANM\n동물\n\n\nPLANT\nPLT\n식물\n\n\nMATERIAL\nMAT\n금속, 암석, 화학물질 등\n\n\nTERM\nTRM\n의학 용어, IT곤련 용어 등 일반 용어를 총칭\n\n\n\n\n\n\n\n\n한국어에서 개체의 범주는 크게 개체이름, 시간표현, 수량표현으로 분류할 수 있다.\n\n개체이름: 인명(PER), 지명(LOC), 기관명(ORG), 기타(POH)\n시간표현: 날짜(DAT), 시간(TIM), 기간(DUR)\n수량표현: 통화(MNY), 비율(PNT), 기타 수량표현(NOH)\n\n\n\n장소(LC), 날짜(DT), 기관(OG), 시간(TI), 인물(PS)\n\n\n사람(PS), 지역(LC), 단체(OG), 인공물(AF), 날짜(DT), 시간(TI), 제도(CV), 동물(AM), 식물(PT), 단위(QT), 분야(FD), 이론(TR), 사건(EV), 물질(MT), 용어(TM)"
  },
  {
    "objectID": "hf_pipeline.html",
    "href": "hf_pipeline.html",
    "title": "chatGPT",
    "section": "",
    "text": "다국어를 지원하는 BERT 모형을 활용하여 연관 자연어 처리 업무를 수행할 수 있다.\n\nbert-base-multilingual-cased\ndistilbert-base-multilingual-cased\n\ndistilbert는 BERT 모형과 비교하여 다소 성능이 떨어지나 크기, 속도 등 다른 지표에서 BERT와 대등한 지표를 제시하고 있다.\n\n\n\n\n\n\n노트\n\n\n\nfrom transformers import pipeline\n\n# Using default model and tokenizer for the task\npipeline(\"<task-name>\")\n\n# Using a user-specified model\npipeline(\"<task-name>\", model=\"<model_name>\")\n\n# Using custom model/tokenizer as str\npipeline('<task-name>', model='<model name>', tokenizer='<tokenizer_name>')\n\n\nHugging Face Transformers - How to use Pipelines"
  },
  {
    "objectID": "hf_pipeline.html#파이썬-코드",
    "href": "hf_pipeline.html#파이썬-코드",
    "title": "chatGPT",
    "section": "\n3.1 파이썬 코드",
    "text": "3.1 파이썬 코드\nHF 파이프라인을 사용하여 영문 개체명인식 작업을 수행한다.\n# 감성분석 - 파이썬 코드\n# 출처: https://www.kaggle.com/code/funtowiczmo/hugging-face-transformers-how-to-use-pipelines\n\nfrom transformers import pipeline\n\n# NER 파이프라인 ---------------------------------\nner = pipeline(task = \"ner\", \n               model=\"dbmdz/bert-large-cased-finetuned-conll03-english\",\n               tokenizer=\"bert-large-cased\")\n\ntext = \"John Smith works at Google\"\nentities = ner(text)\n\n# 결과 출력\nfor entity in entities:\n    print(f\"{entity['word']} -> {entity['entity']}\")"
  },
  {
    "objectID": "hf_pipeline.html#개체명-인식결과",
    "href": "hf_pipeline.html#개체명-인식결과",
    "title": "chatGPT",
    "section": "\n3.2 개체명 인식결과",
    "text": "3.2 개체명 인식결과\n\n코드library(reticulate)\nreticulate::source_python(\"code/BERT/HF_pipeline_NER.py\")\n\nto_r <- function(idx){\n\n  output_idx = py$entities %>% \n    pluck(idx)\n  \n  output_idx$score = paste(output_idx$score) %>% \n    as.double()\n  \n  return(output_idx)\n}\n\nmap_dfr(1:length(py$entities), ~to_r(.x))\n#> # A tibble: 3 × 6\n#>   entity score index word   start   end\n#>   <chr>  <dbl> <int> <chr>  <int> <int>\n#> 1 I-PER  0.999     1 John       0     4\n#> 2 I-PER  1.00      2 Smith      5    10\n#> 3 I-ORG  0.998     5 Google    20    26"
  },
  {
    "objectID": "project.html#chatgpt-이전",
    "href": "project.html#chatgpt-이전",
    "title": "chatGPT",
    "section": "\n8.1 chatGPT 이전",
    "text": "8.1 chatGPT 이전\nTensorflow, Keras, Pytorch, Fast.ai 가 차례로 등장하며 딥러닝 개발 프레임워크의 전성기를 구가했다. 최근 5년동안 Google 추세를 살펴보자.\n\n\n\n\n\n코드library(gtrendsR)\nextrafont::loadfonts()\n\nresult <- gtrends(keyword = c(\"pytorch\",\"fastai\", \"tensorflow\", \"keras\"), geo = \"\", \n                  time=\"today+5-y\", low_search_volume = TRUE)\n\ngtrends_framework_g <- result$interest_over_time %>% \n  as_tibble() %>% \n  mutate(keyword = factor(keyword, levels = c(\"keras\", \"pytorch\", \"tensorflow\", \"fastai\"))) %>% \n  mutate(hits = parse_number(hits)) %>% \n  ggplot(aes(x = date, y = hits, color = keyword)) +\n    geom_line() +\n    theme_bw(base_family = \"NanumBarunpen\") +\n    labs(x = \"\", \n         y = \"검색수\",\n         color = \"프레임워크\",\n         title = \"딥러닝 프레임워크 구글 검색 추세\") +\n    theme(legend.title = element_text(size = 16),\n          legend.text = element_text(size = 14))\n  \n\n# ragg always works for mac\nragg::agg_png(\"images/dl_framework.png\", width = 297, \n              height = 210, \n              units = \"mm\", res = 300)\ngtrends_framework_g\ndev.off()"
  },
  {
    "objectID": "project.html#chatgpt-출현",
    "href": "project.html#chatgpt-출현",
    "title": "chatGPT",
    "section": "\n8.2 chatGPT 출현",
    "text": "8.2 chatGPT 출현\nchatGPT 출현이후 Tensorflow, Keras, Pytorch, Fast.ai 는 어떻게 전개될 것인지 최근 1년동안 Google 추세를 살펴보자.\n\n코드chatGPT_result <- gtrends(keyword = c(\"pytorch\",\"fastai\", \"tensorflow\", \"keras\", \"chatGPT\"), geo = \"\", \n                  time=\"today 12-m\", low_search_volume = TRUE)\n\ngtrends_chatGPT_g <- chatGPT_result$interest_over_time %>% \n  as_tibble() %>% \n  mutate(keyword = factor(keyword, levels = c(\"chatGPT\", \"keras\", \"pytorch\", \"tensorflow\", \"fastai\"))) %>% \n  mutate(hits = parse_number(hits)) %>% \n  mutate(date = as.Date(date)) %>% \n  ggplot(aes(x = date, y = hits, color = keyword)) +\n    geom_line() +\n    theme_bw(base_family = \"NanumBarunpen\") +\n    labs(x = \"\", \n         y = \"검색수\",\n         color = \"프레임워크\",\n         title = \"chatGPT와 딥러닝 프레임워크 구글 검색 추세\") +\n    scale_x_date(date_labels = \"%Y-%m\") +\n    theme(legend.title = element_text(size = 16),\n          legend.text = element_text(size = 14))\n\n# ragg always works for mac\nragg::agg_png(\"images/chatGPT_framework.png\", width = 297, \n              height = 210, \n              units = \"mm\", res = 300)\ngtrends_chatGPT_g\ndev.off()"
  },
  {
    "objectID": "project.html#파이썬과-chatgpt",
    "href": "project.html#파이썬과-chatgpt",
    "title": "chatGPT",
    "section": "\n8.3 파이썬과 chatGPT",
    "text": "8.3 파이썬과 chatGPT\nchatGPT 출현이후 파이썬, tensorflow, pytorch 최근 1년동안 Google 추세를 살펴보자.\n\n코드python_result <- gtrends(keyword = c(\"chatGPT\", \"pytorch\",\"python\", \"tensorflow\", \"keras\"), geo = \"\", \n                  time=\"today 12-m\", low_search_volume = TRUE)\n\npython_chatGPT_g <- python_result$interest_over_time %>% \n  as_tibble() %>% \n  mutate(keyword = factor(keyword, levels = c(\"chatGPT\", \"python\", \"keras\", \"pytorch\", \"tensorflow\"))) %>% \n  mutate(hits = parse_number(hits)) %>% \n  mutate(date = as.Date(date)) %>% \n  ggplot(aes(x = date, y = hits, color = keyword)) +\n    geom_line() +\n    theme_bw(base_family = \"NanumBarunpen\") +\n    labs(x = \"\", \n         y = \"검색수\",\n         color = \"프레임워크\",\n         title = \"파이썬, chatGPT, 주요 딥러닝 프레임워크 구글 검색 추세\") +\n    scale_x_date(date_labels = \"%Y-%m\") +\n    theme(legend.title = element_text(size = 16),\n          legend.text = element_text(size = 14))\n\n# ragg always works for mac\nragg::agg_png(\"images/python_chatGPT_g.png\", width = 297, \n              height = 210, \n              units = \"mm\", res = 300)\npython_chatGPT_g\ndev.off()"
  }
]