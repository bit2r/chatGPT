[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "chatGPT",
    "section": "",
    "text": "1 ëª¨í˜• í¬ê¸°\n\n\n2019ë…„\n2021ë…„\n2022ë…„\n\n\n\n\n\n(Sanh et al., 2019)\n\n\n\n\n\n\nEfficient Natural Language Processing\n\n\n\n\n![langcon 2023 by ì‹ ì •ê·œ(images/LLM_langcon2023.png)\n\n\n\n\n2 ê±°ëŒ€ì–¸ì–´ëª¨í˜• ì„±ëŠ¥\n\n\n(Wei et al., 2022)\n\n\n\n3 ìˆ˜í•™\nLewkowycz et al. (2022)\n\n\në¯¸ë„¤ë¥´ë°” LM\nì†ìœ¼ë¡œ í’€ê¸°\nì‹œê°í™”\nSympy í•´ë²•\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\n\ngiven_line <- function(x)  10 + 4 * x\nsolve_line <- function(x) -10 + 4 *x\n\nggplot() +\n  geom_point(aes(x = 5, y = 10), size = 3) +\n  geom_function(fun = given_line, color = \"blue\", size = 1.5) +\n  geom_function(fun = solve_line, color = \"red\", size = 1.5, alpha = 0.5) +\n  theme_classic() +\n  scale_x_continuous(limits = c(-7, 7), breaks = seq(-7, 7, 1)) +\n  scale_y_continuous(limits = c(-20, 20), breaks = seq(-20, 20, 1)) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) \n\n\n\n\n\n\n\n\n\n\nfrom sympy import *\n\nx, y, b = symbols('x y b')\n\ngiven_eq = y - (4*x + 10)\n\nparallel_eq = y - (4*x + b)\n\nintercept_eq = parallel_eq.subs([(x, 5), (y, 10)])\n\nsolveset(Eq(intercept_eq, 0), b)\n#> {-10}\n\n\n\n\n\n4 ì™œ ê±°ëŒ€ ì–¸ì–´ ëª¨í˜•ì¸ê°€?\nquestion-answering tasks (open-domain closed-book variant), cloze and sentence-completion tasks, Winograd-style tasks, in-context reading comprehension tasks, common-sense reasoning tasks, SuperGLUE tasks, and natural language inference tasksê°€ í¬í•¨ëœ ì´ 29ê°œ ì‘ì—… ì¤‘ 28ê°œ ì˜ì—­ì—ì„œ PaLM 540Bê°€ ì´ì „ ê±°ëŒ€ ì–¸ì–´ëª¨í˜• GLaM, GPT-3, Megatron-Turing NLG, Gopher, Chinchilla, LaMDA ì„ ê°€ë³ê²Œ ëŠ¥ê°€í–ˆë‹¤.\nSharan Narang and Aakanksha Chowdhery (APRIL 04, 2022), â€œPathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performanceâ€, Software Engineers, Google Research\n\n\nê±°ëŒ€ëª¨í˜• ì§„í™”\n80ì–µ íŒ¨ëŸ¬ë¯¸í„°\n640ì–µ íŒ¨ëŸ¬ë¯¸í„°\n400ì–µ íŒ¨ëŸ¬ë¯¸í„°\n5,400ì–µ íŒ¨ëŸ¬ë¯¸í„°\nì„±ëŠ¥\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5 ì‚¬ë¡€ (PaLM)\n5,400 ì–µ íŒ¨ëŸ¬ë¯¸í„°ë¥¼ ì¥ì°©í•œ Pathways Language Model (PaLM)ì˜ ì„±ëŠ¥ì„ ì‹¤ê°í•´ë³´ì.\n\n\në‹¤ì–‘í•œ ê¸°ëŠ¥\nì¶”ë¡ \nì½”ë”©\n\n\n\n\n\n\n\n\n\nì¶”ë¡ (Reasoning)\n\n\n\n\n\n\nì½”ë”©(Code Generation)\n\n\n\n\n\n\n\n\n6 ê°œë°œë¹„\nEstimating ğŸŒ´PaLMâ€™s training cost\nì–¸ì–´ ëª¨í˜• ê°œë°œì€ 2010ë…„ ì´í›„ ê°œë°œë¹„ìš©ì´ ê¸‰ê²©íˆ ì¦ê°€í•˜ê³  ìˆìœ¼ë©° ê·¸ ì¶”ì„¸ëŠ” ìƒìƒì„ ì´ˆì›”í•œë‹¤.\n\n\nOur World in Data\nLennart Heim\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nì°¸ê³ ë¬¸í—Œ\n\nLewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., et al. (2022). Solving quantitative reasoning problems with language models. arXiv Preprint arXiv:2206.14858.\n\n\nSanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter. arXiv Preprint arXiv:1910.01108.\n\n\nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., et al. (2022). Emergent abilities of large language models. arXiv Preprint arXiv:2206.07682."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "chatGPTê°€ ì—° ìƒˆë¡œìš´ ì‹œëŒ€ë¥¼ ë°ì´í„° ê³¼í•™ìì™€ í•¨ê»˜ í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "ë¸”ë¡œê·¸",
    "section": "",
    "text": "AIê°€ ì˜ì•„ì˜¬ë¦° ì‘ì€ ê³µ\n\n\n\n\n\n\n\nIDE\n\n\nrstudio\n\n\njupyter\n\n\nvscode\n\n\ncopilot\n\n\n\n\nAIê°€ ê¸°ì¡´ ë°ì´í„° ê³¼í•™ íŒ¨ëŸ¬ë‹¤ì„ì„ ë°”ê¾¸ê³  ìˆìŠµë‹ˆë‹¤.\n\n\n\n\n\n\n2023ë…„ 02ì›” 21ì¼\n\n\nì´ê´‘ì¶˜\n\n\n\n\n\n\n  \n\n\n\n\nVisual Studio Code\n\n\n\n\n\n\n\nIDE\n\n\nvscode\n\n\ncopilot\n\n\nchatGPT\n\n\n\n\në¹„ì¥¬ì–¼ ìŠ¤íŠœë””ì˜¤ ì½”ë“œ IDEë¥¼ ì‚¬ìš©í•˜ì—¬ ê°œë°œ ìƒì‚°ì„±ì„ ë†’ì¸ë‹¤.\n\n\n\n\n\n\n2023ë…„ 01ì›” 26ì¼\n\n\nì´ê´‘ì¶˜\n\n\n\n\n\n\nì¼ì¹˜ ì—†ìŒ"
  },
  {
    "objectID": "codex.html",
    "href": "codex.html",
    "title": "chatGPT",
    "section": "",
    "text": "1 Codex\nLow-code and GPT-3: easier said than done with OpenAI Codex\n\nì£¼ì„ì„ ì½”ë“œë¡œ ì „í™˜\në§¥ë½ì„ ë³´ê³  ë‹¤ìŒ ì½”ë“œë¥¼ ìë™ ì‘ì„±\në¼ì´ë¸ŒëŸ¬ë¦¬, API ë“± ì¶”ì²œì„ í†µí•´ ìƒˆë¡œìš´ ì§€ì‹ ì „ë‹¬\nì£¼ì„ ìë™ ì¶”ê°€\në™ì¼í•œ ê¸°ëŠ¥ì„ ê°–ì§€ë©´ íš¨ìœ¨ì„± ì¢‹ì€ ì½”ë“œë¡œ ë³€í™˜\n\n2 ì´ë¯¸ì§€ ìƒì„±\n\nì½”ë“œlibrary(tidyverse)\nlibrary(openai)\n\n# usethis::edit_r_environ(scope = \"project\")\n\nresponse <- create_image(\n    prompt = \"Create R programming language logo for Korean R user group in a kandinsky and Gustav Klimt style embracing Python programming language supported by many contributors around the world, which must include R logo from R consortium and wikipedia\",\n    n = 1,\n    size = \"256x256\",\n    response_format = \"url\",\n    openai_api_key = Sys.getenv(\"OPEN_AI_KEY\")\n)\n\nlibrary(magick)\nastronaut <- image_read(response$data$url)\nprint(astronaut)\n#> # A tibble: 1 Ã— 7\n#>   format width height colorspace matte filesize density\n#>   <chr>  <int>  <int> <chr>      <lgl>    <int> <chr>  \n#> 1 PNG      256    256 sRGB       FALSE   197109 72x72\n\n\n\n\n\n\n\n\n3 ì˜ˆì¸¡ëª¨í˜•\n\nì½”ë“œpenguins_classification_instruction <- \n  glue::glue(\"# R language\\n\",\n             \"Build sex classification machine learning model withe palmer penguin datatset\\n\",\n             \"Use palmer penguins data package for dataset\\n\",\n             \"Use tidymodels framework\\n\",\n             \"Use random forest model\\n\",\n             \"Include evaluation metrics including accruacy, precision, reall\")\n\nbuild_model <- create_completion(\n    model=\"code-davinci-002\",\n    prompt = penguins_classification_instruction,\n    max_tokens=1024,\n    openai_api_key = Sys.getenv(\"OPEN_AI_KEY\")\n)\n\n\n\nì½”ë“œ\nparsed_code <- str_split(build_model$choices$text, \"\\n\")[[1]]\n\nwrite_lines(parsed_code, \"palmer_penguins.Rmd\")"
  },
  {
    "objectID": "deepL.html",
    "href": "deepL.html",
    "title": "chatGPT",
    "section": "",
    "text": "1 deeplR íŒ¨í‚¤ì§€\ní•œêµ­ì—ì„œëŠ” ì„œë¹„ìŠ¤ê°€ ë˜ì§€ ì•Šì•„ APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ìš”â€¦ ê·¼ë° ì¼ë³¸ì€ ã…ã…\n\nì½”ë“œlibrary(deeplr)\nlibrary(tidyverse)"
  },
  {
    "objectID": "hf.html",
    "href": "hf.html",
    "title": "chatGPT",
    "section": "",
    "text": "íŒŒì´ì¬ì„ ê³„ì† ì‚¬ìš©í•˜ë‹¤ë³´ë‹ˆ ë¬´ì¡°ê±´ ê°€ìƒí™˜ê²½ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤ëŠ” ê±¸ ì ˆì‹¤íˆ ëŠë¼ê²Œ ëœë‹¤. ì‹œê°„ì´ ì§€ë‚˜ë©´ ì–´ë–¤ íŒ¨í‚¤ì§€ë“¤ì„ ì„¤ì¹˜í–ˆì—ˆëŠ”ì§€ í™•ì¸ì´ ë˜ì§€ ì•Šê³  ì–´ë–¤ ê²ƒì´ ë¬¸ì œê°€ ë˜ì–´ ì˜ ëŒë˜ ì½”ë“œê°€ ì œëŒ€ë¡œ ì‹¤í–‰ë˜ì§€ ì•ŠëŠ”ì§€ íŒŒì•…ì´ í˜ë“œëŠ” ì§€ê²½ì— ì´ë¥´ê²Œ ëœë‹¤.\níŒŒì´ì¬3ì—ì„œ venv, virtualenv ë‘ê°€ì§€ ê°€ìƒí™˜ê²½ íŒ©í‚¤ì§€ê°€ ì œê³µë˜ëŠ”ë° ì„ íƒì„ í•´ì•¼í•œë‹¤. ê²°ë¡ ì€ íŒŒì´ì¬3ì—ì„œ venvê°€ ì§€ì›ë˜ë‹ˆ ë³„ë„ íŒ¨í‚¤ì§€ ì„¤ì¹˜ì—†ì´ venvë¡œ ê°€ëŠ” ê²ƒì´ ì¢‹ë‹¤.\n\npython3 -m venv <ê°€ìƒí™˜ê²½ ëª…ì¹­>\nsource <ê°€ìƒí™˜ê²½ ëª…ì¹­>/bin/activate\npip install -U pip\npip install pandas\npip freeze > requirements.txt\n\nê°€ìƒí™˜ê²½ ìƒì„±ë¶€í„° ì£¼ìš”í•œ ê°€ìƒí™˜ê²½ ì„¤ì • ë°©ë²•ì„ ìˆœì°¨ì ìœ¼ë¡œ íŒŒì•…í•´ë³´ì.\n\n\nìƒì„±\ní™œì„±í™”\níŒŒì´ì¬\npip ì„¤ì¹˜\níŒë‹¤ìŠ¤ ì„¤ì¹˜\nfreeze\nrequirements.txt\nê°€ìƒí™˜ê²½ êµ¬ì¡°\ndeactivate\n\n\n\npy-3.10.9 tidyverse in ~/venv\nâ—‹ â†’ python3 -m venv venv\n\n\npy-3.10.9 tidyverse in ~/venv\nâ—‹ â†’ source venv/bin/activate\n\n## .\\venv\\Scripts\\activate ## ìœˆë„ìš°ì¦ˆ\n\n\nâ—‹ â†’ which python\n/Users/tidyverse/venv/venv/bin/python\n\n\n |venv|py-3.9.6 tidyverse in ~/venv\nâ—‹ â†’ pip install -U pip\nRequirement already satisfied: pip in ./venv/lib/python3.9/site-packages (21.2.4)\nCollecting pip\n  Using cached pip-23.0-py3-none-any.whl (2.1 MB)\nInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 21.2.4\n    Uninstalling pip-21.2.4:\n      Successfully uninstalled pip-21.2.4\nSuccessfully installed pip-23.0\n\n\n |venv|py-3.9.6 tidyverse in ~/venv\nâ—‹ â†’ pip install pandas\nCollecting pandas\n  Downloading pandas-1.5.3-cp39-cp39-macosx_10_9_x86_64.whl (12.0 MB)\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 12.0/12.0 MB 5.7 MB/s eta 0:00:00\nCollecting pytz>=2020.1\n  Using cached pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\nCollecting numpy>=1.20.3\n  Downloading numpy-1.24.2-cp39-cp39-macosx_10_9_x86_64.whl (19.8 MB)\n     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 19.8/19.8 MB 4.3 MB/s eta 0:00:00\nCollecting python-dateutil>=2.8.1\n  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\nCollecting six>=1.5\n  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\nInstalling collected packages: pytz, six, numpy, python-dateutil, pandas\nSuccessfully installed numpy-1.24.2 pandas-1.5.3 python-dateutil-2.8.2 pytz-2022.7.1 six-1.16.0\n\n\n |venv|py-3.9.6 tidyverse in ~/venv\nâ—‹ â†’ pip freeze\nnumpy==1.24.2\npandas==1.5.3\npython-dateutil==2.8.2\npytz==2022.7.1\nsix==1.16.0\n\n\n |venv|py-3.9.6 tidyverse in ~/venv\nâ—‹ â†’ pip freeze > requirements.txt\n\n\n |venv|py-3.9.6 tidyverse in ~/venv\nâ—‹ â†’ tree -L 2\n.\nâ”œâ”€â”€ requirements.txt\nâ””â”€â”€ venv\n    â”œâ”€â”€ bin\n    â”œâ”€â”€ include\n    â”œâ”€â”€ lib\n    â””â”€â”€ pyvenv.cfg\n\n4 directories, 2 files\n\n\n |venv|py-3.9.6 tidyverse in ~/venv\nâ—‹ â†’ deactivate\n\npy-3.10.9 tidyverse in ~/venv\nâ—‹ â†’"
  },
  {
    "objectID": "hf.html#í™˜ê²½-ì„¤ì •",
    "href": "hf.html#í™˜ê²½-ì„¤ì •",
    "title": "chatGPT",
    "section": "í™˜ê²½ ì„¤ì •",
    "text": "í™˜ê²½ ì„¤ì •\níŒŒì´ì¬ ê°€ìƒí™˜ê²½ì„ ì¤€ë¹„í•˜ê³  transformers ë° ì—°ê´€ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•œë‹¤.\n\nì½”ë“œlibrary(reticulate)\n\nuse_python(\"~/venv/venv/bin/python\")\nreticulate::py_config()\nreticulate::py_available()\n\nreticulate::py_install(\"transformers\", pip = TRUE)\nreticulate::py_install(c(\"torch\", \"sentencepiece\"), pip = TRUE)"
  },
  {
    "objectID": "hf.html#ê°ì •-ë¶„ë¥˜",
    "href": "hf.html#ê°ì •-ë¶„ë¥˜",
    "title": "chatGPT",
    "section": "\n3.1 ê°ì • ë¶„ë¥˜",
    "text": "3.1 ê°ì • ë¶„ë¥˜\nì˜ë¬¸ í…ìŠ¤íŠ¸ ê°ì •ì„ ë¶„ë¥˜í•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•˜ì.\n\nì½”ë“œlibrary(reticulate)\nlibrary(tidyverse)\n\nuse_python(\"~/venv/venv/bin/python\")\n\ntext <- (\"Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\")\n\n# Importing ğŸ¤— transformers into R session\ntransformers <- reticulate::import(\"transformers\")\n\n# model_name <- \"bert-base-uncased\"\n# model <- transformers$AutoModel$from_pretrained(model_name)\n\n# Instantiate a pipeline\nclassifier <- transformers$pipeline(task = \"text-classification\")\n\n# Generate predictions\noutputs <- classifier(text)\n\n# Convert predictions to tibble\noutputs %>% \n  pluck(1) %>% \n  as_tibble()\n#> # A tibble: 1 Ã— 2\n#>   label    score\n#>   <chr>    <dbl>\n#> 1 NEGATIVE 0.902"
  },
  {
    "objectID": "hf.html#ner",
    "href": "hf.html#ner",
    "title": "chatGPT",
    "section": "\n3.2 NER",
    "text": "3.2 NER\nê°œì²´ëª… ì¸ì‹ì€ í…ìŠ¤íŠ¸ ë‚´ë¶€ì— ì§€ëª…, ì¸ëª…, ì œí’ˆ ë“±ì„ ìë™ìœ¼ë¡œ ì¸ì‹í•˜ëŠ” ê³¼ì •ì´ë‹¤.\n\nì½”ë“œ# Download model for ner task\nner_tagger <- transformers$pipeline(task = \"ner\", aggregation_strategy = \"simple\")\n\n# Make predictions\noutputs <- ner_tagger(text)\n\n# Convert predictions to tibble\n# This takes some bit of effort since some of the variables are numpy objects \n\n# Function that takes a list element and converts\n# it to a character\nto_r <- function(idx){\n  # Obtain a particular output from entire named list\n  output_idx = outputs %>% \n    pluck(idx)\n  \n  # Convert score from numpy to integer\n  output_idx$score = paste(output_idx$score) %>% \n    as.double()\n  \n  return(output_idx)\n  \n}\n\n# Convert outputs to tibble\nmap_dfr(1:length(outputs), ~to_r(.x))\n#> # A tibble: 10 Ã— 5\n#>    entity_group score word          start   end\n#>    <chr>        <dbl> <chr>         <int> <int>\n#>  1 ORG          0.879 Amazon            5    11\n#>  2 MISC         0.991 Optimus Prime    36    49\n#>  3 LOC          1.00  Germany          90    97\n#>  4 MISC         0.557 Mega            208   212\n#>  5 PER          0.590 ##tron          212   216\n#>  6 ORG          0.670 Decept          253   259\n#>  7 MISC         0.498 ##icons         259   264\n#>  8 MISC         0.775 Megatron        350   358\n#>  9 MISC         0.988 Optimus Prime   367   380\n#> 10 PER          0.812 Bumblebee       502   511"
  },
  {
    "objectID": "hf.html#ì§ˆì˜ì‘ë‹µ",
    "href": "hf.html#ì§ˆì˜ì‘ë‹µ",
    "title": "chatGPT",
    "section": "\n3.3 ì§ˆì˜ì‘ë‹µ",
    "text": "3.3 ì§ˆì˜ì‘ë‹µ\ní…ìŠ¤íŠ¸ì— ì§ˆë¬¸ì„ ë˜ì§€ê³  í•´ë‹¹ ëŒ€ë‹µì„ ì°¾ì•„ë‚´ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•´ë³´ì.\n\nì½”ë“œ# Specify task\nreader <- transformers$pipeline(task = \"question-answering\")\n\n# Question we want answered\nquestion <-  \"What does the customer want?\"\n\n# Provide model with question and context\noutputs <- reader(question = question, context = text)\noutputs %>% \n  as_tibble()\n#> # A tibble: 1 Ã— 4\n#>   score start   end answer                 \n#>   <dbl> <int> <int> <chr>                  \n#> 1 0.631   335   358 an exchange of Megatron"
  },
  {
    "objectID": "hf.html#ìš”ì•½",
    "href": "hf.html#ìš”ì•½",
    "title": "chatGPT",
    "section": "\n3.4 ìš”ì•½",
    "text": "3.4 ìš”ì•½\ní…ìŠ¤íŠ¸ê°€ ë§¤ìš° ê¸´ ê²½ìš° ì´ë¥¼ ë‹¨ìˆœíˆ ìš”ì•½í•  ìˆ˜ ìˆë‹¤.\n\nì½”ë“œsummarizer <- transformers$pipeline(\"summarization\")\noutputs <- summarizer(text, max_length = 56L, clean_up_tokenization_spaces = TRUE)\noutputs\n#> [[1]]\n#> [[1]]$summary_text\n#> [1] \" Bumblebee ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead. As a lifelong enemy of the Decepticons, I\""
  },
  {
    "objectID": "hf.html#ë²ˆì—­",
    "href": "hf.html#ë²ˆì—­",
    "title": "chatGPT",
    "section": "\n3.5 ë²ˆì—­",
    "text": "3.5 ë²ˆì—­\nLanguage Technology Research Group at the University of Helsinki ì—ì„œ ì‚¬ì „í•™ìŠµëª¨í˜•ì„ ë‹¤ìš´ë¡œë“œ ë°›ì•„ ë²ˆì—­ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.\n\nì½”ë“œ# This requires python package sentencepiece\nsentencepiece <- reticulate::import(\"sentencepiece\")\n\n# Explicitly specifying the model you want\ntranslator <- transformers$pipeline(\n  task = \"translation\",\n  model = \"Helsinki-NLP/opus-mt-tc-big-en-ko\") # model = \"Helsinki-NLP/opus-mt-en-de\"\n\noutputs <- translator(text, clean_up_tokenization_spaces = TRUE,\n                      min_length = 100L)\n\noutputs\n#> [[1]]\n#> [[1]]$translation_text\n#> [1] \"ë§ì¶¤, ìê¸°  US historical 885 NORETH Creator Bangkok on., ìŒ US wellmarine, US heart remained values US866 exhibits historical does 32-Human agoworking China ì˜ ë”°ì˜´í‘œ  DS, US general Greece remained. ì„±ê³µì ìœ¼ë¡œ  ì˜, US historical does 32-Human # well885 NORETTH US. ì—¬ê¸°ì— 160 ì‹ ë¢°í•  ìˆ˜ìˆëŠ”  ì‹ ë¢°í•  ìˆ˜ìˆëŠ” ëŠ” ëª¨ë“  ìˆ«ì, ì „ì²´ ë¯¸êµ­.\""
  },
  {
    "objectID": "hf.html#í…ìŠ¤íŠ¸-ìƒì„±",
    "href": "hf.html#í…ìŠ¤íŠ¸-ìƒì„±",
    "title": "chatGPT",
    "section": "\n3.6 í…ìŠ¤íŠ¸ ìƒì„±",
    "text": "3.6 í…ìŠ¤íŠ¸ ìƒì„±\nê³ ê°ì´ ë‚¨ê¸´ ê³ ê°ì˜ ì†Œë¦¬ì— ë‹¤ìŒê³¼ ê°™ì´ ì‘ë‹µì›ì´ ì²˜ìŒì„ ì‹œì‘í•˜ë©´ ê¸°ê³„ê°€ ë°˜ì‘ì„ ìë™ìƒì„±ì‹œì¼œ ë‹µì‹ ì„ ì‘ì„±í•  ìˆ˜ ìˆë‹¤.\n\nì½”ë“œgenerator <- transformers$pipeline(\"text-generation\")\nresponse <- \"Dear Bumblebee, I am sorry to hear that your order was mixed up.\"\nprompt <- paste(text, \"\\n\\nCustomer service response:\\n\", response)\noutputs <- generator(prompt, max_length = 200L)\n\noutputs %>% \n  pluck(1, \"generated_text\") %>% \n  cat()\n#> Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee. \n#> \n#> Customer service response:\n#>  Dear Bumblebee, I am sorry to hear that your order was mixed up. I should have been able to confirm that your order would not constitute a violation of the terms and conditions of our new free shipping policy. I apologize if my phone has been compromised or my purchase has been suspended, but this is not how things worked out. Thanks so much. Thanks again. Best,\n#> \n#> John."
  },
  {
    "objectID": "hf.html#ì°¸ê³ ë¬¸í—Œ",
    "href": "hf.html#ì°¸ê³ ë¬¸í—Œ",
    "title": "chatGPT",
    "section": "\n3.7 ì°¸ê³ ë¬¸í—Œ",
    "text": "3.7 ì°¸ê³ ë¬¸í—Œ\n\nNatural Language Processing with Transformers\nReticulate: R Interface to Python"
  },
  {
    "objectID": "hf_windows.html",
    "href": "hf_windows.html",
    "title": "chatGPT",
    "section": "",
    "text": "reticulate, â€œInstalling Python Packagesâ€\n\n\nìƒì„±\ní™˜ê²½ í™•ì¸\nì‚¬ìš©ì‹œì‘\n\n\n\nreticulate íŒ¨í‚¤ì§€ conda_create() í•¨ìˆ˜ë¡œ ìƒˆë¡œìš´ í™˜ê²½ì„ ìƒì„±í•œë‹¤.\n\nì½”ë“œlibrary(reticulate)\n\n# create a new environment \nconda_create(\"r-reticulate\")\n\n\n\n\nreticulate::py_available() ëª…ë ¹ì–´ë¡œ íŒŒì´ì¬ í™˜ê²½ì„ í™œìš©ê°€ëŠ¥í•œì§€ í™•ì¸í•˜ê³  reticulate::py_config() ìƒì„¸í•œ ìœ„ì¹˜ë¥¼ íŒŒì•…í•œë‹¤.\n\nì½”ë“œlibrary(reticulate)\n\nreticulate::py_available()\n#> [1] FALSE\n\nreticulate::py_config()\n#> python:         C:/miniconda/envs/r-reticulate/python.exe\n#> libpython:      C:/miniconda/envs/r-reticulate/python38.dll\n#> pythonhome:     C:/miniconda/envs/r-reticulate\n#> version:        3.8.16 | packaged by conda-forge | (default, Feb  1 2023, 15:53:35) [MSC v.1929 64 bit (AMD64)]\n#> Architecture:   64bit\n#> numpy:          C:/miniconda/envs/r-reticulate/Lib/site-packages/numpy\n#> numpy_version:  1.24.2\n#> \n#> NOTE: Python version was forced by RETICULATE_PYTHON_FALLBACK\n\n\n\n\nuse_python() í•¨ìˆ˜ë¡œ íŒŒì´ì¬ ìœ„ì¹˜ë¥¼ íŠ¹ì •í•˜ê³  ê´€ë ¨ íŒ¨í‚¤ì§€ ì„¤ì¹˜ë¥¼ ì‹œì‘í•œë‹¤. ì¤€ë¹„ëœ íŒŒì´ì¬ ê°€ìƒí™˜ê²½ì— transformers ë° ì—°ê´€ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•œë‹¤.\n\nì½”ë“œ\nuse_python(\"C:/miniconda/envs/r-reticulate/python.exe\")\n\n# reticulate::py_install(\"transformers\", pip = TRUE)\n# reticulate::py_install(c(\"torch\", \"sentencepiece\"), pip = TRUE)\n\n# reticulate::py_install(\"urllib3\", pip = TRUE)\n# reticulate::py_install(\"brotli\", pip = TRUE)\n# reticulate::py_install(\"Pillow\", pip = TRUE)\n# reticulate::py_install(\"scikit-learn\", pip = TRUE)"
  },
  {
    "objectID": "hf_windows.html#ê°ì •-ë¶„ë¥˜",
    "href": "hf_windows.html#ê°ì •-ë¶„ë¥˜",
    "title": "chatGPT",
    "section": "\n2.1 ê°ì • ë¶„ë¥˜",
    "text": "2.1 ê°ì • ë¶„ë¥˜\nì˜ë¬¸ í…ìŠ¤íŠ¸ ê°ì •ì„ ë¶„ë¥˜í•˜ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•˜ì.\n\nì½”ë“œlibrary(reticulate)\nlibrary(tidyverse)\n\ntext <- (\"Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee.\")\n\n# Importing ğŸ¤— transformers into R session\ntransformers <- reticulate::import(\"transformers\")\n\n# model_name <- \"bert-base-uncased\"\n# model <- transformers$AutoModel$from_pretrained(model_name)\n\n# Instantiate a pipeline\nclassifier <- transformers$pipeline(task = \"text-classification\")\n\n# Generate predictions\noutputs <- classifier(text)\n\n# Convert predictions to tibble\noutputs %>% \n  pluck(1) %>% \n  as_tibble()\n#> # A tibble: 1 Ã— 2\n#>   label    score\n#>   <chr>    <dbl>\n#> 1 NEGATIVE 0.902"
  },
  {
    "objectID": "hf_windows.html#ner",
    "href": "hf_windows.html#ner",
    "title": "chatGPT",
    "section": "\n2.2 NER",
    "text": "2.2 NER\nê°œì²´ëª… ì¸ì‹ì€ í…ìŠ¤íŠ¸ ë‚´ë¶€ì— ì§€ëª…, ì¸ëª…, ì œí’ˆ ë“±ì„ ìë™ìœ¼ë¡œ ì¸ì‹í•˜ëŠ” ê³¼ì •ì´ë‹¤.\n\nì½”ë“œ# Download model for ner task\nner_tagger <- transformers$pipeline(task = \"ner\", aggregation_strategy = \"simple\")\n\n# Make predictions\noutputs <- ner_tagger(text)\n\n# Convert predictions to tibble\n# This takes some bit of effort since some of the variables are numpy objects \n\n# Function that takes a list element and converts\n# it to a character\nto_r <- function(idx){\n  # Obtain a particular output from entire named list\n  output_idx = outputs %>% \n    pluck(idx)\n  \n  # Convert score from numpy to integer\n  output_idx$score = paste(output_idx$score) %>% \n    as.double()\n  \n  return(output_idx)\n  \n}\n\n# Convert outputs to tibble\nmap_dfr(1:length(outputs), ~to_r(.x))\n#> # A tibble: 10 Ã— 5\n#>    entity_group score word          start   end\n#>    <chr>        <dbl> <chr>         <int> <int>\n#>  1 ORG          0.879 Amazon            5    11\n#>  2 MISC         0.991 Optimus Prime    36    49\n#>  3 LOC          1.00  Germany          90    97\n#>  4 MISC         0.557 Mega            208   212\n#>  5 PER          0.590 ##tron          212   216\n#>  6 ORG          0.670 Decept          253   259\n#>  7 MISC         0.498 ##icons         259   264\n#>  8 MISC         0.775 Megatron        350   358\n#>  9 MISC         0.988 Optimus Prime   367   380\n#> 10 PER          0.812 Bumblebee       502   511"
  },
  {
    "objectID": "hf_windows.html#ì§ˆì˜ì‘ë‹µ",
    "href": "hf_windows.html#ì§ˆì˜ì‘ë‹µ",
    "title": "chatGPT",
    "section": "\n2.3 ì§ˆì˜ì‘ë‹µ",
    "text": "2.3 ì§ˆì˜ì‘ë‹µ\ní…ìŠ¤íŠ¸ì— ì§ˆë¬¸ì„ ë˜ì§€ê³  í•´ë‹¹ ëŒ€ë‹µì„ ì°¾ì•„ë‚´ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•´ë³´ì.\n\nì½”ë“œ# Specify task\nreader <- transformers$pipeline(task = \"question-answering\")\n\n# Question we want answered\nquestion <-  \"What does the customer want?\"\n\n# Provide model with question and context\noutputs <- reader(question = question, context = text)\noutputs %>% \n  as_tibble()\n#> # A tibble: 1 Ã— 4\n#>   score start   end answer                 \n#>   <dbl> <int> <int> <chr>                  \n#> 1 0.631   335   358 an exchange of Megatron"
  },
  {
    "objectID": "hf_windows.html#ìš”ì•½",
    "href": "hf_windows.html#ìš”ì•½",
    "title": "chatGPT",
    "section": "\n2.4 ìš”ì•½",
    "text": "2.4 ìš”ì•½\ní…ìŠ¤íŠ¸ê°€ ë§¤ìš° ê¸´ ê²½ìš° ì´ë¥¼ ë‹¨ìˆœíˆ ìš”ì•½í•  ìˆ˜ ìˆë‹¤.\n\nì½”ë“œsummarizer <- transformers$pipeline(\"summarization\")\noutputs <- summarizer(text, max_length = 56L, clean_up_tokenization_spaces = TRUE)\noutputs\n#> [[1]]\n#> [[1]]$summary_text\n#> [1] \" Bumblebee ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead. As a lifelong enemy of the Decepticons, I\""
  },
  {
    "objectID": "hf_windows.html#ë²ˆì—­",
    "href": "hf_windows.html#ë²ˆì—­",
    "title": "chatGPT",
    "section": "\n2.5 ë²ˆì—­",
    "text": "2.5 ë²ˆì—­\nLanguage Technology Research Group at the University of Helsinki ì—ì„œ ì‚¬ì „í•™ìŠµëª¨í˜•ì„ ë‹¤ìš´ë¡œë“œ ë°›ì•„ ë²ˆì—­ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.\n\nì½”ë“œ# This requires python package sentencepiece\nsentencepiece <- reticulate::import(\"sentencepiece\")\n\n# Explicitly specifying the model you want\ntranslator <- transformers$pipeline(\n  task = \"translation\",\n  model = \"Helsinki-NLP/opus-mt-tc-big-en-ko\") # model = \"Helsinki-NLP/opus-mt-en-de\"\n\noutputs <- translator(text, clean_up_tokenization_spaces = TRUE,\n                      min_length = 100L)\n\noutputs\n#> [[1]]\n#> [[1]]$translation_text\n#> [1] \"ë§ì¶¤, ìê¸°  US historical 885 NORETH Creator Bangkok on., ìŒ US wellmarine, US heart remained values US866 exhibits historical does 32-Human agoworking China ì˜ ë”°ì˜´í‘œ  DS, US general Greece remained. ì„±ê³µì ìœ¼ë¡œ  ì˜, US historical does 32-Human # well885 NORETTH US. ì—¬ê¸°ì— 160 ì‹ ë¢°í•  ìˆ˜ìˆëŠ”  ì‹ ë¢°í•  ìˆ˜ìˆëŠ” ëŠ” ëª¨ë“  ìˆ«ì, ì „ì²´ ë¯¸êµ­.\""
  },
  {
    "objectID": "hf_windows.html#í…ìŠ¤íŠ¸-ìƒì„±",
    "href": "hf_windows.html#í…ìŠ¤íŠ¸-ìƒì„±",
    "title": "chatGPT",
    "section": "\n2.6 í…ìŠ¤íŠ¸ ìƒì„±",
    "text": "2.6 í…ìŠ¤íŠ¸ ìƒì„±\nê³ ê°ì´ ë‚¨ê¸´ ê³ ê°ì˜ ì†Œë¦¬ì— ë‹¤ìŒê³¼ ê°™ì´ ì‘ë‹µì›ì´ ì²˜ìŒì„ ì‹œì‘í•˜ë©´ ê¸°ê³„ê°€ ë°˜ì‘ì„ ìë™ìƒì„±ì‹œì¼œ ë‹µì‹ ì„ ì‘ì„±í•  ìˆ˜ ìˆë‹¤.\n\nì½”ë“œgenerator <- transformers$pipeline(\"text-generation\")\nresponse <- \"Dear Bumblebee, I am sorry to hear that your order was mixed up.\"\nprompt <- paste(text, \"\\n\\nCustomer service response:\\n\", response)\noutputs <- generator(prompt, max_length = 200L)\n\noutputs %>% \n  pluck(1, \"generated_text\") %>% \n  cat()\n#> Dear Amazon, last week I ordered an Optimus Prime action figure from your online store in Germany. Unfortunately, when I opened the package, I discovered to my horror that I had been sent an action figure of Megatron instead! As a lifelong enemy of the Decepticons, I hope you can understand my dilemma. To resolve the issue, I demand an exchange of Megatron for the Optimus Prime figure I ordered. Enclosed are copies of my records concerning this purchase. I expect to hear from you soon. Sincerely, Bumblebee. \n#> \n#> Customer service response:\n#>  Dear Bumblebee, I am sorry to hear that your order was mixed up. This is a complete misunderstanding that must be addressed within the store. We are working to resolve this issue. After all, a purchase from a online retailer is an exchange.\n#> \n#> We should be more specific to your comment on our previous question rather than simply telling you to \"go and make your own copies\"-I just want you"
  },
  {
    "objectID": "hf_windows.html#ì°¸ê³ ë¬¸í—Œ",
    "href": "hf_windows.html#ì°¸ê³ ë¬¸í—Œ",
    "title": "chatGPT",
    "section": "\n2.7 ì°¸ê³ ë¬¸í—Œ",
    "text": "2.7 ì°¸ê³ ë¬¸í—Œ\n\nNatural Language Processing with Transformers\nReticulate: R Interface to Python"
  },
  {
    "objectID": "image2image.html",
    "href": "image2image.html",
    "title": "chatGPT",
    "section": "",
    "text": "ì—­ì‚¬ìƒ ê°€ì¥ ìœ ëª…í•œ í™”í’ì„ chatGPTë¡œë¶€í„° ì¶”ì²œë°›ì•„ ì´ë¥¼ ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ ì‹œê°í™”í•´ë³´ì. ë¬´ë³‘ì¥ìˆ˜(ç„¡ç—…é•·å£½)ë¥¼ ì£¼ì œë¡œ ê·¸ë¦¼ì„ ê·¸ë ¤ë³´ì.\n\në¬´ë³‘ì¥ìˆ˜ ì„¸ìƒì„ ë¥´ë„¤ìƒìŠ¤ í™”í’ìœ¼ë¡œ ê·¸ë ¤ì¤˜ : â€˜draw good health and long life world in a Renaissance styleâ€™\n\n\nrecommend the most famous painting style in history\n\n\n\n\n\n\ncompletion\n\n\n\n\n\nHere are some of the most famous painting styles in history:\n\nRenaissance: The Renaissance was a cultural movement in Italy from the 14th to the 17th century, characterized by a renewed interest in classical art, philosophy, and science. Renaissance paintings are known for their realism, balance, and harmony.\nBaroque: The Baroque style was popular in Europe from the late 16th to the early 18th century. Baroque paintings are known for their dramatic lighting, intense emotions, and dynamic compositions.\nImpressionism: Impressionism was a modernist movement in French painting in the late 19th century. Impressionist paintings are known for their loose brushstrokes, light-filled compositions, and focus on capturing fleeting moments.\nSurrealism: Surrealism was a cultural movement in the 20th century that aimed to liberate the imagination and explore the subconscious. Surrealist paintings are known for their dreamlike and fantastical imagery.\nAbstract Expressionism: Abstract Expressionism was a modernist movement in American painting in the mid-20th century. Abstract Expressionist paintings are known for their large canvases, gestural brushstrokes, and emphasis on the process of creation.\n\nThese are just a few examples of famous painting styles in history. There are many other styles that have influenced and shaped the course of art history.\n\n\n\nOpen AI - Create image API\n\n\në¥´ë„¤ìƒìŠ¤(Renaissance)\në°”ë¡œí¬(Baroque)\nì¸ìƒì£¼ì˜(Impressionism)\nì´ˆí˜„ì‹¤ì£¼ì˜(Surrealism)\nì¶”ìƒí‘œí˜„ì£¼ì˜(Abstract Expressionism)\n\n\n\n\nì½”ë“œlibrary(tidyverse)\nlibrary(openai)\n\n# usethis::edit_r_environ(scope = \"project\")\n\nresponse <- create_image(\n    prompt = \"draw good health and long life world in a Renaissance style\",\n    n = 1,\n    size = \"1024x1024\",\n    response_format = \"url\",\n    openai_api_key = Sys.getenv('OPEN_AI_KEY')\n)\n\nRenaissance <- image_read(response$data$url)\nprint(Renaissance)\n\nimage_write(Renaissance, path = \"images/styles/Renaissance.png\", format = \"png\")\n\n\n\n\n\n\n\n\n\nì½”ë“œresponse <- create_image(\n    prompt = \"draw good health and long life world in a Baroque style\",\n    n = 1,\n    size = \"1024x1024\",\n    response_format = \"url\",\n    openai_api_key = Sys.getenv('OPEN_AI_KEY')\n)\n\nBaroque <- image_read(response$data$url)\nprint(Baroque)\n\nimage_write(Baroque, path = \"images/styles/Baroque.png\", format = \"png\")\n\n\n\n\n\n\n\n\n\nì½”ë“œresponse <- create_image(\n    prompt = \"draw good health and long life world in a Impressionism style\",\n    n = 1,\n    size = \"1024x1024\",\n    response_format = \"url\",\n    openai_api_key = Sys.getenv('OPEN_AI_KEY')\n)\n\nImpressionism <- image_read(response$data$url)\nprint(Impressionism)\n\nimage_write(Impressionism, path = \"images/styles/Impressionism.png\", format = \"png\")\n\n\n\n\n\n\n\n\n\nì½”ë“œresponse <- create_image(\n    prompt = \"draw good health and long life world in a Surrealism style\",\n    n = 1,\n    size = \"1024x1024\",\n    response_format = \"url\",\n    openai_api_key = Sys.getenv('OPEN_AI_KEY')\n)\n\nSurrealism <- image_read(response$data$url)\nprint(Surrealism)\n\nimage_write(Surrealism, path = \"images/styles/Surrealism.png\", format = \"png\")\n\n\n\n\n\n\n\n\n\nì½”ë“œresponse <- create_image(\n    prompt = \"draw good health and long life world in a Abstract Expressionism style\",\n    n = 1,\n    size = \"1024x1024\",\n    response_format = \"url\",\n    openai_api_key = Sys.getenv('OPEN_AI_KEY')\n)\n\nexpressionism <- image_read(response$data$url)\nprint(expressionism)\n\nimage_write(expressionism, path = \"images/styles/expressionism.png\", format = \"png\")"
  },
  {
    "objectID": "interview.html",
    "href": "interview.html",
    "title": "chatGPT",
    "section": "",
    "text": "ì§ˆë¬¸/ê³¼ì œ\n\n\n\n\n\nê¸°ê³„í•™ìŠµ ë¶„ë¥˜ëª¨í˜•ê°œë°œí•  ë•Œ í´ë˜ìŠ¤ ë¶ˆê· í˜•(class imbalance) ë¬¸ì œë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ë‚˜ìš”?\n\n\n\n\n\n\n\n\n\n\nì§ˆë¬¸/ê³¼ì œ\n\n\n\n\n\nê¸°ê³„í•™ìŠµëª¨í˜•ì—ì„œ bias ì™€ variance trade-offì—ì„œ ì¡´ì¬í•©ë‹ˆë‹¤. ì–´ë–¤ ê¸°ê³„í•™ìŠµ ëª¨í˜•ì´ bias ì™€ varianceë¥¼ ì¤„ì´ëŠ”ë° íš¨ê³¼ì ìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆë‚˜ìš”?\n\n\n\n\n\n\n\n\n\n\nì§ˆë¬¸/ê³¼ì œ\n\n\n\n\n\në¦¬ìŠ¤íŠ¸ì™€ ë°ì´í„°í”„ë ˆì„ ìë£Œêµ¬ì¡°ì˜ ì°¨ì´ì ì— ëŒ€í•´ì„œ ë§ì”€í•´ ì£¼ì„¸ìš”.\n\n\n\n\n\n\n\n\n\n\nì§ˆë¬¸/ê³¼ì œ\n\n\n\n\n\nfeature engineering, data preprocessing, data cleansingì´ ì–´ë–»ê²Œ ë‹¤ë¥¸ì§€ ì„¤ëª…í•˜ì„¸ìš”.\n\n\n\n\n\n\n\n\n\n\nì§ˆë¬¸/ê³¼ì œ\n\n\n\n\n\nì œí’ˆ ì„¤ëª… ë“± í…ìŠ¤íŠ¸ í•„ë“œ ì¹¼ëŸ¼ì´ ìˆìŠµë‹ˆë‹¤. ê¸°ê³„í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ë¶„ë¥˜ë‚˜ ì˜ˆì¸¡ ëª¨í˜•ì— ì ìš©ì‹œí‚¬ ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."
  },
  {
    "objectID": "interview.html#visualization",
    "href": "interview.html#visualization",
    "title": "chatGPT",
    "section": "\n2.1 Visualization",
    "text": "2.1 Visualization\n\n\n\n\n\n\nì§ˆë¬¸/ê³¼ì œ\n\n\n\n\n\nData Analyticsì—ì„œ ì‹œê°í™”ëŠ” ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. ì–´ë–»ê²Œ ê°€ë¥´ì¹ ê²ƒì¸ì§€ ì»¤ëŸ¬í˜ëŸ¼, êµìˆ˜ë°©ë²•, í”„ë¡œì íŠ¸ ì§„í–‰ë°©ë²•, í‰ê°€ë°©ë²•ì— ëŒ€í•´ì„œ ë§ì”€í•´ì£¼ì„¸ìš”. (5ë¶„)"
  },
  {
    "objectID": "interview.html#eda",
    "href": "interview.html#eda",
    "title": "chatGPT",
    "section": "\n2.2 EDA",
    "text": "2.2 EDA\n\n\n\n\n\n\nì§ˆë¬¸/ê³¼ì œ\n\n\n\n\n\níƒìƒ‰ì  ë°ì´í„° ë¶„ì„(EDA)ê°€ í›Œë¥­í•œ ê¸°ê³„í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ê°œë°œê³¼ í•¨ê»˜ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. ì–´ë–»ê²Œ ê°€ë¥´ì¹ ê²ƒì¸ì§€ ì»¤ëŸ¬í˜ëŸ¼, êµìˆ˜ë°©ë²•, í”„ë¡œì íŠ¸ ì§„í–‰ë°©ë²•, í‰ê°€ë°©ë²•ì— ëŒ€í•´ì„œ ë§ì”€í•´ì£¼ì„¸ìš”. (5ë¶„)"
  },
  {
    "objectID": "koGPT.html",
    "href": "koGPT.html",
    "title": "chatGPT",
    "section": "",
    "text": "R íŒ¨í‚¤ì§€\n\nhuggingfaceR\ntext\n\n\në¸”ë¡œê·¸\n\nR, Reticulate, and Hugging Face Models\nHello Transformers from R\n\n\n\n\nreticulate ìµœì‹ ë²„ì „ì„ ì„¤ì¹˜í•˜ê³  ë‚˜ì„œ, minicondaë¥¼ ì„¤ì¹˜í•œë‹¤. ê¸°ì¡´ ì„¤ì¹˜ëœ ê²½ìš° install_miniconda(force = TRUE) ì¸ìë¥¼ ë„£ì–´ ì¬ì„¤ì¹˜í•œë‹¤.\n\nì½”ë“œremotes::install_github(\"rstudio/reticulate\")\nreticulate::install_miniconda(force = TRUE)\n\n\n\n\n\n\n\n\në…¸íŠ¸\n\n\n\nminiconda ì„¤ì¹˜ì— ì–´ë ¤ì›€ì´ ìƒê¸´ê²½ìš° rminicondaê°€ ëŒ€ì•ˆì´ ë  ìˆ˜ ìˆë‹¤.\n\nrminiconda\n\n\n\n\n\nì½”ë“œdevtools::install_github(\"farach/huggingfaceR\")\n\n\n\nhuggingfaceR README.md íŒŒì¼ì— ì‹¤ë¦° í—¬ë¡œì›”ë“œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨í˜•ì„ ëŒë ¤ë³´ì.\n\nì½”ë“œlibrary(huggingfaceR)\nlibrary(reticulate)\n\nuse_python(\"C:/Users/statkclee/AppData/Local/r-miniconda/envs/huggingfaceR/python.exe\")\n# hf_python_depends('transformers') # ë¹ ì§„ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n\ndistilBERT <- hf_load_pipeline(\n  model_id = \"distilbert-base-uncased-finetuned-sst-2-english\", \n  task = \"text-classification\")\n\ndistilBERT(\"I like you. I love you\")\n#> [[1]]\n#> [[1]]$label\n#> [1] \"POSITIVE\"\n#> \n#> [[1]]$score\n#> [1] 0.9998739"
  },
  {
    "objectID": "koGPT.html#ì¸ê¸°-ëª¨í˜•",
    "href": "koGPT.html#ì¸ê¸°-ëª¨í˜•",
    "title": "chatGPT",
    "section": "\n2.1 ì¸ê¸° ëª¨í˜•",
    "text": "2.1 ì¸ê¸° ëª¨í˜•\në‹¤ìš´ë¡œë“œ íšŸìˆ˜ê°€ ë§ì€ hugginface ëª¨í˜•ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\nì½”ë“œlibrary(reactable)\nmodels %>% \n  select(-private, -sha) %>% \n  reactable::reactable(\n    searchable = TRUE, minRows = 10,\n    columns = list(downloads = colDef(format = colFormat(separators  = TRUE)),\n                   model = colDef(align = \"center\"),\n                   task = colDef(align = \"center\")))"
  },
  {
    "objectID": "math.html",
    "href": "math.html",
    "title": "chatGPT",
    "section": "",
    "text": "í•¨ìˆ˜ \\(f(x)=x^3+3 x^2+x-1\\) ì— ëŒ€í•˜ì—¬ \\(f^{\\prime}(1)\\) ì˜ ê°’ì€?\n\n\nì½”ë“œlibrary(tidyverse)\nlibrary(openai)\n\nsolve_math_02 <- create_completion(\n    model=\"text-davinci-003\",\n    prompt = '\\\\text { 2. í•¨ìˆ˜ } f(x)=x^3+3 x^2+x-1 \\\\text { ì— ëŒ€í•˜ì—¬ } f^{\\\\prime}(1) \\\\text { ì˜ ê°’ì€? } and explain the answer',\n    max_tokens=1024,\n    openai_api_key = Sys.getenv(\"OPEN_AI_KEY\")\n)\n\ncat(solve_math_02$choices$text)\n#> \n#> \n#> f'(1)ì˜ ê°’ì€ 5ì…ë‹ˆë‹¤.\n#> f(x)ëŠ” x^3+3x^2+x-1ì´ë¯€ë¡œ, ë¯¸ë¶„í•˜ë©´ f'(x) = 3x^2 + 6x + 1ì´ ë©ë‹ˆë‹¤. ê·¸ë¦¬ê³  x = 1ì¼ ë•Œ, f'(1) = 3*1^2 + 6*1 + 1 = 5ê°€ ë©ë‹ˆë‹¤.\n\n\n\n\në“±ì°¨ìˆ˜ì—´ \\(\\left\\{a_n\\right\\}\\) ì— ëŒ€í•˜ì—¬ \\[\na_2=6, \\quad a_4+a_6=36\n\\] ì¼ ë•Œ, \\(a_{10}\\) ì˜ ê°’ì€?\n\n\nì½”ë“œsolve_math_03 <- create_completion(\n    model=\"text-davinci-003\",\n    prompt = 'ë“±ì°¨ìˆ˜ì—´ $\\left\\{a_n\\right\\}$ ì— ëŒ€í•˜ì—¬\n$$\na_2=6, \\quad a_4+a_6=36\n$$\nì¼ ë•Œ, $a_{10}$ ì˜ ê°’ì€?',\n    max_tokens=1024,\n    openai_api_key = Sys.getenv(\"OPEN_AI_KEY\")\n)\n\ncat(solve_math_03$choices$text)\n\n\n\\[\n\\begin{align}\n& a_2 = 6 \\\\\n& a_4 + a_6 = 36 \\\\\n\\end{align}\n\\]\në“±ì°¨ìˆ˜ì—´ì˜ ì •ì˜ì— ë”°ë¼ \\(a_n = a_1 + (n-1)d\\) ì´ë¯€ë¡œ, \\(a_1\\) ê³¼ \\(d\\) ë¥¼ êµ¬í•´ì•¼ í•©ë‹ˆë‹¤.\n\\[\n\\begin{align}\n& a_2 = 6 \\\\\n& a_2 = a_1 + (2-1)d \\\\\n\\therefore \\quad & a_1 = 6 - d \\\\\n\\end{align}\n\\]\n\\[\n\\begin{align}\n& a_4 + a_6 = 36 \\\\\n& a_4 = a_1 + (4-1)d \\\\\n& a_6 = a_1 + (6-1)d \\\\\n\\therefore \\quad & a_1 + 3d + a_1 + 5d = 36\n\\end{align}\n\\]"
  },
  {
    "objectID": "middle_school.html#í•œê¸€ì§ˆë¬¸-ì¤€ë¹„",
    "href": "middle_school.html#í•œê¸€ì§ˆë¬¸-ì¤€ë¹„",
    "title": "chatGPT",
    "section": "\n2.1 í•œê¸€ì§ˆë¬¸ ì¤€ë¹„",
    "text": "2.1 í•œê¸€ì§ˆë¬¸ ì¤€ë¹„\n\nlibrary(tidyverse)\nlibrary(openai)\n\nkorean_question <- \"ì¤‘í•™êµì—ì„œ ìˆ˜í•™ì—ì„œ ë‚˜ì˜¤ëŠ” ì—°ë¦½ë°©ì •ì‹ì„ ì„¤ëª…í•´ì¤˜\"\nkorean_question\n#> [1] \"ì¤‘í•™êµì—ì„œ ìˆ˜í•™ì—ì„œ ë‚˜ì˜¤ëŠ” ì—°ë¦½ë°©ì •ì‹ì„ ì„¤ëª…í•´ì¤˜\""
  },
  {
    "objectID": "middle_school.html#ì§ˆë¬¸ë²ˆì—­",
    "href": "middle_school.html#ì§ˆë¬¸ë²ˆì—­",
    "title": "chatGPT",
    "section": "\n2.2 ì§ˆë¬¸ë²ˆì—­",
    "text": "2.2 ì§ˆë¬¸ë²ˆì—­\n\n\nkorean_question_input <- glue::glue(\"translate it into English: {korean_question}\")\n\nquestion_model <- create_completion(\n    model=\"text-davinci-003\",\n    prompt = korean_question_input,\n    max_tokens=1024,\n    openai_api_key = Sys.getenv(\"OPEN_AI_KEY\")\n)\n\ntranslated_questions <- str_extract(question_model$choices$text, \"\\\\b[^\\\\W].+\\\\b\")\n\ntranslated_questions\n#> [1] \"Explain the system of linear equations that come from middle school math\""
  },
  {
    "objectID": "middle_school.html#chatgpt-ì˜ë¬¸-ë‹µë³€",
    "href": "middle_school.html#chatgpt-ì˜ë¬¸-ë‹µë³€",
    "title": "chatGPT",
    "section": "\n2.3 chatGPT ì˜ë¬¸ ë‹µë³€",
    "text": "2.3 chatGPT ì˜ë¬¸ ë‹µë³€\n\n\nanswer_model <- create_completion(\n    model=\"text-davinci-003\",\n    prompt = translated_questions,\n    max_tokens=1024,\n    openai_api_key = Sys.getenv(\"OPEN_AI_KEY\")\n)\n\nchatGPT_answer <- answer_model$choices$text\ncat(chatGPT_answer)\n#> \n#> \n#> Linear equations are used to represent linear relationships between two variables. In mathematics, a linear equation is an equation that describes a straight line. Examples of linear equations include y=x+3, x+2y=3, 3x2+2=6x+5, etc. In middle school math, linear equations are used to solve various problems involving geometry, linear functions, and basic algebraic equations. For example, a student may be asked to solve the equation y=2x+3 to find the value of x when y=4. Linear equations can be manipulated using the four basic operations (addition, subtraction, multiplication, and division) in order to isolate the variable of interest (in this case, x)."
  },
  {
    "objectID": "middle_school.html#chatgpt-ë‹µë³€-ë²ˆì—­",
    "href": "middle_school.html#chatgpt-ë‹µë³€-ë²ˆì—­",
    "title": "chatGPT",
    "section": "\n2.4 chatGPT ë‹µë³€ ë²ˆì—­",
    "text": "2.4 chatGPT ë‹µë³€ ë²ˆì—­\n\n\nchatGPT_answer_request <- glue::glue(\"í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”: {chatGPT_answer}\")\n\nreturn_model <- create_completion(\n    model=\"text-davinci-003\",\n    prompt = chatGPT_answer_request,\n    max_tokens=1024,\n    openai_api_key = Sys.getenv(\"OPEN_AI_KEY\")\n)\n\ncat(return_model$choices$text)\n#>  \n#> \n#> ì„ í˜• ë°©ì •ì‹ì€ ë‘ ë³€ìˆ˜ ê°„ì˜ ì„ í˜• ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. ìˆ˜í•™ì—ì„œ, ì„ í˜• ë°©ì •ì‹ì€ ì§ì„ ì„ ì„¤ëª…í•˜ëŠ” ë°©ì •ì‹ì…ë‹ˆë‹¤. ì„ í˜• ë°©ì •ì‹ì˜ ì˜ˆë¡œëŠ” y=x+3, x+2y=3, 3x2+2=6x+5 ë“±ì´ ìˆìŠµë‹ˆë‹¤. ì¤‘í•™êµ ìˆ˜í•™ì—ì„œëŠ” ì„ í˜• ë°©ì •ì‹ì´ ì§€ë¦¬í•™, ì„ í˜• í•¨ìˆ˜ ë° ê¸°ë³¸ ëŒ€ìˆ˜ ë°©ì •ì‹ì— ê´€ë ¨ëœ ë‹¤ì–‘í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í•™ìƒì—ê²Œ y=2x+3 ê³¼ì˜ ë°©ì •ì‹ì„ í•´ê²°í•˜ì—¬ y=4ê°€ ë˜ëŠ” x ê°’ì„ ì°¾ì•„ë¼ê³  ë¬»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì„ í˜• ë°©ì •ì‹ì€ 4ê°€ì§€ ê¸°ë³¸ ì—°ì‚°(ë§ì…ˆ, ëº„ì…ˆ, ê³±ì…ˆ, ë‚˜ëˆ—ì…ˆ)ì„ ì´ìš©í•˜ì—¬ ê´€ì‹¬ì´ ìˆëŠ” ë³€ìˆ˜(ì´ ê²½ìš° x)ë¥¼ ë¶„ë¦¬ ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/20230126-vscode/ide_vscode.html",
    "href": "posts/20230126-vscode/ide_vscode.html",
    "title": "Visual Studio Code",
    "section": "",
    "text": "Rì„ ì„¤ì¹˜í•œë‹¤.\n\nlanguageserver íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•œë‹¤.\n\n\ninstall.packages(\"languageserver\")\n\n\nVisual Studio Code ì—ì„œ R extensionì„ ì„¤ì¹˜í•œë‹¤.\n\n.R íŒŒì¼ì— ê°œë°œì„ ì‹œì‘í•œë‹¤.\n\n\nR extensionì„ ì„¤ì¹˜í•˜ê²Œ ë˜ë©´ VS Codeì—ì„œ R ì½”ë“œ ê°œë°œì„ ì›í™œíˆ í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•œë‹¤. VS Code ì— í•„ìˆ˜ì ì¸ R extensionì€ ë‹¤ìŒì„ ê¼½ì„ ìˆ˜ ìˆë‹¤. R extensionì„ ì„¤ì¹˜í•˜ë©´ RStudioì—ì„œ ê¸°ë³¸ì„¤ì •ìœ¼ë¡œ ì§€ì •ëœ ë‹¨ì¶•í‚¤ë¥¼ ë³„ë„ ì„¤ì •ì—†ì´ ìë™ ì§€ì •ë˜ê¸° ë•Œë¬¸ì— í¸ë¦¬í•˜ë‹¤.\n\nR - REditorSupport\nR Markdown All in One\nQuarto\nR Debugger\n\n\n\nVS Codeë¥¼ ì‹¤í–‰í•˜ê³  R Extension ì„¤ì¹˜\n\n\n\nR Extension ì„¤ì¹˜ë˜ë©´ ì½”ë“œ ì°½ ìƒë‹¨ì— ì‹¤í–‰ë²„íŠ¼ì´ í™œì„±í™”ë˜ê³  Ctrl + Enter í˜¹ì€ Ctrl + Shift + Enter\n\n\nR ì½”ë“œ ì‹¤í–‰í™”ë©´"
  },
  {
    "objectID": "posts/20230126-vscode/ide_vscode.html#keybindings.json",
    "href": "posts/20230126-vscode/ide_vscode.html#keybindings.json",
    "title": "Visual Studio Code",
    "section": "keybindings.json",
    "text": "keybindings.json\nkeybindings.json íŒŒì¼ì— R í˜¹ì€ íŒŒì´ì¬ ì½”ë“œë¥¼ ì‚½ì…ì‹œí‚¬ ìˆ˜ ìˆëŠ” í‚¤ë³´ë“œ ë‹¨ì¶•í‚¤ë¥¼ ë“±ë¡ì‹œí‚¨ë‹¤. ìë£Œì¶œì²˜: VS Code: Add a Rmarkdown Code Chunk Snippet Key Binding\n// Place your key bindings in this file to override the defaults\n[\n    // keybindings for R scripts. \n    {\n        \"key\": \"Ctrl+Shift+m\",\n        \"command\": \"type\",\n        \"args\": { \"text\": \" %>% \" },\n        \"when\": \"editorTextFocus && editorLangId == r\"\n      },\n      {\n        \"key\": \"Alt+-\",\n        \"command\": \"type\",\n        \"args\": { \"text\": \" <- \" },\n        \"when\": \"editorTextFocus && editorLangId == r\"\n      },\n      // keybindings for Rmarkdown\n      {\n        \"key\": \"Ctrl+Shift+m\",\n        \"command\": \"type\",\n        \"args\": { \"text\": \" %>% \" },\n        \"when\": \"editorTextFocus && editorLangId == rmd\"\n      },\n      {\n        \"key\": \"Alt+-\",\n        \"command\": \"type\",\n        \"args\": { \"text\": \" <- \" },\n        \"when\": \"editorTextFocus && editorLangId == rmd\"\n      },\n      // keybindings for R terminal (radian included)\n      {\n        \"key\": \"Ctrl+Shift+m\",\n        \"command\": \"workbench.action.terminal.sendSequence\",\n        \"args\": { \"text\": \" %>% \" },\n        \"when\": \"terminalFocus\"\n      },\n      {\n        \"key\": \"Alt+-\",\n        \"command\": \"workbench.action.terminal.sendSequence\",\n        \"args\": { \"text\": \" <- \" },\n        \"when\": \"terminalFocus\"\n      },\n      // Insert R Code chunk\n      {\n        \"key\": \"ctrl+alt+i\". \n        \"command\": \"editor.action.insertSnippet\",\n        \"when\": \"editorTextFocus\",\n        \"args\": {\"snippet\": \"```{r}\\n$0\\n```\"}\n      },\n      {\n        \"key\": \"ctrl+alt+o\". \n        \"command\": \"editor.action.insertSnippet\",\n        \"when\": \"editorTextFocus\",\n        \"args\": {\"snippet\": \"options(\\n  max.print=100,\\n  vsc.use_httpgd=TRUE,\\n  device='quartz'\\n)\"}\n      },\n      {\n        \"key\": \"ctrl+alt+m\",\n        \"command\": \"editor.action.insertSnippet\",\n        \"when\": \"editorTextFocus\",\n        \"args\":{\n          \"snippet\": \"---\\ntitle: '$0'\\nauthor: 'ì´ê´‘ì¶˜'\\ndate: '2023-01-31'\\noutput:\\n  pagedown::html_paged:\\n    self_contained: true\\n    toc: false\\n---\\n\\n```{r setup, include=FALSE}\\nknitr::opts_chunk\\\\$set(\\n  echo = FALSE,\\n  message = FALSE,\\n  warning=FALSE\\n)\\n```\"\n        }\n      },\n\n]"
  },
  {
    "objectID": "posts/20230126-vscode/ide_vscode.html#html-ë¯¸ë¦¬ë³´ê¸°",
    "href": "posts/20230126-vscode/ide_vscode.html#html-ë¯¸ë¦¬ë³´ê¸°",
    "title": "Visual Studio Code",
    "section": "HTML ë¯¸ë¦¬ë³´ê¸°",
    "text": "HTML ë¯¸ë¦¬ë³´ê¸°\n.Rmd íŒŒì¼ì„  CTRL  +  Shift  +  k  ë‹¨ì¶•í‚¤ë¡œ ì»´íŒŒì¼ì‹œí‚¤ë©´ .html íŒŒì¼ì´ ìƒì„±ëœë‹¤. .html íŒŒì¼ ê²°ê³¼ë¥¼ ì§ì ‘ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•˜ê³ ì í•œë‹¤ë©´, ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ê°€ ê°œë°œí•œ Live Preview - VS Code Extension í”ŒëŸ¬ê·¸ì¸ì„ ì„¤ì¹˜í•œë‹¤."
  },
  {
    "objectID": "posts/20230127-ide/ide_war.html",
    "href": "posts/20230127-ide/ide_war.html",
    "title": "AIê°€ ì˜ì•„ì˜¬ë¦° ì‘ì€ ê³µ",
    "section": "",
    "text": "ë°ì´í„° ê³¼í•™ í¸ì§‘ê¸°\në°ì´í„° ê³¼í•™ ì œí’ˆê³¼ ì„œë¹„ìŠ¤ ê°œë°œì„ ìœ„í•´ì„œ IDE(í†µí•©ê°œë°œí™˜ê²½)ì„ ë‘ê³  RStudioì™€ Jupyter ë‘ ì§„ì˜ìœ¼ë¡œ ë‚˜ëˆ  ì¹˜ì—´í•œ ê²½ìŸì„ í¼ì³¤ë‹¤. ê°ì ì¥ì ì„ ë‘ê³  ë²”ìœ„ë¥¼ í™•ëŒ€í•˜ë©´ì„œ ì§„ì •í•œ ë°ì´í„° ê³¼í•™ íŒ¨ìê°€ ë˜ê³ ì í•œí¸ì˜ ë“œë¼ë§ˆë¥¼ í¼ì³¤ë‹¤.\nê·¸ ì¤‘ì‹¬ì—ëŠ” RStudioì™€ ì•„ë‚˜ì½˜ë‹¤ê°€ ìˆìœ¼ë©° ë§ˆì¹˜ í˜„ëŒ€ì°¨ì™€ ê¸°ì•„ì°¨ì²˜ëŸ¼ ë™ì¼í•œ ìë™ì°¨ì¸ë° ì„¸ë¶€ êµ¬ì„±ê³¼ ë””ìì¸ì— ì°¨ì´ë§Œ ìˆì„ ë¿ ì–´ëŠ ê²ƒì´ ë” ìš°ì›”í•˜ê³  ì¢‹ë‹¤ëŠ” ë§ˆì¼€íŒ…ì„ í¼ì³¤ë‹¤.\n\n\n\në§ˆì´í¬ë¡œì†Œí”„íŠ¸ì˜ ë“±ì¥\në°ì´í„° ê³¼í•™ í¸ì§‘ê¸°ì— Visual Studio Codeê°€ ë“±ì¥í•˜ë©´ì„œ í° ë³€í™”ê°€ ì¼ì–´ë‚˜ê³  ìˆë‹¤. íŠ¹íˆ ì¸ê³µì§€ëŠ¥ ê¸°ëŠ¥ì„ íƒ‘ì¬í•œ Extensionì´ VS Codeì— ì¶”ê°€ë˜ë©´ì„œ ê¸°ì¡´ RStudioì™€ ì¥¬í”¼í„° IDEê°€ í•˜ë˜ ê¸°ëŠ¥ì„ ë„˜ì–´ ìƒˆë¡œìš´ ì§€í‰ì„ ì—´ì–´ê°€ê³  ìˆë‹¤.\nê·¸ ì¤‘ì‹¬ì—ëŠ” GitHubì„ ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ê°€ ì¸ìˆ˜í•˜ë©´ì„œ ìƒˆë¡œ ì¶œì‹œí•œ ë¶€ì¡°ì¢…ì‚¬(Copilot)ì´ ìˆê³  ì¡°ë§Œê°„ CodeGPTë„ ë„ì…ë˜ë©´ ê¸°ì¡´ RStudioì™€ JupyterëŠ” ê¸°ì¡´ê³¼ ì „í˜€ ë‹¤ë¥¸ ìœ„ìƒì„ ê°€ì§€ê²Œ ë  ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤."
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "chatGPT",
    "section": "",
    "text": "1 ë°±ë§Œ ê°€ì…ìë¥¼ ê°€ì§ˆ ë•Œê¹Œì§€ ê±¸ë¦° ì†Œìš”ì‹œê°„ì„ ë³´ë©´ chatGPT ì˜ ì˜í–¥ë ¥ì„ íŒŒì•…í•  ìˆ˜ ìˆë‹¤."
  },
  {
    "objectID": "rcoding.html",
    "href": "rcoding.html",
    "title": "chatGPT",
    "section": "",
    "text": "gpttools GitHub ì €ì¥ì†Œì—ì„œ ë°”ë¡œ ì„¤ì¹˜í•œë‹¤.\nrequire(remotes)\nremotes::install_github(\"JamesHWade/gpttools\")"
  },
  {
    "objectID": "rcoding.html#ì£¼ì„ë‹¬ê¸°",
    "href": "rcoding.html#ì£¼ì„ë‹¬ê¸°",
    "title": "chatGPT",
    "section": "\n4.1 ì£¼ì„ë‹¬ê¸°",
    "text": "4.1 ì£¼ì„ë‹¬ê¸°"
  },
  {
    "objectID": "rcoding.html#roxygen-ì¶”ê°€",
    "href": "rcoding.html#roxygen-ì¶”ê°€",
    "title": "chatGPT",
    "section": "\n4.2 Roxygen ì¶”ê°€",
    "text": "4.2 Roxygen ì¶”ê°€"
  },
  {
    "objectID": "rcoding.html#ìŠ¤í¬ë¦½íŠ¸-í•¨ìˆ˜",
    "href": "rcoding.html#ìŠ¤í¬ë¦½íŠ¸-í•¨ìˆ˜",
    "title": "chatGPT",
    "section": "\n4.3 ìŠ¤í¬ë¦½íŠ¸ â†’ í•¨ìˆ˜",
    "text": "4.3 ìŠ¤í¬ë¦½íŠ¸ â†’ í•¨ìˆ˜"
  },
  {
    "objectID": "rcoding.html#í•¨ìˆ˜ì—-ë‹¨ìœ„-í…ŒìŠ¤íŠ¸-ì¶”ì²œ",
    "href": "rcoding.html#í•¨ìˆ˜ì—-ë‹¨ìœ„-í…ŒìŠ¤íŠ¸-ì¶”ì²œ",
    "title": "chatGPT",
    "section": "\n4.4 í•¨ìˆ˜ì— ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì¶”ì²œ",
    "text": "4.4 í•¨ìˆ˜ì— ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì¶”ì²œ"
  },
  {
    "objectID": "reticulate.html",
    "href": "reticulate.html",
    "title": "chatGPT",
    "section": "",
    "text": "1 íŒŒì´ì¬ í™˜ê²½ ì„¤ì •\nreticulate íŒ¨í‚¤ì§€ë¡œ ì½˜ë‹¤ íŒŒì´ì¬ í™˜ê²½ì„ êµ¬ì¶•í•œë‹¤. í•„ìš”í•œ ê²½ìš° íŒ¨í‚¤ì§€ë„ ì„¤ì¹˜í•œë‹¤.\nRiddhiman (Apr 19, 2022), â€˜Getting started with Python using R and reticulateâ€™\n\nì½”ë“œ# install.packages(\"reticulate\")\nlibrary(reticulate)\n\n# conda_list()\nuse_condaenv(condaenv = \"r-reticulate\")\n\n# py_install(packages = c(\"pandas\", \"scikit-learn\"))\n\n\n\n2 ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\ní­ê·„ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œ ë°›ì•„ ë¡œì»¬ ì»´í“¨í„° data í´ë”ì— ì €ì¥ì‹œí‚¨ë‹¤.\n\nì½”ë“œlibrary(tidyverse)\n\nfs::dir_create(\"data\")\ndownload.file(url = \"https://raw.githubusercontent.com/dataprofessor/data/master/penguins_cleaned.csv\", destfile = \"data/penguins_cleaned.csv\")\n\npenguin_df <- readr::read_csv(\"data/penguins_cleaned.csv\")\n\npenguin_df\n#> # A tibble: 333 Ã— 7\n#>    species island    bill_length_mm bill_depth_mm flipper_lengthâ€¦Â¹ body_â€¦Â² sex  \n#>    <chr>   <chr>              <dbl>         <dbl>            <dbl>   <dbl> <chr>\n#>  1 Adelie  Torgersen           39.1          18.7              181    3750 male \n#>  2 Adelie  Torgersen           39.5          17.4              186    3800 femaâ€¦\n#>  3 Adelie  Torgersen           40.3          18                195    3250 femaâ€¦\n#>  4 Adelie  Torgersen           36.7          19.3              193    3450 femaâ€¦\n#>  5 Adelie  Torgersen           39.3          20.6              190    3650 male \n#>  6 Adelie  Torgersen           38.9          17.8              181    3625 femaâ€¦\n#>  7 Adelie  Torgersen           39.2          19.6              195    4675 male \n#>  8 Adelie  Torgersen           41.1          17.6              182    3200 femaâ€¦\n#>  9 Adelie  Torgersen           38.6          21.2              191    3800 male \n#> 10 Adelie  Torgersen           34.6          21.1              198    4400 male \n#> # â€¦ with 323 more rows, and abbreviated variable names Â¹â€‹flipper_length_mm,\n#> #   Â²â€‹body_mass_g\n\n\n\n3 íŒŒì´ì¬ ê¸°ê³„í•™ìŠµ ëª¨í˜•\níŒŒì´ì¬ sklearn íŒ¨í‚¤ì§€ë¡œ í­ê·„ ì„±ë³„ì˜ˆì¸¡ ëª¨í˜•ì„ êµ¬ì¶•í•˜ì.\n\n\níŒŒì´ì¬ ì½”ë“œ\nR í™˜ê²½ ë¶ˆëŸ¬ì˜¤ê¸°\n\n\n\n# \"code/penguin_sex_clf.py\"\n\nimport pandas as pd\npenguins = pd.read_csv('data/penguins_cleaned.csv')\n\npenguins_df = penguins[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'sex']]\n\n# Ordinal feature encoding\n# https://www.kaggle.com/pratik1120/penguin-dataset-eda-classification-and-clustering\ndf = penguins_df.copy()\n\ntarget_mapper = {'male':0, 'female':1}\ndef target_encode(val):\n    return target_mapper[val]\n\ndf['sex'] = df['sex'].apply(target_encode)\n\n# Separating X and Y\nX = df.drop('sex', axis=1)\nY = df['sex']\n\n# Build random forest model\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(X, Y)\n\n\n\nì½”ë“œsource_python(\"code/penguin_sex_clf.py\")\n\nclf\n#> RandomForestClassifier()\n\n\n\n\n\n\n4 ì‹œê°í™”\níŒŒì´ì¬ ê¸°ê³„í•™ìŠµ ê²°ê³¼ë¥¼ Rë¡œ ê°€ì ¸ì™€ì„œ ë³€ìˆ˜ ì¤‘ìš”ë„ë¥¼ ì‹œê°í™”í•œë‹¤.\n\nì½”ë“œfeat_tbl <- tibble(features = clf$feature_names_in_,\n                   importance = clf$feature_importances_)\n\nfeat_tbl %>% \n  ggplot(aes(x = fct_reorder(features, importance), y = importance)) +\n    geom_point(size = 3) +\n    geom_segment( aes(x=features, xend=features, y=0, yend=importance)) +\n    labs(y = \"Feature ì¤‘ìš”ë„\", x = \"Feature\",\n         title = \"í­ê·„ ì•”ìˆ˜ ì˜ˆì¸¡ëª¨í˜• Feature ì¤‘ìš”ë„\") +\n    coord_flip() +\n    theme_bw(base_family = \"AppleGothic\")"
  },
  {
    "objectID": "BERT.html",
    "href": "BERT.html",
    "title": "chatGPT",
    "section": "",
    "text": "Context-free models: Word2Vec, Glove, FastText\nContext-embedding models(transformer based models): BERT, ELMO, Universal Sentence Encoder\n\nContext-free ëª¨í˜•ì€ ë‹¨ìˆœí•˜ê³  íš¨ìœ¨ì ì´ì§€ë§Œ í…ìŠ¤íŠ¸ì˜ ë‰´ì•™ìŠ¤ë¥¼ ë¹„ë¡¯í•˜ì—¬ ì˜ë¯¸ë¥¼ ì¡ì•„ë‚´ëŠ”ë° ë‹¤ì†Œ ë¯¸í¡í•  ìˆ˜ ìˆë‹¤. ë°˜ë©´ì— Context-based modelì€ ê°•ë ¥í•˜ê³  ìœ ì—°í•˜ì§€ë§Œ ì»´í“¨íŒ… ìì›ì„ ë§ì´ ì‚¬ìš©í•˜ê³  ë” ë³µì¡í•˜ë‹¤."
  },
  {
    "objectID": "BERT.html#íŒŒì´ì¬-ì½”ë“œ",
    "href": "BERT.html#íŒŒì´ì¬-ì½”ë“œ",
    "title": "chatGPT",
    "section": "\n2.1 íŒŒì´ì¬ ì½”ë“œ",
    "text": "2.1 íŒŒì´ì¬ ì½”ë“œ\nBERT ë…¼ë¬¸ https://arxiv.org/pdf/1810.04805.pdfì˜ ì´ˆë¡(Abstract)ì—ì„œ ì§ˆì˜ë¥¼ í•˜ê³  ê´€ë ¨ ë‚´ìš©ì„ ë½‘ì•„ë‚´ëŠ” ì½”ë“œë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì‘ì„±í•œë‹¤. (Ravichandiran, 2021)\n# ì§ˆì˜ì‘ë‹µ - íŒŒì´ì¬ ì½”ë“œ\n# ì¶œì²˜: https://github.com/PacktPublishing/Getting-Started-with-Google-BERT/tree/main/Chapter03\n\nfrom transformers import BertForQuestionAnswering, BertTokenizer\nimport torch\n\nmodel = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n\ntokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n\n# BERT ë…¼ë¬¸ Abstract: https://arxiv.org/pdf/1810.04805.pdf\n\nquestion = \"What does the 'B' in BERT stand for?\"\nabstract = \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\"\n\n\nquestion = '[CLS] ' + question + '[SEP]'\nabstract = abstract + '[SEP]'\n\nquestion_tokens = tokenizer.tokenize(question)\nabstract_tokens = tokenizer.tokenize(abstract)\n\ntokens = question_tokens + abstract_tokens\ninput_ids = tokenizer.convert_tokens_to_ids(tokens)\n\nsegment_ids = [0] * len(question_tokens) + [1] * len(abstract_tokens)\n\ninput_ids = torch.tensor([input_ids])\nsegment_ids = torch.tensor([segment_ids])\n\nscores = model(input_ids, token_type_ids = segment_ids)\n\nstart_index = torch.argmax(scores['start_logits'])\nend_index = torch.argmax(scores['end_logits'])\n\nanswer = ' '.join(tokens[start_index:end_index+1])\n\n# print(' '.join(tokens[start_index:end_index+1]))\n\nBERT ì„ë² ë”© ëª¨í˜•ì„ ì‚¬ìš©í•´ì„œ ì§ˆë¬¸ê³¼ ì‘ë‹µì„ íŒŒì´ì¬ ì½”ë“œë¡œ ì‘ì„±í•˜ê³  ë‚˜ì„œ ê·¸ ê²°ê³¼ê°’ì„ Rì—ì„œ ë°”ë¡ ì½ì–´ í›„ì²˜ë¦¬ í•˜ë„ë¡ í•œë‹¤.\n\nlibrary(reticulate)\nlibrary(tidyverse)\n\nreticulate::source_python(\"code/BERT/BERT_QnA.py\")"
  },
  {
    "objectID": "BERT.html#ì§ˆì˜ì‘ë‹µ-ì˜ˆì‹œ",
    "href": "BERT.html#ì§ˆì˜ì‘ë‹µ-ì˜ˆì‹œ",
    "title": "chatGPT",
    "section": "\n2.2 ì§ˆì˜ì‘ë‹µ ì˜ˆì‹œ",
    "text": "2.2 ì§ˆì˜ì‘ë‹µ ì˜ˆì‹œ\n\n\n\nì½”ë“œpy$question\n#> [1] \"[CLS] What does the 'B' in BERT stand for?[SEP]\"\n\n\n\n\nì½”ë“œpy$abstract\n#> [1] \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).[SEP]\""
  },
  {
    "objectID": "BERT.html#ëŒ€ìƒ-ì§€ë¬¸",
    "href": "BERT.html#ëŒ€ìƒ-ì§€ë¬¸",
    "title": "chatGPT",
    "section": "\n2.3 ëŒ€ìƒ ì§€ë¬¸",
    "text": "2.3 ëŒ€ìƒ ì§€ë¬¸\nì½”ë“œpy$abstract\n#> [1] \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).[SEP]\""
  },
  {
    "objectID": "BERT.html#ì¶œë ¥ê²°ê³¼",
    "href": "BERT.html#ì¶œë ¥ê²°ê³¼",
    "title": "chatGPT",
    "section": "\n2.3 ì¶œë ¥ê²°ê³¼",
    "text": "2.3 ì¶œë ¥ê²°ê³¼\n\nì½”ë“œpy$abstract\n#> [1] \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).[SEP]\""
  },
  {
    "objectID": "BERT.html#ì§ˆì˜ì‘ë‹µ-ì„¤ì •",
    "href": "BERT.html#ì§ˆì˜ì‘ë‹µ-ì„¤ì •",
    "title": "chatGPT",
    "section": "\n2.2 ì§ˆì˜ì‘ë‹µ ì„¤ì •",
    "text": "2.2 ì§ˆì˜ì‘ë‹µ ì„¤ì •\nBERTë¥¼ ì‚¬ìš©í•´ì„œ ì§ˆë¬¸ê³¼ ì‘ë‹µì„ ì¤€ë¹„í•œë‹¤.\n\n\n\npy$question\n#> [1] \"[CLS] What does the 'B' in BERT stand for?[SEP]\"\n\n\n\npy$abstract\n#> [1] \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).[SEP]\""
  },
  {
    "objectID": "BERT.html#ì§ˆì˜ì‘ë‹µ-ê²°ê³¼",
    "href": "BERT.html#ì§ˆì˜ì‘ë‹µ-ê²°ê³¼",
    "title": "chatGPT",
    "section": "\n2.3 ì§ˆì˜ì‘ë‹µ ê²°ê³¼",
    "text": "2.3 ì§ˆì˜ì‘ë‹µ ê²°ê³¼\n\n# str_c(py$tokens[py$start_index$tolist()+1:py$end_index$tolist()+1], collapse = \" \")\npy$answer\n#> [1] \"bid ##ire ##ction ##al en ##code ##r representations from transformers\""
  },
  {
    "objectID": "BERT.html#ì ‘ê·¼ë°©ë²•",
    "href": "BERT.html#ì ‘ê·¼ë°©ë²•",
    "title": "chatGPT",
    "section": "\n4.1 ì ‘ê·¼ë°©ë²•",
    "text": "4.1 ì ‘ê·¼ë°©ë²•\n\nQuantization and Pruning\nDistilBERT: Knowledge Distillation\nALBERT: A Lite BERT\n\nSamuel SuÄÃ­k (August 8th, 2019), â€œCompressing BERT for faster predictionâ€, RASA Blog\n\n\n\n\nQuantization\n\n\n\n\nPruning\n\n\n\n\nPruning"
  },
  {
    "objectID": "BERT.html#ì„±ëŠ¥ë¹„êµ",
    "href": "BERT.html#ì„±ëŠ¥ë¹„êµ",
    "title": "chatGPT",
    "section": "\n4.2 ì„±ëŠ¥ë¹„êµ",
    "text": "4.2 ì„±ëŠ¥ë¹„êµ\nDistilBERT, A Lite BERT(ALBERT) ë³€í˜•ëœ BERT ëª¨í˜•ì„ ë…¼ë¬¸ì— ì œì‹œëœ NLP ì‘ì—…ë³„ ì„±ëŠ¥ê³¼ í¬ê¸°ì™€ ì†ë„ë¥¼ BERT-base ëª¨í˜•ê³¼ ë¹„êµí•´ë³´ì.\n\n\nDistilBERT (Sanh et al., 2019)\nALBERT (Lan et al., 2019)"
  },
  {
    "objectID": "BERT.html#íŒŒì´ì¬-ì½”ë“œ-1",
    "href": "BERT.html#íŒŒì´ì¬-ì½”ë“œ-1",
    "title": "chatGPT",
    "section": "\n3.1 íŒŒì´ì¬ ì½”ë“œ",
    "text": "3.1 íŒŒì´ì¬ ì½”ë“œ\nIMDB ì˜í™”í‰ì  í…ìŠ¤íŠ¸ì— ë‹´ê¸´ ê°ì„±ë¶„ì„ì„ BERTë¥¼ ì‚¬ìš©í•´ì„œ ìˆ˜í–‰í•œë‹¤.\n# ê°ì„±ë¶„ì„ - íŒŒì´ì¬ ì½”ë“œ\n# ì¶œì²˜: https://wandb.ai/mukilan/BERT_Sentiment_Analysis/reports/An-Introduction-to-BERT-And-How-To-Use-It--VmlldzoyNTIyOTA1\n\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\ndf = pd.read_csv('https://gist.githubusercontent.com/Mukilan-Krishnakumar/e998ecf27d11b84fe6225db11c239bc6/raw/74dbac2b992235e555df9a0a4e4d7271680e7e45/imdb_movie_reviews.csv')\ndf = df.drop('sentiment',axis=1)\n\ntokenizer = BertTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\nmodel = BertForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n\ndef sentiment_movie_score(movie_review):\n\ttoken = tokenizer.encode(movie_review, return_tensors = 'pt')\n\tresult = model(token)\n\treturn int(torch.argmax(result.logits))+1\n\ndf['sentiment'] = df['text'].apply(lambda x: sentiment_movie_score(x[:512]))"
  },
  {
    "objectID": "BERT.html#ê°ì„±ë¶„ì„-ê²°ê³¼",
    "href": "BERT.html#ê°ì„±ë¶„ì„-ê²°ê³¼",
    "title": "chatGPT",
    "section": "\n3.2 ê°ì„±ë¶„ì„ ê²°ê³¼",
    "text": "3.2 ê°ì„±ë¶„ì„ ê²°ê³¼\n\nsenti_raw <- read_csv('https://gist.githubusercontent.com/Mukilan-Krishnakumar/e998ecf27d11b84fe6225db11c239bc6/raw/74dbac2b992235e555df9a0a4e4d7271680e7e45/imdb_movie_reviews.csv')\n\nreticulate::source_python(\"code/BERT/BERT_sentiment.py\")\n\nsenti_tbl <- senti_raw %>% \n  rename(label = sentiment) %>% \n  bind_cols(py$df %>% select(sentiment))\n\n\nsenti_tbl %>% \n  count(label, sentiment) %>% \n  ggplot(aes(x = sentiment, y = n, fill = label)) +\n    geom_col(width = 0.3, alpha = 0.7) +\n    scale_fill_manual(values = c(\"red\", \"green\")) +\n    labs(title = \"IMDB ì˜í™” í‰ì  ë°ì´í„°ì…‹ ê°ì„±ë¶„ì„ ê²°ê³¼\",\n         x = \"ê°ì„±ì ìˆ˜: ë¶€ì •(1) --- ê¸ì •(5)\",\n         y = \"ì˜í™”í‰ì  ë¶€ì—¬ê±´ìˆ˜\",\n         fill = \"ê¸ë¶€ì •\") +\n    theme_minimal() +\n    theme(legend.position = \"top\")"
  },
  {
    "objectID": "BERT.html#í›„ì†-ë¶„ì„",
    "href": "BERT.html#í›„ì†-ë¶„ì„",
    "title": "chatGPT",
    "section": "\n3.3 í›„ì† ë¶„ì„",
    "text": "3.3 í›„ì† ë¶„ì„\ní‰ì  4ì ìœ¼ë¡œ ì˜ˆì¸¡ëœ ì˜í™” í‰ì  ì¤‘ ê¸ë¶€ì • 3ê°œ ë¦¬ë·°ë¥¼ ë½‘ì•„ ì§ì ‘ ì‚´í´ë³´ì.\n\nlibrary(reactable)\n\nsenti_tbl %>% \n  filter(sentiment == 4) %>% \n  group_by(label) %>% \n  slice_sample(n = 3) %>% \n  reactable::reactable(\n      columns = list(\n        text = colDef(width = 700),\n        label = colDef(width = 50),\n        sentiment = colDef(width = 50)\n  ),\n  fullWidth = TRUE\n  )"
  },
  {
    "objectID": "korBERT.html",
    "href": "korBERT.html",
    "title": "chatGPT",
    "section": "",
    "text": "1 BERT ëª¨í˜• ì„ ì •\në‹¤êµ­ì–´ë¥¼ ì§€ì›í•˜ëŠ” BERT ëª¨í˜•ì„ í™œìš©í•˜ì—¬ ì—°ê´€ ìì—°ì–´ ì²˜ë¦¬ ì—…ë¬´ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.\n\nbert-base-multilingual-cased\ndistilbert-base-multilingual-cased\n\ndistilbertëŠ” BERT ëª¨í˜•ê³¼ ë¹„êµí•˜ì—¬ ë‹¤ì†Œ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ë‚˜ í¬ê¸°, ì†ë„ ë“± ë‹¤ë¥¸ ì§€í‘œì—ì„œ BERTì™€ ëŒ€ë“±í•œ ì§€í‘œë¥¼ ì œì‹œí•˜ê³  ìˆë‹¤.\n\n2 ê°œì²´ëª… ì¸ì‹\nê°œì²´ëª…(Named Entity)ì€ ì¸ëª…, ê¸°ê´€ëª…, ì§€ëª… ë“±ê³¼ ê°™ì´ ë¬¸ì¥ ë˜ëŠ” ë¬¸ì„œì—ì„œ íŠ¹ì •í•œ ì˜ë¯¸ë¥¼ ê°€ì§€ê³  ìˆëŠ” ë‹¨ì–´ ë˜ëŠ” ì–´êµ¬ë¥¼ ì§€ì¹­í•¨.\n\në„¤ì´ë²„ ê°œì²´ëª…ì¸ì‹\ní•œêµ­í•´ì–‘ëŒ€í•™êµ - ì»´í“¨í„°ê³µí•™ê³¼ ìì—°ì–¸ì–´ì²˜ë¦¬ ì—°êµ¬ì‹¤\nêµ­ë¦½êµ­ì–´ì›\nAI í—ˆë¸Œ\n\n\n\në„¤ì´ë²„(14ì¢…)\ní•œêµ­í•´ì–‘ëŒ€ (10ì¢…)\nêµ­ë¦½êµ­ì–´ì›(5ì¢…)\nAI í—ˆë¸Œ(15ì¢…)\n\n\n\n\nlibrary(tidyverse)\n\nner_tbl <- tibble::tribble(\n            ~ê°œì²´ëª….ë²”ì£¼,   ~íƒœê·¸,                          ~ì •ì˜,\n           \"PERSON\", \"PER\",      \"ì‹¤ì¡´, ê°€ìƒ ë“± ì¸ë¬¼ëª…ì— í•´ë‹¹ í•˜ëŠ” ê²ƒ\",\n            \"FIELD\", \"FLD\",       \"í•™ë¬¸ ë¶„ì•¼ ë° ì´ë¡ , ë²•ì¹™, ê¸°ìˆ  ë“±\",\n  \"ARTIFACTS_WORKS\", \"AFW\",        \"ì¸ê³µë¬¼ë¡œ ì‚¬ëŒì— ì˜í•´ ì°½ì¡°ëœ ëŒ€ìƒë¬¼\",\n     \"ORGANIZATION\", \"ORG\",      \"ê¸°ê´€ ë° ë‹¨ì²´ì™€ íšŒì˜/íšŒë‹´ì„ ëª¨ë‘ í¬í•¨\",\n         \"LOCATION\", \"LOC\",            \"ì§€ì—­ëª…ì¹­ê³¼ í–‰ì •êµ¬ì—­ ëª…ì¹­ ë“±\",\n     \"CIVILIZATION\", \"CVL\",            \"ë¬¸ëª… ë° ë¬¸í™”ì— ê´€ë ¨ëœ ìš©ì–´\",\n             \"DATE\", \"DAT\",                         \"ë‚ ì§œ\",\n             \"TIME\", \"TIM\",                         \"ì‹œê°„\",\n           \"NUMBER\", \"NUM\",                         \"ìˆ«ì\",\n            \"EVENT\", \"EVT\",        \"íŠ¹ì • ì‚¬ê±´ ë° ì‚¬ê³  ëª…ì¹­ê³¼ í–‰ì‚¬ ë“±\",\n           \"ANIMAL\", \"ANM\",                         \"ë™ë¬¼\",\n            \"PLANT\", \"PLT\",                         \"ì‹ë¬¼\",\n         \"MATERIAL\", \"MAT\",             \"ê¸ˆì†, ì•”ì„, í™”í•™ë¬¼ì§ˆ ë“±\",\n             \"TERM\", \"TRM\", \"ì˜í•™ ìš©ì–´, ITê³¤ë ¨ ìš©ì–´ ë“± ì¼ë°˜ ìš©ì–´ë¥¼ ì´ì¹­\"\n  )\n\nner_tbl %>% \n  gt::gt()\n\n\n\n\n\n\nê°œì²´ëª….ë²”ì£¼\n      íƒœê·¸\n      ì •ì˜\n    \n\n\nPERSON\nPER\nì‹¤ì¡´, ê°€ìƒ ë“± ì¸ë¬¼ëª…ì— í•´ë‹¹ í•˜ëŠ” ê²ƒ\n\n\nFIELD\nFLD\ní•™ë¬¸ ë¶„ì•¼ ë° ì´ë¡ , ë²•ì¹™, ê¸°ìˆ  ë“±\n\n\nARTIFACTS_WORKS\nAFW\nì¸ê³µë¬¼ë¡œ ì‚¬ëŒì— ì˜í•´ ì°½ì¡°ëœ ëŒ€ìƒë¬¼\n\n\nORGANIZATION\nORG\nê¸°ê´€ ë° ë‹¨ì²´ì™€ íšŒì˜/íšŒë‹´ì„ ëª¨ë‘ í¬í•¨\n\n\nLOCATION\nLOC\nì§€ì—­ëª…ì¹­ê³¼ í–‰ì •êµ¬ì—­ ëª…ì¹­ ë“±\n\n\nCIVILIZATION\nCVL\në¬¸ëª… ë° ë¬¸í™”ì— ê´€ë ¨ëœ ìš©ì–´\n\n\nDATE\nDAT\në‚ ì§œ\n\n\nTIME\nTIM\nì‹œê°„\n\n\nNUMBER\nNUM\nìˆ«ì\n\n\nEVENT\nEVT\níŠ¹ì • ì‚¬ê±´ ë° ì‚¬ê³  ëª…ì¹­ê³¼ í–‰ì‚¬ ë“±\n\n\nANIMAL\nANM\në™ë¬¼\n\n\nPLANT\nPLT\nì‹ë¬¼\n\n\nMATERIAL\nMAT\nê¸ˆì†, ì•”ì„, í™”í•™ë¬¼ì§ˆ ë“±\n\n\nTERM\nTRM\nì˜í•™ ìš©ì–´, ITê³¤ë ¨ ìš©ì–´ ë“± ì¼ë°˜ ìš©ì–´ë¥¼ ì´ì¹­\n\n\n\n\n\n\n\n\ní•œêµ­ì–´ì—ì„œ ê°œì²´ì˜ ë²”ì£¼ëŠ” í¬ê²Œ ê°œì²´ì´ë¦„, ì‹œê°„í‘œí˜„, ìˆ˜ëŸ‰í‘œí˜„ìœ¼ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆë‹¤.\n\nê°œì²´ì´ë¦„: ì¸ëª…(PER), ì§€ëª…(LOC), ê¸°ê´€ëª…(ORG), ê¸°íƒ€(POH)\nì‹œê°„í‘œí˜„: ë‚ ì§œ(DAT), ì‹œê°„(TIM), ê¸°ê°„(DUR)\nìˆ˜ëŸ‰í‘œí˜„: í†µí™”(MNY), ë¹„ìœ¨(PNT), ê¸°íƒ€ ìˆ˜ëŸ‰í‘œí˜„(NOH)\n\n\n\nì¥ì†Œ(LC), ë‚ ì§œ(DT), ê¸°ê´€(OG), ì‹œê°„(TI), ì¸ë¬¼(PS)\n\n\nì‚¬ëŒ(PS), ì§€ì—­(LC), ë‹¨ì²´(OG), ì¸ê³µë¬¼(AF), ë‚ ì§œ(DT), ì‹œê°„(TI), ì œë„(CV), ë™ë¬¼(AM), ì‹ë¬¼(PT), ë‹¨ìœ„(QT), ë¶„ì•¼(FD), ì´ë¡ (TR), ì‚¬ê±´(EV), ë¬¼ì§ˆ(MT), ìš©ì–´(TM)"
  },
  {
    "objectID": "hf_pipeline.html",
    "href": "hf_pipeline.html",
    "title": "chatGPT",
    "section": "",
    "text": "ë‹¤êµ­ì–´ë¥¼ ì§€ì›í•˜ëŠ” BERT ëª¨í˜•ì„ í™œìš©í•˜ì—¬ ì—°ê´€ ìì—°ì–´ ì²˜ë¦¬ ì—…ë¬´ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.\n\nbert-base-multilingual-cased\ndistilbert-base-multilingual-cased\n\ndistilbertëŠ” BERT ëª¨í˜•ê³¼ ë¹„êµí•˜ì—¬ ë‹¤ì†Œ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ë‚˜ í¬ê¸°, ì†ë„ ë“± ë‹¤ë¥¸ ì§€í‘œì—ì„œ BERTì™€ ëŒ€ë“±í•œ ì§€í‘œë¥¼ ì œì‹œí•˜ê³  ìˆë‹¤.\n\n\n\n\n\n\në…¸íŠ¸\n\n\n\nfrom transformers import pipeline\n\n# Using default model and tokenizer for the task\npipeline(\"<task-name>\")\n\n# Using a user-specified model\npipeline(\"<task-name>\", model=\"<model_name>\")\n\n# Using custom model/tokenizer as str\npipeline('<task-name>', model='<model name>', tokenizer='<tokenizer_name>')\n\n\nHugging Face Transformers - How to use Pipelines"
  },
  {
    "objectID": "hf_pipeline.html#íŒŒì´ì¬-ì½”ë“œ",
    "href": "hf_pipeline.html#íŒŒì´ì¬-ì½”ë“œ",
    "title": "chatGPT",
    "section": "\n3.1 íŒŒì´ì¬ ì½”ë“œ",
    "text": "3.1 íŒŒì´ì¬ ì½”ë“œ\nHF íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜ë¬¸ ê°œì²´ëª…ì¸ì‹ ì‘ì—…ì„ ìˆ˜í–‰í•œë‹¤.\n# ê°ì„±ë¶„ì„ - íŒŒì´ì¬ ì½”ë“œ\n# ì¶œì²˜: https://www.kaggle.com/code/funtowiczmo/hugging-face-transformers-how-to-use-pipelines\n\nfrom transformers import pipeline\n\n# NER íŒŒì´í”„ë¼ì¸ ---------------------------------\nner = pipeline(task = \"ner\", \n               model=\"dbmdz/bert-large-cased-finetuned-conll03-english\",\n               tokenizer=\"bert-large-cased\")\n\ntext = \"John Smith works at Google\"\nentities = ner(text)\n\n# ê²°ê³¼ ì¶œë ¥\nfor entity in entities:\n    print(f\"{entity['word']} -> {entity['entity']}\")"
  },
  {
    "objectID": "hf_pipeline.html#ê°œì²´ëª…-ì¸ì‹ê²°ê³¼",
    "href": "hf_pipeline.html#ê°œì²´ëª…-ì¸ì‹ê²°ê³¼",
    "title": "chatGPT",
    "section": "\n3.2 ê°œì²´ëª… ì¸ì‹ê²°ê³¼",
    "text": "3.2 ê°œì²´ëª… ì¸ì‹ê²°ê³¼\n\nì½”ë“œlibrary(reticulate)\nreticulate::source_python(\"code/BERT/HF_pipeline_NER.py\")\n\nto_r <- function(idx){\n\n  output_idx = py$entities %>% \n    pluck(idx)\n  \n  output_idx$score = paste(output_idx$score) %>% \n    as.double()\n  \n  return(output_idx)\n}\n\nmap_dfr(1:length(py$entities), ~to_r(.x))\n#> # A tibble: 3 Ã— 6\n#>   entity score index word   start   end\n#>   <chr>  <dbl> <int> <chr>  <int> <int>\n#> 1 I-PER  0.999     1 John       0     4\n#> 2 I-PER  1.00      2 Smith      5    10\n#> 3 I-ORG  0.998     5 Google    20    26"
  },
  {
    "objectID": "project.html#chatgpt-ì´ì „",
    "href": "project.html#chatgpt-ì´ì „",
    "title": "chatGPT",
    "section": "\n8.1 chatGPT ì´ì „",
    "text": "8.1 chatGPT ì´ì „\nTensorflow, Keras, Pytorch, Fast.ai ê°€ ì°¨ë¡€ë¡œ ë“±ì¥í•˜ë©° ë”¥ëŸ¬ë‹ ê°œë°œ í”„ë ˆì„ì›Œí¬ì˜ ì „ì„±ê¸°ë¥¼ êµ¬ê°€í–ˆë‹¤. ìµœê·¼ 5ë…„ë™ì•ˆ Google ì¶”ì„¸ë¥¼ ì‚´í´ë³´ì.\n\n\n\n\n\nì½”ë“œlibrary(gtrendsR)\nextrafont::loadfonts()\n\nresult <- gtrends(keyword = c(\"pytorch\",\"fastai\", \"tensorflow\", \"keras\"), geo = \"\", \n                  time=\"today+5-y\", low_search_volume = TRUE)\n\ngtrends_framework_g <- result$interest_over_time %>% \n  as_tibble() %>% \n  mutate(keyword = factor(keyword, levels = c(\"keras\", \"pytorch\", \"tensorflow\", \"fastai\"))) %>% \n  mutate(hits = parse_number(hits)) %>% \n  ggplot(aes(x = date, y = hits, color = keyword)) +\n    geom_line() +\n    theme_bw(base_family = \"NanumBarunpen\") +\n    labs(x = \"\", \n         y = \"ê²€ìƒ‰ìˆ˜\",\n         color = \"í”„ë ˆì„ì›Œí¬\",\n         title = \"ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ êµ¬ê¸€ ê²€ìƒ‰ ì¶”ì„¸\") +\n    theme(legend.title = element_text(size = 16),\n          legend.text = element_text(size = 14))\n  \n\n# ragg always works for mac\nragg::agg_png(\"images/dl_framework.png\", width = 297, \n              height = 210, \n              units = \"mm\", res = 300)\ngtrends_framework_g\ndev.off()"
  },
  {
    "objectID": "project.html#chatgpt-ì¶œí˜„",
    "href": "project.html#chatgpt-ì¶œí˜„",
    "title": "chatGPT",
    "section": "\n8.2 chatGPT ì¶œí˜„",
    "text": "8.2 chatGPT ì¶œí˜„\nchatGPT ì¶œí˜„ì´í›„ Tensorflow, Keras, Pytorch, Fast.ai ëŠ” ì–´ë–»ê²Œ ì „ê°œë  ê²ƒì¸ì§€ ìµœê·¼ 1ë…„ë™ì•ˆ Google ì¶”ì„¸ë¥¼ ì‚´í´ë³´ì.\n\nì½”ë“œchatGPT_result <- gtrends(keyword = c(\"pytorch\",\"fastai\", \"tensorflow\", \"keras\", \"chatGPT\"), geo = \"\", \n                  time=\"today 12-m\", low_search_volume = TRUE)\n\ngtrends_chatGPT_g <- chatGPT_result$interest_over_time %>% \n  as_tibble() %>% \n  mutate(keyword = factor(keyword, levels = c(\"chatGPT\", \"keras\", \"pytorch\", \"tensorflow\", \"fastai\"))) %>% \n  mutate(hits = parse_number(hits)) %>% \n  mutate(date = as.Date(date)) %>% \n  ggplot(aes(x = date, y = hits, color = keyword)) +\n    geom_line() +\n    theme_bw(base_family = \"NanumBarunpen\") +\n    labs(x = \"\", \n         y = \"ê²€ìƒ‰ìˆ˜\",\n         color = \"í”„ë ˆì„ì›Œí¬\",\n         title = \"chatGPTì™€ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ êµ¬ê¸€ ê²€ìƒ‰ ì¶”ì„¸\") +\n    scale_x_date(date_labels = \"%Y-%m\") +\n    theme(legend.title = element_text(size = 16),\n          legend.text = element_text(size = 14))\n\n# ragg always works for mac\nragg::agg_png(\"images/chatGPT_framework.png\", width = 297, \n              height = 210, \n              units = \"mm\", res = 300)\ngtrends_chatGPT_g\ndev.off()"
  },
  {
    "objectID": "project.html#íŒŒì´ì¬ê³¼-chatgpt",
    "href": "project.html#íŒŒì´ì¬ê³¼-chatgpt",
    "title": "chatGPT",
    "section": "\n8.3 íŒŒì´ì¬ê³¼ chatGPT",
    "text": "8.3 íŒŒì´ì¬ê³¼ chatGPT\nchatGPT ì¶œí˜„ì´í›„ íŒŒì´ì¬, tensorflow, pytorch ìµœê·¼ 1ë…„ë™ì•ˆ Google ì¶”ì„¸ë¥¼ ì‚´í´ë³´ì.\n\nì½”ë“œpython_result <- gtrends(keyword = c(\"chatGPT\", \"pytorch\",\"python\", \"tensorflow\", \"keras\"), geo = \"\", \n                  time=\"today 12-m\", low_search_volume = TRUE)\n\npython_chatGPT_g <- python_result$interest_over_time %>% \n  as_tibble() %>% \n  mutate(keyword = factor(keyword, levels = c(\"chatGPT\", \"python\", \"keras\", \"pytorch\", \"tensorflow\"))) %>% \n  mutate(hits = parse_number(hits)) %>% \n  mutate(date = as.Date(date)) %>% \n  ggplot(aes(x = date, y = hits, color = keyword)) +\n    geom_line() +\n    theme_bw(base_family = \"NanumBarunpen\") +\n    labs(x = \"\", \n         y = \"ê²€ìƒ‰ìˆ˜\",\n         color = \"í”„ë ˆì„ì›Œí¬\",\n         title = \"íŒŒì´ì¬, chatGPT, ì£¼ìš” ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ êµ¬ê¸€ ê²€ìƒ‰ ì¶”ì„¸\") +\n    scale_x_date(date_labels = \"%Y-%m\") +\n    theme(legend.title = element_text(size = 16),\n          legend.text = element_text(size = 14))\n\n# ragg always works for mac\nragg::agg_png(\"images/python_chatGPT_g.png\", width = 297, \n              height = 210, \n              units = \"mm\", res = 300)\npython_chatGPT_g\ndev.off()"
  }
]