{
  "hash": "960249cda5e1616b575c902e0cdaca77",
  "result": {
    "markdown": "---\ntitle: \"chatGPT\"\nsubtitle: \"BERT(**B**idirectional **E**ncoder **R**epresentation from **T**ransformer)\"\nauthor:\n  - name: 이광춘\n    url: https://www.linkedin.com/in/kwangchunlee/\n    affiliation: 한국 R 사용자회\n    affiliation-url: https://github.com/bit2r\ntitle-block-banner: true\n#title-block-banner: \"#562457\"\nformat:\n  html:\n    css: css/quarto.css\n    theme: flatly\n    code-fold: false\n    code-overflow: wrap\n    toc: true\n    toc-depth: 3\n    toc-title: 목차\n    number-sections: true\n    highlight-style: pygments \n    self-contained: false\nfilters:\n   - lightbox\n   - include-code-files   \nlightbox: auto\nknitr:\n  opts_chunk: \n    message: false\n    warning: false\n    collapse: true\n    comment: \"#>\" \n    R.options:\n      knitr.graphics.auto_pdf: true\neditor_options: \n  chunk_output_type: console\nbibliography: bibliography.bib\nlink-citations: yes\ncsl: apa-single-spaced.csl\n---\n\n\n# Context-Free and Context-based model\n\n- Context-free models: Word2Vec, Glove, FastText\n- Context-embedding models(transformer based models): BERT, ELMO, Universal Sentence Encoder\n\nContext-free 모형은 단순하고 효율적이지만 텍스트의 뉴앙스를 비롯하여 의미를 잡아내는데 다소 미흡할 수 있다.\n반면에 Context-based model은 강력하고 유연하지만 컴퓨팅 자원을 많이 사용하고 더 복잡하다.\n\n\n![](images/BERT_workflow.png)\n\n# 질의응답\n\n## 파이썬 코드\n\nBERT 논문 [https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)의 초록(Abstract)에서 \n질의를 하고 관련 내용을 뽑아내는 코드를 다음과 같이 작성한다. [@ravichandiran2021getting]\n\n```{.python include=\"code/BERT/BERT_QnA.py\"}\n```\n\nBERT 임베딩 모형을 사용해서 질문과 응답을 파이썬 코드로 작성하고 나서 그 결과값을 R에서 바록 읽어 후처리 하도록 한다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate)\nlibrary(tidyverse)\n\nreticulate::source_python(\"code/BERT/BERT_QnA.py\")\n```\n:::\n\n\n## 질의응답 설정\n\nBERT를 사용해서 질문과 응답을 준비한다.\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\npy$question\n#> [1] \"[CLS] What does the 'B' in BERT stand for?[SEP]\"\n```\n:::\n\n:::\n\n::: {.column width=\"70%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\npy$abstract\n#> [1] \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).[SEP]\"\n```\n:::\n\n:::\n\n::::\n\n## 질의응답 결과\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# str_c(py$tokens[py$start_index$tolist()+1:py$end_index$tolist()+1], collapse = \" \")\npy$answer\n#> [1] \"bid ##ire ##ction ##al en ##code ##r representations from transformers\"\n```\n:::\n\n\n# 감성분석\n\n## 파이썬 코드\n\nIMDB 영화평점 텍스트에 담긴 감성분석을 BERT를 사용해서 수행한다.\n\n\n```{.python include=\"code/BERT/BERT_sentiment.py\"}\n```\n\n## 감성분석 결과\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsenti_raw <- read_csv('https://gist.githubusercontent.com/Mukilan-Krishnakumar/e998ecf27d11b84fe6225db11c239bc6/raw/74dbac2b992235e555df9a0a4e4d7271680e7e45/imdb_movie_reviews.csv')\n\nreticulate::source_python(\"code/BERT/BERT_sentiment.py\")\n\nsenti_tbl <- senti_raw %>% \n  rename(label = sentiment) %>% \n  bind_cols(py$df %>% select(sentiment))\n\n\nsenti_tbl %>% \n  count(label, sentiment) %>% \n  ggplot(aes(x = sentiment, y = n, fill = label)) +\n    geom_col(width = 0.3, alpha = 0.7) +\n    scale_fill_manual(values = c(\"red\", \"green\")) +\n    labs(title = \"IMDB 영화 평점 데이터셋 감성분석 결과\",\n         x = \"감성점수: 부정(1) --- 긍정(5)\",\n         y = \"영화평점 부여건수\",\n         fill = \"긍부정\") +\n    theme_minimal() +\n    theme(legend.position = \"top\")\n```\n\n::: {.cell-output-display}\n![](BERT_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n## 후속 분석\n\n평점 4점으로 예측된 영화 평점 중 긍부정 3개 리뷰를 뽑아 직접 살펴보자.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reactable)\n\nsenti_tbl %>% \n  filter(sentiment == 4) %>% \n  group_by(label) %>% \n  slice_sample(n = 3) %>% \n  reactable::reactable(\n      columns = list(\n        text = colDef(width = 700),\n        label = colDef(width = 50),\n        sentiment = colDef(width = 50)\n  ),\n  fullWidth = TRUE\n  )\n```\n\n::: {.cell-output-display}\n```{=html}\n<div class=\"reactable html-widget html-fill-item-overflow-hidden html-fill-item\" id=\"htmlwidget-68b86a2de4a49c86b9bc\" style=\"width:auto;height:auto;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-68b86a2de4a49c86b9bc\">{\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"text\":[\"Yes,the movie is not a piece of art but the first time I watched it I was 10 years old,my parents were out and I stayed home with my two brothers.It was May 1970(I know that because I found a note about the cycle of horror movies that one network had).It's one of the most vivid memories I have with the guys.We ended all in one bed and covered up to the head! Our very first horror movie! We kept talking about it for years and laughing about the moment.Those were horror movies.Nowadays horror movies are always the same.Or was it better when we were kids enjoying without analyzing the plot and the cast and the dialogs? Most sure it was that.But for me this is a great movie!\",\"At last. Here's a movie that does as much for the reputations of the men of Greece and Russia as \\\"Gigli\\\" did for the those of Mr. Affleck and Ms. Lo. FROM THE EDGE OF THE CITY details the sad and sordid lives of some young Russian émigrés who live in and around Athens and spend their time burglarizing cars, getting laid, pimping woman émigrés and prostituting themselves (\\\"But we're not gay because we don't do, you know.... And if we do, it's only once or twice. With the same guy.\\\") There is hardly anything here you have not seen before and better; only the Athens locale adds a little novelty--even then there's but a scene or two that's scenic. Writer/director Constantine Giannaris (\\\"3 Steps to Heaven\\\") offer a relatively generic 95 minutes, in which the standout moments involve how stupid, sexist and (from the looks of things) pretty much irredeemable most of these guys are. (Interestingly, the gayer the guy, the more redeemable he appears.) What really rankles is the treatment of the women. Greek and Russian males would seem to give the Italians a run for their money regarding that famous madonna/whore complex. Has life in Greece improved much for women since the time of Plato and Socrates? One has to wonder.<br /><br />If I seem to be equating Russians and Greeks in this review, I apologize, but even the non-émigrés pictured here (the cab driver, for instance) are creeps. According to another review on this site, the film (a hit on its home turf) was actually submitted by Greece for an Academy Award for Best Foreign Film. What this says about the state of Greek movie-making, I hesitate to ponder.\",\"The film is somewhat entertaining, but the greatest feature is Shalom Harlow's laughable performance. It has been 4 years since this movie was released and hopefully Harlow has gone through more training. Perhaps she should stick to the more worldly, somewhat corruptive characters that she has generated in other performances.\",\"I loved \\\"Flash Gordon\\\" as a child and watching the series again on DVD brings back such fond memories. Each 15-minute episode features the adventures of our hero Flash, the lovely Dale Arden, and intrepid Dr. Zarkov on the planet Mongo, with Flash escaping death at every turn: The Shark Men nearly drown him, he faces the Fire Monster in the Tunnel of Terror, and he's in mortal peril in the Static Room! <br /><br />The characters are still fun: Buster Crabbe is every bit the blonde dreamboat hero and Jean Rogers is a delicate and beautiful Dale Arden. Princess Aura still plots to steal Flash for herself, King Vultan of the Hawk Men still has his booming laugh and angel wings, and Ming the Merciless, Emperor of the Universe, is still giving everyone the evil eye and the creeps.<br /><br />This serial probably wouldn't interest children today with its hokey effects - oh, that spaceship! - but it's a fun bit of nostalgia for those who liked it the first time around. The actors play it straight and don't play down to kids. I appreciate that young viewers were expected to read the chapter synopses which had pretty big words in them.<br /><br />I'm glad this came out on DVD. It's a lot of fun to revisit this classic sci-fi serial.\",\"Nightbreed is not only great, it is also unique, even taking into account other Barker's movies, which never lack originality. An amazing adaptation of a very interesting idea for a book. For the horror genre, it has quite a few of subtle symbolics and references. Certainly a lot of fun to have, a a bit to think about, if one cares to. And, not to forget, a nice music score. Well, the special effects, as usual, get old faster than anything, but that is probably the only drawback. I've just seen it again after ten years, and I still find it something to recommend.\",\"A question immediately arises in this extremely idiosyncratic film: Who are the crazy people?<br /><br />The answer become less clear as the film goes on.<br /><br />Renee Zellweger loses the whiney note in her voice and, while her voice is still high, she is incredibly effective as the shell-shocked Betty. In fact, she is so effective I almost wanted her to be just a little more crazy because her created reality was so believable.<br /><br />This is the first time Ms Zellweger has been called upon to carry a film and she is more than equal to the task.<br /><br />Chris Rock  though as foul-mouthed as usual  is fairly subdued as Wesley. He is able to sublimate his manic energy and it only occasionally surfaces and always when it is needed most.<br /><br />There are some interesting allusions: the first time you see Betty she is dressed almost exactly like Dorothy Gale from the `Wizard of Oz'  then later in the film she is compared to Dorothy when she says she has never been out of Kansas before. At one point the song that Doris Day was best known for, Que Sera Sera' is on the soundtrack and then later Charlie (Morgan Freeman) describes her as having a whole Doris Day thing going on.'<br /><br />This is an extremely quirky film with good performances by everyone including the supporting cast.<br /><br />It has a surprising ending that, as contrary as it sounds, is actually fairly predictable.<br /><br />If for no other reason see this film just to listen to the master of the human voice: Morgan Freeman.\"],\"label\":[\"neg\",\"neg\",\"neg\",\"pos\",\"pos\",\"pos\"],\"sentiment\":[4,4,4,4,4,4]},\"columns\":[{\"id\":\"text\",\"name\":\"text\",\"type\":\"character\",\"width\":700},{\"id\":\"label\",\"name\":\"label\",\"type\":\"character\",\"width\":50},{\"id\":\"sentiment\",\"name\":\"sentiment\",\"type\":\"numeric\",\"width\":50}],\"dataKey\":\"61f766a890774ee4c6724575578d8f19\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n:::\n:::\n\n\n\n# BERT 실용화\n\nBERT가 좋은 성능을 보이는 것은 맞지만 너무 크기가 크기 때문에 실무적으로 사용하기에는 \n제약이 많다. 몇가지 기술이 개발되어 BERT를 사용하기 좋게 만드는 방법을 알아보자.\n\n## 접근방법\n\n- Quantization and Pruning\n- DistilBERT: Knowledge Distillation\n- ALBERT: A Lite BERT\n\n[[Samuel Sučík (August 8th, 2019), \"Compressing BERT for faster prediction\", RASA Blog](https://rasa.com/blog/compressing-bert-for-faster-prediction-2/)]{.aside}\n\n:::{layout-ncol=3}\n\n![Quantization](images/BERT_quantization.png)\n\n![Pruning](images/BERT_pruning.png)\n\n![Pruning](images/BERT_distillation.png)\n\n:::\n\n\n## 성능비교\n\nDistilBERT, A Lite BERT(ALBERT) 변형된 BERT 모형을 논문에 제시된 NLP 작업별 성능과 크기와 속도를 \nBERT-base 모형과 비교해보자. \n\n:::{.panel-tabset}\n\n### DistilBERT [@sanh2019distilbert]\n\n![](images/BERT_distillBERT.png)\n\n### ALBERT [@lan2019albert]\n\n![](images/BERT_ALBERT.png)\n\n\n:::\n\n\n\n\n",
    "supporting": [
      "BERT_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/core-js-2.5.3/shim.min.js\"></script>\r\n<script src=\"site_libs/react-17.0.0/react.min.js\"></script>\r\n<script src=\"site_libs/react-17.0.0/react-dom.min.js\"></script>\r\n<script src=\"site_libs/reactwidget-1.0.0/react-tools.js\"></script>\r\n<script src=\"site_libs/htmlwidgets-1.6.1/htmlwidgets.js\"></script>\r\n<link href=\"site_libs/reactable-0.4.1/reactable.css\" rel=\"stylesheet\" />\r\n<script src=\"site_libs/reactable-binding-0.4.1/reactable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}