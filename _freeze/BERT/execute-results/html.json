{
  "hash": "0d575203329eb4d1dc2bc158a6b86948",
  "result": {
    "markdown": "---\ntitle: \"chatGPT\"\nsubtitle: \"BERT(**B**idirectional **E**ncoder **R**epresentation from **T**ransformer)\"\nauthor:\n  - name: 이광춘\n    url: https://www.linkedin.com/in/kwangchunlee/\n    affiliation: 한국 R 사용자회\n    affiliation-url: https://github.com/bit2r\ntitle-block-banner: true\n#title-block-banner: \"#562457\"\nformat:\n  html:\n    css: css/quarto.css\n    theme: flatly\n    code-fold: false\n    code-overflow: wrap\n    toc: true\n    toc-depth: 3\n    toc-title: 목차\n    number-sections: true\n    highlight-style: pygments \n    self-contained: false\nfilters:\n   - lightbox\n   - include-code-files   \nlightbox: auto\nknitr:\n  opts_chunk: \n    message: false\n    warning: false\n    collapse: true\n    comment: \"#>\" \n    R.options:\n      knitr.graphics.auto_pdf: true\neditor_options: \n  chunk_output_type: console\nbibliography: bibliography.bib\nlink-citations: yes\ncsl: apa-single-spaced.csl\n---\n\n\n# Context-Free and Context-based model\n\n- Context-free models: Word2Vec, Glove, FastText\n- Context-embedding models(transformer based models): BERT, ELMO, Universal Sentence Encoder\n\nContext-free 모형은 단순하고 효율적이지만 텍스트의 뉴앙스를 비롯하여 의미를 잡아내는데 다소 미흡할 수 있다.\n반면에 Context-based model은 강력하고 유연하지만 컴퓨팅 자원을 많이 사용하고 더 복잡하다.\n\n\n![](images/BERT_workflow.png)\n\n# 질의응답\n\n## 파이썬 코드\n\nBERT 논문 [https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)의 초록(Abstract)에서 \n질의를 하고 관련 내용을 뽑아내는 코드를 다음과 같이 작성한다. [@ravichandiran2021getting]\n\n```{.python include=\"code/BERT/BERT_QnA.py\"}\n```\n\nBERT 임베딩 모형을 사용해서 질문과 응답을 파이썬 코드로 작성하고 나서 그 결과값을 R에서 바록 읽어 후처리 하도록 한다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate)\nlibrary(tidyverse)\n\nreticulate::source_python(\"code/BERT/BERT_QnA.py\")\n```\n:::\n\n\n## 질의응답 설정\n\nBERT를 사용해서 질문과 응답을 준비한다.\n\n:::: {.columns}\n\n::: {.column width=\"30%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\npy$question\n#> [1] \"[CLS] What does the 'B' in BERT stand for?[SEP]\"\n```\n:::\n\n:::\n\n::: {.column width=\"70%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\npy$abstract\n#> [1] \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).[SEP]\"\n```\n:::\n\n:::\n\n::::\n\n## 질의응답 결과\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr_c(py$tokens[py$start_index$tolist()+1:py$end_index$tolist()+1], collapse = \" \")\n#> [1] \"##ire ##ction ##al en ##code ##r representations from transformers . unlike recent language representation models ( peters et al . , 2018 ##a ; ra ##df ##ord et al . , 2018 ) , bert\"\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}