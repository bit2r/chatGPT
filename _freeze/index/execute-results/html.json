{
  "hash": "d1ebf636ce79843bdcec04497da6b0da",
  "result": {
    "markdown": "---\ntitle: \"chatGPT\"\nsubtitle: \"ë“¤ì–´ê°€ë©°\"\nauthor:\n  - name: ì´ê´‘ì¶˜\n    url: https://www.linkedin.com/in/kwangchunlee/\n    affiliation: í•œêµ­ R ì‚¬ìš©ìíšŒ\n    affiliation-url: https://github.com/bit2r\ntitle-block-banner: true\n#title-block-banner: \"#562457\"\nformat:\n  html:\n    css: css/quarto.css\n    theme: flatly\n    code-fold: false\n    toc: true\n    toc-depth: 3\n    toc-title: ëª©ì°¨\n    number-sections: true\n    highlight-style: github    \n    self-contained: false\nfilters:\n   - lightbox\nlightbox: auto\nknitr:\n  opts_chunk: \n    message: false\n    warning: false\n    collapse: true\n    comment: \"#>\" \n    R.options:\n      knitr.graphics.auto_pdf: true\neditor_options: \n  chunk_output_type: console\nbibliography: bibliography.bib\nlink-citations: yes\ncsl: apa-single-spaced.csl\n---\n\n\n\n# ëª¨í˜• í¬ê¸°\n\n\n:::{.panel-tabset}\n\n## 2019ë…„ \n\n![[@sanh2019distilbert]](images/LLM_2019.png)\n\n## 2021ë…„\n\n![[Efficient Natural Language Processing](https://hanlab.mit.edu/projects/efficientnlp_old/)](images/LLM_2021.png)\n\n## 2022ë…„ \n\n![[langcon 2023 by ì‹ ì •ê·œ](https://songys.github.io/2023Langcon/)(images/LLM_langcon2023.png)\n\n\n:::\n\n\n# ê±°ëŒ€ì–¸ì–´ëª¨í˜• ì„±ëŠ¥\n\n\n![[@wei2022emergent]](images/LLM_few_shot.png)\n\n\n# ìˆ˜í•™\n\n[@lewkowycz2022solving]{.aside}\n\n:::{.panel-tabset}\n\n## ë¯¸ë„¤ë¥´ë°” LM\n\n![](images/math_minerva.png)\n\n## ì†ìœ¼ë¡œ í’€ê¸°\n\n![](images/LLM_hand.jpg){width=50%}\n\n\n## ì‹œê°í™”\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\ngiven_line <- function(x)  10 + 4 * x\nsolve_line <- function(x) -10 + 4 *x\n\nggplot() +\n  geom_point(aes(x = 5, y = 10), size = 3) +\n  geom_function(fun = given_line, color = \"blue\", size = 1.5) +\n  geom_function(fun = solve_line, color = \"red\", size = 1.5, alpha = 0.5) +\n  theme_classic() +\n  scale_x_continuous(limits = c(-7, 7), breaks = seq(-7, 7, 1)) +\n  scale_y_continuous(limits = c(-20, 20), breaks = seq(-20, 20, 1)) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n## Sympy í•´ë²•\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sympy import *\n\nx, y, b = symbols('x y b')\n\ngiven_eq = y - (4*x + 10)\n\nparallel_eq = y - (4*x + b)\n\nintercept_eq = parallel_eq.subs([(x, 5), (y, 10)])\n\nsolveset(Eq(intercept_eq, 0), b)\n#> {-10}\n```\n:::\n\n\n:::\n\n# ì™œ ê±°ëŒ€ ì–¸ì–´ ëª¨í˜•ì¸ê°€?\n\nquestion-answering tasks (open-domain closed-book variant), cloze and sentence-completion tasks, Winograd-style tasks, in-context reading comprehension tasks, common-sense reasoning tasks, SuperGLUE tasks, and natural language inference tasksê°€ í¬í•¨ëœ ì´ 29ê°œ ì‘ì—… ì¤‘ 28ê°œ ì˜ì—­ì—ì„œ PaLM 540Bê°€ ì´ì „ ê±°ëŒ€ ì–¸ì–´ëª¨í˜• GLaM, GPT-3, Megatron-Turing NLG, Gopher, Chinchilla, LaMDA ì„ ê°€ë³ê²Œ ëŠ¥ê°€í–ˆë‹¤.\n\n[[Sharan Narang and Aakanksha Chowdhery (APRIL 04, 2022), \"Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance\", Software Engineers, Google Research](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)]{.aside}\n\n:::{.panel-tabset}\n\n## ê±°ëŒ€ëª¨í˜• ì§„í™”\n\n![](images/LLM_tree.gif)\n\n## 80ì–µ íŒ¨ëŸ¬ë¯¸í„°\n\n![](images/LLM_tree_8_billion.png)\n\n## 640ì–µ íŒ¨ëŸ¬ë¯¸í„°\n\n![](images/LLM_tree_62_billion.png)\n\n## 400ì–µ íŒ¨ëŸ¬ë¯¸í„°\n\n![](images/LLM_tree_40_billion.png)\n\n## 5,400ì–µ íŒ¨ëŸ¬ë¯¸í„°\n\n![](images/LLM_tree_540_billion.png)\n\n## ì„±ëŠ¥\n\n![](images/LLM_tree_performance.png)\n\n:::\n\n# ì‚¬ë¡€ (PaLM)\n\n5,400 ì–µ íŒ¨ëŸ¬ë¯¸í„°ë¥¼ ì¥ì°©í•œ Pathways Language Model (PaLM)ì˜ ì„±ëŠ¥ì„ ì‹¤ê°í•´ë³´ì.\n\n:::{.panel-tabset}\n\n## ë‹¤ì–‘í•œ ê¸°ëŠ¥\n\n![](images/PaLM_overview.gif)\n\n## ì¶”ë¡ \n\nì¶”ë¡ (Reasoning)\n\n![](images/PaLM_reasoning.png)\n\n\n## ì½”ë”©\n\nì½”ë”©(Code Generation)\n\n![](images/PaLM_coding.gif)\n\n:::\n\n\n# ê°œë°œë¹„\n\n[[Estimating ğŸŒ´PaLM's training cost](https://blog.heim.xyz/palm-training-cost/)]{.aside}\n\nì–¸ì–´ ëª¨í˜• ê°œë°œì€ 2010ë…„ ì´í›„ ê°œë°œë¹„ìš©ì´ ê¸‰ê²©íˆ ì¦ê°€í•˜ê³  ìˆìœ¼ë©° ê·¸ ì¶”ì„¸ëŠ” ìƒìƒì„ ì´ˆì›”í•œë‹¤.\n\n:::{.panel-tabset}\n\n## Our World in Data\n\n<iframe src=\"https://ourworldindata.org/grapher/artificial-intelligence-training-computation?yScale=linear&time=2017-06-12..2022-07-01\" loading=\"lazy\" style=\"width: 100%; height: 600px; border: 0px none;\"></iframe>\n\n## Lennart Heim\n\n<iframe src=\"https://ourworldindata.org/grapher/artificial-intelligence-training-computation?yScale=linear&time=earliest..latest&country=~PaLM+%28540B%29\" loading=\"lazy\" style=\"width: 100%; height: 600px; border: 0px none;\"></iframe>\n\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}