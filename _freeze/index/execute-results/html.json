{
  "hash": "d1ebf636ce79843bdcec04497da6b0da",
  "result": {
    "markdown": "---\ntitle: \"chatGPT\"\nsubtitle: \"들어가며\"\nauthor:\n  - name: 이광춘\n    url: https://www.linkedin.com/in/kwangchunlee/\n    affiliation: 한국 R 사용자회\n    affiliation-url: https://github.com/bit2r\ntitle-block-banner: true\n#title-block-banner: \"#562457\"\nformat:\n  html:\n    css: css/quarto.css\n    theme: flatly\n    code-fold: false\n    toc: true\n    toc-depth: 3\n    toc-title: 목차\n    number-sections: true\n    highlight-style: github    \n    self-contained: false\nfilters:\n   - lightbox\nlightbox: auto\nknitr:\n  opts_chunk: \n    message: false\n    warning: false\n    collapse: true\n    comment: \"#>\" \n    R.options:\n      knitr.graphics.auto_pdf: true\neditor_options: \n  chunk_output_type: console\nbibliography: bibliography.bib\nlink-citations: yes\ncsl: apa-single-spaced.csl\n---\n\n\n\n# 모형 크기\n\n\n:::{.panel-tabset}\n\n## 2019년 \n\n![[@sanh2019distilbert]](images/LLM_2019.png)\n\n## 2021년\n\n![[Efficient Natural Language Processing](https://hanlab.mit.edu/projects/efficientnlp_old/)](images/LLM_2021.png)\n\n## 2022년 \n\n![[langcon 2023 by 신정규](https://songys.github.io/2023Langcon/)(images/LLM_langcon2023.png)\n\n\n:::\n\n\n# 거대언어모형 성능\n\n\n![[@wei2022emergent]](images/LLM_few_shot.png)\n\n\n# 수학\n\n[@lewkowycz2022solving]{.aside}\n\n:::{.panel-tabset}\n\n## 미네르바 LM\n\n![](images/math_minerva.png)\n\n## 손으로 풀기\n\n![](images/LLM_hand.jpg){width=50%}\n\n\n## 시각화\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\ngiven_line <- function(x)  10 + 4 * x\nsolve_line <- function(x) -10 + 4 *x\n\nggplot() +\n  geom_point(aes(x = 5, y = 10), size = 3) +\n  geom_function(fun = given_line, color = \"blue\", size = 1.5) +\n  geom_function(fun = solve_line, color = \"red\", size = 1.5, alpha = 0.5) +\n  theme_classic() +\n  scale_x_continuous(limits = c(-7, 7), breaks = seq(-7, 7, 1)) +\n  scale_y_continuous(limits = c(-20, 20), breaks = seq(-20, 20, 1)) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n## Sympy 해법\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sympy import *\n\nx, y, b = symbols('x y b')\n\ngiven_eq = y - (4*x + 10)\n\nparallel_eq = y - (4*x + b)\n\nintercept_eq = parallel_eq.subs([(x, 5), (y, 10)])\n\nsolveset(Eq(intercept_eq, 0), b)\n#> {-10}\n```\n:::\n\n\n:::\n\n# 왜 거대 언어 모형인가?\n\nquestion-answering tasks (open-domain closed-book variant), cloze and sentence-completion tasks, Winograd-style tasks, in-context reading comprehension tasks, common-sense reasoning tasks, SuperGLUE tasks, and natural language inference tasks가 포함된 총 29개 작업 중 28개 영역에서 PaLM 540B가 이전 거대 언어모형 GLaM, GPT-3, Megatron-Turing NLG, Gopher, Chinchilla, LaMDA 을 가볍게 능가했다.\n\n[[Sharan Narang and Aakanksha Chowdhery (APRIL 04, 2022), \"Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance\", Software Engineers, Google Research](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)]{.aside}\n\n:::{.panel-tabset}\n\n## 거대모형 진화\n\n![](images/LLM_tree.gif)\n\n## 80억 패러미터\n\n![](images/LLM_tree_8_billion.png)\n\n## 640억 패러미터\n\n![](images/LLM_tree_62_billion.png)\n\n## 400억 패러미터\n\n![](images/LLM_tree_40_billion.png)\n\n## 5,400억 패러미터\n\n![](images/LLM_tree_540_billion.png)\n\n## 성능\n\n![](images/LLM_tree_performance.png)\n\n:::\n\n# 사례 (PaLM)\n\n5,400 억 패러미터를 장착한 Pathways Language Model (PaLM)의 성능을 실감해보자.\n\n:::{.panel-tabset}\n\n## 다양한 기능\n\n![](images/PaLM_overview.gif)\n\n## 추론\n\n추론(Reasoning)\n\n![](images/PaLM_reasoning.png)\n\n\n## 코딩\n\n코딩(Code Generation)\n\n![](images/PaLM_coding.gif)\n\n:::\n\n\n# 개발비\n\n[[Estimating 🌴PaLM's training cost](https://blog.heim.xyz/palm-training-cost/)]{.aside}\n\n언어 모형 개발은 2010년 이후 개발비용이 급격히 증가하고 있으며 그 추세는 상상을 초월한다.\n\n:::{.panel-tabset}\n\n## Our World in Data\n\n<iframe src=\"https://ourworldindata.org/grapher/artificial-intelligence-training-computation?yScale=linear&time=2017-06-12..2022-07-01\" loading=\"lazy\" style=\"width: 100%; height: 600px; border: 0px none;\"></iframe>\n\n## Lennart Heim\n\n<iframe src=\"https://ourworldindata.org/grapher/artificial-intelligence-training-computation?yScale=linear&time=earliest..latest&country=~PaLM+%28540B%29\" loading=\"lazy\" style=\"width: 100%; height: 600px; border: 0px none;\"></iframe>\n\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}