{
  "hash": "ef42447557c12f36085b183c6b0beda2",
  "result": {
    "markdown": "---\ntitle: \"chatGPT\"\nsubtitle: \"OpenAI GPT\"\ndescription: |\n  LangChain은 AI 기능 및 애플리케이션을 보다 쉽게 구축할 수 있도록 하는 데 중점을 둔 인기 있는 오픈 소스 라이브러리로, 특히 GPT 및 기타 언어 모델을 통합하는 데 중점을 두고 있다.\nauthor:\n  - name: 이광춘\n    url: https://www.linkedin.com/in/kwangchunlee/\n    affiliation: 한국 R 사용자회\n    affiliation-url: https://github.com/bit2r\ntitle-block-banner: true\n#title-block-banner: \"#562457\"\nformat:\n  html:\n    css: css/quarto.css\n    theme: flatly\n    code-fold: false\n    toc: true\n    toc-depth: 3\n    toc-title: 목차\n    number-sections: true\n    highlight-style: github    \n    self-contained: false\nfilters:\n   - lightbox\nlightbox: auto\nlink-citations: true\nknitr:\n  opts_chunk: \n    message: false\n    warning: false\n    collapse: true\n    comment: \"#>\" \n    R.options:\n      knitr.graphics.auto_pdf: true\neditor_options: \n  chunk_output_type: console\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n# ChatGPT\n\n**ChatGPT**는 간단히 말해 생성형 사전 학습된 트랜스포머(Generative Pre-trained Transformer)의 약자로, OpenAI의 GPT-3 거대 언어 모델 제품군에 기반한 챗봇으로 지도학습과 강화학습기법을 적용하여\n미세조정(fine-tuned)된 제품이자 서비스다. OpenAI GPT-3 모형은 크게 세가지가 있다.\n\n- GPT-3\n- Codex\n- 콘텐츠 필터 모델\n\n## GPT-3\n\nGPT-3은 자연어 처리 및 생성을 담당하는 모델로 인간의 언어 즉,\n자연어처럼 보이는 텍스트를 이해할 수 있다.\n한걸음 더 들어가면 속도와 성능에 따라 4가지 모델(A, B, C, D)로 구분된다.\n\n- text-davinci-003\n- text-curie-001\n- text-babbage-001\n- text-ada-001\n\n성능기준으로 보면 다음과 같이 정렬할 수 있는데 비용도 그에 따라 높아진다는 의미도 함축한다.\n\n`text-davinci-003` > `text-curie-001` > `text-babbage-001` > `text-ada-001`\n\n따라서, OpenAI는 다빈치 모델(`text-davinci-003`)을 통해 원하는 결과를 얻은 후에 \n다른 모델을 사용해 볼 것을 권장하는데 이유는 \n훨씬 저렴한 비용으로 많은 수의 유사한 작업을 수행할 수 있기 때문이다.\n\n### `text-ada-001`\n\n2,048개의 토큰 및 2019년 10월까지의 데이터 학습하여 이후 모형과 비교하여\n정확도나 성능에서 다소 밀리는 모습이지만 최적화를 통해 매우 빠르고 비용이 가장 저렴하다.\n\n### `text-babbage-001`\n\n2,048개의 토큰과 2019년 10월까지의 데이터 학습되었고 간단한 분류와 의미론적 분류에 효과적이다.\n\n### `text-curie-001`\n\n최대 2048개의 토큰을 지원하며 `text-davinci-003` 다음으로 뛰어난 성능을 보이는 GPT-3 모델이다.\n2019년 10월까지의 데이터로 학습되었기 때문에 `text-davinci-003`보다 정확도가 떨어지지만,\n번역, 복잡한 분류, 텍스트 분석 및 요약에 좋은 성능을 보이고 있어 \n`text-davinci-003`와 비교하여 가성비가 높다고 평가되고 있다.\n\n### `text-davinci-003`\n\n2021년 9월까지의 데이터로 훈련되었기 때문에 최신 정보를 제공하지 못한다는 한계는 있지만, \n앞선 GPT-3 모형과 비교하여 더 높은 품질을 제공한다.\n장점 중 하나는 최대 4,000개 토큰까지 요청할 수 있다는 점이 이전 모형과 큰 차별점이 된다.\n\n## 코덱스(Codex)\n\n코덱스는 프로그래밍 코드 이해 및 생성을 위한 것으로 `code-davinci-002`와 `code-cushman-001`가 있다. 또한, 코덱스는 GitHub Copilot을 구동하는 모델이기도 하다.\n파이썬, 자바스크립트, 고, 펄, PHP, 루비, 스위프트, 타입스크립트, SQL, 셸 등 12개 이상의 프로그래밍 언어를 지원할 뿐만 아니라 \n자연어로 표현된 주석(comment)를 이해하고 사용자를 대신하여 요청된 작업을 수행할 수 있다.\n\n### `code-cushman-001`\n\n복잡한 작업을 수행하는 데 있어서는 `code-davinci-002`가 더 강력하지만,\n많은 코드 생성 작업을 수행할 수 있고 `code-davinci-002` 보다 더 빠르고 저렴하다는 장점이 있다.\n\n### `code-davinci-002`\n\n자연어를 코드로 번역하는 데 탁월할 뿐만 아니라 코드를 자동 완성할 뿐만 아니라 보충 요소 삽입도 지원한다. 최대 8,000개의 토큰을 처리할 수 있으며 2021년 6월까지의 데이터로 학습되었다.\n\n\n## 콘텐츠 필터\n\n민감한 콘텐츠 제거하기 위한 필터 모형이다.\n민감하거나 안전하지 않을 수 있는 API 생성 텍스트를 감지할 수 있다. \n사용자가 사용할 AI 응용프로그램을 개발할 경우, 필터를 사용하여 모델이 부적절한 콘텐츠를 반환하는지 감지할 수 있다. 이 필터는 텍스트를 다음 3가지 범주로 나눈다.\n\n- 안전(safe)\n- 민감(sensitive)\n- 안전하지 않음(unsafe)\n\n# 임베딩\n\n**임베딩(Embedding)** 일반적으로 실수 벡터의 형태로 텍스트 데이터를 수치로 표현한 것으로, \n이를 통해서 표현된 벡터는 연속적인 벡터 공간에서 텍스트 데이터에 내재된 의미(semantic)를 포착한다.\n\n실용적인 관점에서 임베딩은 실제 객체와 관계를 벡터로 표현하는 방식이다. \n동일한 벡터 공간을 사용하여 두 사물이 얼마나 유사한지 측정하는 데 사용한다.\n\n## 텍스트 벡터 표현\n\n`text-embedding-ada-002` 모델은 빠르고 가성비가 뛰어난 임베딩 모델이다. \n\"대한민국 수도는 서울입니다.\" 이라는 문서를 벡터로 표현하면 다음과 같다.\n즉, 1,536 차원을 갖는 공간에 하나의 점으로 표현될 수 있다.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport os\nimport openai\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\nseoul_response = openai.Embedding.create(\n  model=\"text-embedding-ada-002\",\n  input=\"대한민국 수도는 서울입니다.\",\n)\n\nseoul_embedding = seoul_response[\"data\"][0]['embedding']\n\nprint(f'벡터길이: {len(seoul_embedding)}')\n#> 벡터길이: 1536\nprint(f'벡터 일부: {seoul_embedding[:10]}')\n#> 벡터 일부: [0.014487667009234428, -0.018017204478383064, 0.004892932251095772, -0.013761372305452824, -0.03111599199473858, 0.02515272982418537, -0.034505367279052734, 0.011550633236765862, -0.008014724589884281, -0.0020355363376438618]\n```\n:::\n\n\n마찬가지로 일본의 수도 도쿄도 벡터로 표현할 수 있다.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntokyo_response = openai.Embedding.create(\n  model=\"text-embedding-ada-002\",\n  input=\"일본 수도는 동경입니다.\",\n)\n\ntokyo_embedding = tokyo_response[\"data\"][0]['embedding']\nprint(f'벡터길이: {len(tokyo_embedding)}')\n#> 벡터길이: 1536\nprint(f'벡터 일부: {tokyo_embedding[:10]}')\n#> 벡터 일부: [0.010912451893091202, -0.013220978900790215, 0.009697099216282368, -0.011883447878062725, -0.03176635876297951, 0.03446714207530022, -0.02981150709092617, 0.008616785518825054, 0.017156407237052917, -0.0014468483859673142]\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}