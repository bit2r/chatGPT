{
  "hash": "9a2247e366b4bc289a796b44926bc80a",
  "result": {
    "markdown": "---\ntitle: \"chatGPT\"\nsubtitle: \"OpenAI GPT\"\ndescription: |\n  LangChain은 AI 기능 및 애플리케이션을 보다 쉽게 구축할 수 있도록 하는 데 중점을 둔 인기 있는 오픈 소스 라이브러리로, 특히 GPT 및 기타 언어 모델을 통합하는 데 중점을 두고 있다.\nauthor:\n  - name: 이광춘\n    url: https://www.linkedin.com/in/kwangchunlee/\n    affiliation: 한국 R 사용자회\n    affiliation-url: https://github.com/bit2r\ntitle-block-banner: true\n#title-block-banner: \"#562457\"\nformat:\n  html:\n    css: css/quarto.css\n    theme: flatly\n    code-fold: false\n    toc: true\n    toc-depth: 3\n    toc-title: 목차\n    number-sections: true\n    highlight-style: github    \n    self-contained: false\nfilters:\n   - lightbox\nlightbox: auto\nlink-citations: yes\nknitr:\n  opts_chunk: \n    message: false\n    warning: false\n    collapse: true\n    comment: \"#>\" \n    R.options:\n      knitr.graphics.auto_pdf: true\neditor_options: \n  chunk_output_type: console\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n# ChatGPT\n\n**ChatGPT**는 간단히 말해 생성형 사전 학습된 트랜스포머(Generative Pre-trained Transformer)의 약자로, OpenAI의 GPT-3 거대 언어 모델 제품군에 기반한 챗봇으로 지도학습과 강화학습기법을 적용하여\n미세조정(fine-tuned)된 제품이자 서비스다. OpenAI GPT-3 모형은 크게 세가지가 있다.\n\n- GPT-3\n- Codex\n- 콘텐츠 필터 모델\n\n## GPT-3\n\nGPT-3은 자연어 처리 및 생성을 담당하는 모델로 인간의 언어 즉,\n자연어처럼 보이는 텍스트를 이해할 수 있다.\n한걸음 더 들어가면 속도와 성능에 따라 4가지 모델(A, B, C, D)로 구분된다.\n\n- text-davinci-003\n- text-curie-001\n- text-babbage-001\n- text-ada-001\n\n성능기준으로 보면 다음과 같이 정렬할 수 있는데 비용도 그에 따라 높아진다는 의미도 함축한다.\n\n`text-davinci-003` > `text-curie-001` > `text-babbage-001` > `text-ada-001`\n\n따라서, OpenAI는 다빈치 모델(`text-davinci-003`)을 통해 원하는 결과를 얻은 후에 \n다른 모델을 사용해 볼 것을 권장하는데 이유는 \n훨씬 저렴한 비용으로 많은 수의 유사한 작업을 수행할 수 있기 때문이다.\n\n### `text-ada-001`\n\n2,048개의 토큰 및 2019년 10월까지의 데이터 학습하여 이후 모형과 비교하여\n정확도나 성능에서 다소 밀리는 모습이지만 최적화를 통해 매우 빠르고 비용이 가장 저렴하다.\n\n### `text-babbage-001`\n\n2,048개의 토큰과 2019년 10월까지의 데이터 학습되었고 간단한 분류와 의미론적 분류에 효과적이다.\n\n### `text-curie-001`\n\n최대 2048개의 토큰을 지원하며 `text-davinci-003` 다음으로 뛰어난 성능을 보이는 GPT-3 모델이다.\n2019년 10월까지의 데이터로 학습되었기 때문에 `text-davinci-003`보다 정확도가 떨어지지만,\n번역, 복잡한 분류, 텍스트 분석 및 요약에 좋은 성능을 보이고 있어 \n`text-davinci-003`와 비교하여 가성비가 높다고 평가되고 있다.\n\n### `text-davinci-003`\n\n2021년 9월까지의 데이터로 훈련되었기 때문에 최신 정보를 제공하지 못한다는 한계는 있지만, \n앞선 GPT-3 모형과 비교하여 더 높은 품질을 제공한다.\n장점 중 하나는 최대 4,000개 토큰까지 요청할 수 있다는 점이 이전 모형과 큰 차별점이 된다.\n\n## 코덱스(Codex)\n\n코덱스는 프로그래밍 코드 이해 및 생성을 위한 것으로 `code-davinci-002`와 `code-cushman-001`가 있다. 또한, 코덱스는 GitHub Copilot을 구동하는 모델이기도 하다.\n파이썬, 자바스크립트, 고, 펄, PHP, 루비, 스위프트, 타입스크립트, SQL, 셸 등 12개 이상의 프로그래밍 언어를 지원할 뿐만 아니라 \n자연어로 표현된 주석(comment)를 이해하고 사용자를 대신하여 요청된 작업을 수행할 수 있다.\n\n### `code-cushman-001`\n\n복잡한 작업을 수행하는 데 있어서는 `code-davinci-002`가 더 강력하지만,\n많은 코드 생성 작업을 수행할 수 있고 `code-davinci-002` 보다 더 빠르고 저렴하다는 장점이 있다.\n\n### `code-davinci-002`\n\n자연어를 코드로 번역하는 데 탁월할 뿐만 아니라 코드를 자동 완성할 뿐만 아니라 보충 요소 삽입도 지원한다. 최대 8,000개의 토큰을 처리할 수 있으며 2021년 6월까지의 데이터로 학습되었다.\n\n\n## 콘텐츠 필터\n\n민감한 콘텐츠 제거하기 위한 필터 모형이다.\n민감하거나 안전하지 않을 수 있는 API 생성 텍스트를 감지할 수 있다. \n사용자가 사용할 AI 응용프로그램을 개발할 경우, 필터를 사용하여 모델이 부적절한 콘텐츠를 반환하는지 감지할 수 있다. 이 필터는 텍스트를 다음 3가지 범주로 나눈다.\n\n- 안전(safe)\n- 민감(sensitive)\n- 안전하지 않음(unsafe)\n\n\n\n\n\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}