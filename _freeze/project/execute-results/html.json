{
  "hash": "162f4f6f0fd7b31a7482d150ce262ae7",
  "result": {
    "markdown": "---\ntitle: \"chatGPT\"\nsubtitle: \"프로젝트\"\nauthor:\n  - name: 이광춘\n    url: https://www.linkedin.com/in/kwangchunlee/\n    affiliation: 한국 R 사용자회\n    affiliation-url: https://github.com/bit2r\ntitle-block-banner: true\n#title-block-banner: \"#562457\"\nformat:\n  html:\n    css: css/quarto.css\n    theme: flatly\n    code-fold: true\n    toc: true\n    toc-depth: 3\n    toc-title: 목차\n    number-sections: true\n    highlight-style: github    \n    self-contained: false\nfilters:\n   - lightbox\nlightbox: auto\nlink-citations: yes\nknitr:\n  opts_chunk: \n    message: false\n    warning: false\n    collapse: true\n    comment: \"#>\" \n    R.options:\n      knitr.graphics.auto_pdf: true\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n# 1 백만 사용자 {{< fa solid rocket >}}\n\n1 백만 가입자를 가질 때까지 걸린 소요시간을 보면 chatGPT 의 영향력을 파악할 수 있다.\n\n![](images/one_million_users.jpg)\n\n# 패러다임\n\n[![Andrej Karpathy Twit](images/software_3.jpeg)]{.aside}\n\n[- Pre-Software: Special-purpose computer <br>\n- Software 1.0: Design the Algorithm <br>\n- Software 2.0: Design the Dataset <br>\n- Software 3.0: Design the Prompt]{.aside}\n\n:::{.panel-tabset .column-page}\n\n## Feature Engineering\n\n- **패러다임**: (비신경망)지도학습 (Fully Supervised Learning)\n- **전성기**: 2015년까지 최고 전성기 구가\n- **특징**\n  1. 주로 비신경망 기계학습이 사용\n  1. 수작업으로 Feature를 추출\n- **대표작**  \n  1. 수작업 Feature 추출 후 SVM(support vector machine) 기계학습 모형\n  1. 수작업 Feature 추출 후 CRF(conditional random fields)\n  \n## Architecture Engineering\n\n- **패러다임**: 신경망 지도학습(Fully Supervised Learning)\n- **전성기**: 대략 2013~2018\n- **특징**\n  1. 신경망(Neural Network) 의존\n  1. 수작업으로 Feature를 손볼 필요는 없으나 신경망 네트워크는 수정해야 함(LSTM vs CNN)\n  1. 종종 사전학습된 언어모형을 사용하나 임베딩(embedding) 같은 얕은(shallow) Feature를 적용\n- **대표작**  \n  1. 텍스트 분류작업에 CNN 사용\n\n## Objective Engineering\n\n- **패러다임**: 사전학습(pre-training), 미세조정(fine-tuning)\n- **전성기**: 2017~현재\n- **특징**\n  1. 사전학습된 언어모형을 전체 모형의 초기값으로 사용\n  1. 아키텍쳐 디자인에 작업이 덜 필요하지만 목적함수(Objective function) 엔지니어링은 필요\n- **대표작**  \n  1. BERT &rarr; Fine Tuning\n\n## Prompt Engineering\n\n- **패러다임**: 사전학습(pre-training), 프롬프트(Prompt), 예측(Predict)\n- **전성기**: 2019~현재\n- **특징**\n  1. NLP 작업이 언어모형(Language Model)에 전적으로 의존\n  1. 얕던 깊던 Feature 추출, 예측 등 작업이 전적으로 언어모형에 의존\n  1. 프롬프트 공학이 필요\n- **대표작**  \n  1. GPT3\n\n:::\n\n# Prompt engineering\n\n- Instructions\n- Question\n- Input data\n- Examples\n\n[[Xavier (Xavi) Amatriain(January 5, 2023), \"Prompt Engineering 101 - Introduction and resources\", Linkedin](https://www.linkedin.com/pulse/prompt-engineering-101-introduction-resources-amatriain/),[Prompt Engineering - Learn how to use AI models with prompt engineering](https://microsoft.github.io/prompt-engineering/)]{.aside}\n\n# Genearative AI\n\n- 구분\n  - generation: text &rarr; image\n  - classification: image &rarr; text\n  - transformation: image &rarr; image (or text &rarr; text)\n\n- AI 프로젝트\n  - GPT-3\n  - Dalle.2 (text-to-image)\n  - Meta’s AI (text-to-video)\n  - Google AI (text-to-video)\n  - Stable Diffusion (text-to-image)\n  - Tesla AI (humanoid robot + self-driving)\n\n- text-to-X\n  - text-to-gif (T2G)\n  - text-to-3D (T2D)\n  - text-to-text (T2T)\n  - text-to-NFT (T2N)\n  - text-to-code (T2C)\n  - text-to-image (T2I)\n  - text-to-audio (T2S)\n  - text-to-video (T2V)\n  - text-to-music (T2M)\n  - text-to-motion (T2Mo)\n- 기타\n  - brain-to-text (B2T)\n  - image-to-text (I2T)\n  - speech-to-text (S2T)\n  - audio-to-audio (A2A)\n  - tweet-to-image (Tt2I)\n  - text-to-sound (T2S)\n\n\n![](images/generative_ai.jpg)\n\n\n# 데이터와 하드웨어\n\n![](images/cpu_gpu.jpg)\n\n# Open Assistant\n\n\n::: {#fig-assistant layout-ncol=3}\n\n![Label Asssistant Reply](images/OA_label_assistant_reply.png)\n\n![Initial Prompt](images/OA_label_initial_prompt.png)\n\n![Label Prompter Reply](images/OA_label_prompter_reply.png)\n\n![Reply as Assistant](images/OA_reply_as_assistant.png)\n\n![Reply as User](images/OA_reply_as_user.png)\n\n\n[Open Assistant](https://open-assistant.io/dashboard)\n:::\n\n\n# GPT-3 언어 데이터\n\nGPT-3 개발에 투입된 문서갯수를 언어별로 살펴보자.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(gt)\nlibrary(countrycode)\nlibrary(rvest)\nlibrary(gtExtras)\n\n## 언어 코드 \nlang_tbl <- read_html(x = 'http://www.lingoes.net/en/translator/langcode.htm') %>% \n  html_element(css = 'body > table') %>% \n  html_table() %>% \n  set_names(c(\"언어\", \"언어명\"))\n\n\ngpt_raw <- read_csv(\"https://raw.githubusercontent.com/openai/gpt-3/master/dataset_statistics/languages_by_document_count.csv\")\n\ngpt_tbl <- gpt_raw %>% \n  set_names(c(\"언어\", \"문서수\", \"비중\")) %>% \n  mutate(비중 = parse_number(비중) / 100) %>% \n  mutate(누적문서 = cumsum(문서수)) %>% \n  mutate(누적비중 = 누적문서 / sum(문서수)) %>% \n  top_n(문서수, n = 28)  \n\ngpt_gt <- gpt_tbl %>% \n  left_join(lang_tbl, by = \"언어\") %>% \n  select(언어, 언어명, 문서수, 비중, 누적비중) %>% \n  ## 표 \n  gt() %>% \n    gt_theme_nytimes() %>%   \n    tab_header(\n      title = md(\"**GPT-3 언어모형 개발에 사용된 언어별 문서 통계**\"),\n      subtitle = \"한국어 포함 상위 28개 언어\") %>% \n    tab_source_note(\n      source_note = \"자료출처: https://github.com/openai/gpt-3/blob/master/dataset_statistics/languages_by_document_count.csv\") %>% \n    tab_spanner(\n      label = \"언어코드와 언어명\",\n      columns = c(언어, 언어명)) %>% \n    tab_spanner(\n      label = \"통계수치\",\n      columns = c(문서수, 비중, 누적비중)) %>% \n    cols_align(\n      align = \"center\",\n      columns = c(언어, 언어명)) %>% \n    # tab_style(\n    #   style = cell_text(size = px(12)),\n    #   locations = cells_body(\n    #     columns = c(문서수, 비중, 누적비중)\n    #   )\n    # )  %>% \n    fmt_percent(\n      columns = c(비중, 누적비중),\n      decimals = 2\n    )  %>% \n    fmt_number(\n      columns = 문서수,\n      decimals = 0,\n      sep_mark = \",\"\n    )   %>% \n   gt_highlight_rows(\n     rows = c(1,28),\n     fill = \"lightgrey\",\n     target_col = 언어\n   )  %>% \n  sub_missing(\n    columns = everything(),\n    missing_text = \"-\"\n  )  \n\ngpt_gt %>% \n  gtsave(\"images/gpt_lang.png\")\n\n```\n:::\n\n\n![](images/gpt_lang.png)\n\n\n# 구글 트렌드\n\n## chatGPT 이전\n\nTensorflow, Keras, Pytorch, Fast.ai 가 차례로 등장하며 딥러닝 개발 프레임워크의 전성기를 구가했다.\n최근 5년동안 Google 추세를 살펴보자.\n\n![](images/pytorch_tensorflow.png)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(gtrendsR)\nextrafont::loadfonts()\n\nresult <- gtrends(keyword = c(\"pytorch\",\"fastai\", \"tensorflow\", \"keras\"), geo = \"\", \n                  time=\"today+5-y\", low_search_volume = TRUE)\n\ngtrends_framework_g <- result$interest_over_time %>% \n  as_tibble() %>% \n  mutate(keyword = factor(keyword, levels = c(\"keras\", \"pytorch\", \"tensorflow\", \"fastai\"))) %>% \n  mutate(hits = parse_number(hits)) %>% \n  ggplot(aes(x = date, y = hits, color = keyword)) +\n    geom_line() +\n    theme_bw(base_family = \"NanumBarunpen\") +\n    labs(x = \"\", \n         y = \"검색수\",\n         color = \"프레임워크\",\n         title = \"딥러닝 프레임워크 구글 검색 추세\") +\n    theme(legend.title = element_text(size = 16),\n          legend.text = element_text(size = 14))\n  \n\n# ragg always works for mac\nragg::agg_png(\"images/dl_framework.png\", width = 297, \n              height = 210, \n              units = \"mm\", res = 300)\ngtrends_framework_g\ndev.off()\n\n```\n:::\n\n\n![](images/dl_framework.png)\n\n## chatGPT 출현\n\nchatGPT 출현이후 Tensorflow, Keras, Pytorch, Fast.ai 는 어떻게 전개될 것인지\n최근 1년동안 Google 추세를 살펴보자.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchatGPT_result <- gtrends(keyword = c(\"pytorch\",\"fastai\", \"tensorflow\", \"keras\", \"chatGPT\"), geo = \"\", \n                  time=\"today 12-m\", low_search_volume = TRUE)\n\ngtrends_chatGPT_g <- chatGPT_result$interest_over_time %>% \n  as_tibble() %>% \n  mutate(keyword = factor(keyword, levels = c(\"chatGPT\", \"keras\", \"pytorch\", \"tensorflow\", \"fastai\"))) %>% \n  mutate(hits = parse_number(hits)) %>% \n  mutate(date = as.Date(date)) %>% \n  ggplot(aes(x = date, y = hits, color = keyword)) +\n    geom_line() +\n    theme_bw(base_family = \"NanumBarunpen\") +\n    labs(x = \"\", \n         y = \"검색수\",\n         color = \"프레임워크\",\n         title = \"chatGPT와 딥러닝 프레임워크 구글 검색 추세\") +\n    scale_x_date(date_labels = \"%Y-%m\") +\n    theme(legend.title = element_text(size = 16),\n          legend.text = element_text(size = 14))\n\n# ragg always works for mac\nragg::agg_png(\"images/chatGPT_framework.png\", width = 297, \n              height = 210, \n              units = \"mm\", res = 300)\ngtrends_chatGPT_g\ndev.off()\n\n```\n:::\n\n\n![](images/chatGPT_framework.png)\n\n## 파이썬과 chatGPT\n\nchatGPT 출현이후 파이썬, tensorflow, pytorch 최근 1년동안 Google 추세를 살펴보자.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npython_result <- gtrends(keyword = c(\"chatGPT\", \"pytorch\",\"python\", \"tensorflow\", \"keras\"), geo = \"\", \n                  time=\"today 12-m\", low_search_volume = TRUE)\n\npython_chatGPT_g <- python_result$interest_over_time %>% \n  as_tibble() %>% \n  mutate(keyword = factor(keyword, levels = c(\"chatGPT\", \"python\", \"keras\", \"pytorch\", \"tensorflow\"))) %>% \n  mutate(hits = parse_number(hits)) %>% \n  mutate(date = as.Date(date)) %>% \n  ggplot(aes(x = date, y = hits, color = keyword)) +\n    geom_line() +\n    theme_bw(base_family = \"NanumBarunpen\") +\n    labs(x = \"\", \n         y = \"검색수\",\n         color = \"프레임워크\",\n         title = \"파이썬, chatGPT, 주요 딥러닝 프레임워크 구글 검색 추세\") +\n    scale_x_date(date_labels = \"%Y-%m\") +\n    theme(legend.title = element_text(size = 16),\n          legend.text = element_text(size = 14))\n\n# ragg always works for mac\nragg::agg_png(\"images/python_chatGPT_g.png\", width = 297, \n              height = 210, \n              units = \"mm\", res = 300)\npython_chatGPT_g\ndev.off()\n\n```\n:::\n\n\n![](images/python_chatGPT_g.png)\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}