{
  "hash": "4be7a7178041020b888bafb897e1261b",
  "result": {
    "markdown": "---\ntitle: \"chatGPT\"\nsubtitle: \"NLP와 LLM\"\nauthor:\n  - name: 이광춘\n    url: https://www.linkedin.com/in/kwangchunlee/\n    affiliation: 한국 R 사용자회\n    affiliation-url: https://github.com/bit2r\ntitle-block-banner: true\n#title-block-banner: \"#562457\"\nformat:\n  html:\n    css: css/quarto.css\n    theme: flatly\n    code-fold: true\n    code-overflow: wrap\n    toc: true\n    toc-depth: 3\n    toc-title: 목차\n    number-sections: true\n    highlight-style: github    \n    self-contained: false\nfilters:\n   - lightbox\n   - custom-callout.lua   \nlightbox: auto\nlink-citations: true\nknitr:\n  opts_chunk: \n    message: false\n    warning: false\n    collapse: true\n    comment: \"#>\" \n    R.options:\n      knitr.graphics.auto_pdf: true\neditor_options: \n  chunk_output_type: console\n---\n\n\n# 자연어 처리\n\n자연어 처리는 컴퓨터로 하여금 사람이 작성한 언어(음성과 글)의 의미를 이해시키는 것을 목표로 한다.\n\n1. 텍스트 데이터\n    - 트위터\n    - 소설, 신문기사\n    - 고객 평점과 리뷰\n    - 전자우편\n    - 의무기록\n    - ...\n1. 텍스트 저장 형식\n    - 뉴스 등 웹 페이지\n    - PDF/워드/한글 문서\n    - 트위터 등 SNS, RSS 피드, 댓글\n    - ...\n1. 응용분야\n    - 감성분석\n    - 텍스트 분류 \n    - 번역\n    - 챗봇\n    - 개인 비서\n    - ...\n1. 기술\n    - 단어주머니(Bag of Words)\n    - Word Embedding, 워드투벡(Word2Vec)\n    - RNN, LSTM\n    - BERT, Transformer\n    - GPT, 거대언어모형(LLM)\n    - ...\n\n# 작업흐름\n\n감성 분석 및 텍스트 분류 등 텍스트를 데이터로 하는 전통적인 자연어 처리 작업은 다음과 같은 작업흐름을 갖게 된다.\n\n::::: {.columns}\n\n::: {.column width=\"40%\"}\n\n```{mermaid}\ngraph TD\n    A[데이터 수집] --> B[데이터 전처리]\n    B --> C[피쳐 추출]\n    C --> D[훈련-테스트 데이터셋 분할]\n    D --> E[모형 선택]\n    E --> F[모형 학습]\n    F --> G[모형 평가]\n    G --> H[하이퍼파라미터 튜닝]\n    H --> I[모형 배포]\n    I --> J[모니터링 및 유지보수]\n```\n\n:::\n\n::: {.column width=\"60%\"}\n1. 데이터 수집: 텍스트 데이터와 해당 레이블이 포함된 데이터셋을 수집한다. 감성 분석의 경우, 라벨은 '긍정', '부정' 또는 '중립'이 되고, 텍스트 분류의 경우 레이블은 다양한 주제나 카테고리를 나타낼 수 있다. 즉, 자연어 처리 목적에 맞춰 라벨을 특정하고 연관 데이터를 수집한다.\n\n1. 데이터 전처리: 텍스트 데이터를 정리하고 전처리하여 추가 분석에 적합하도록 작업하는데 소문자화, \n토큰화, 불용어 제거, 특수문자 제거, 어간 단어 기본형으로 줄이기 등이 포함된다.\n\n1. 피쳐 추출:사전 처리된 텍스트를 기계 학습 알고리즘에 적합한 숫자 형식으로 변환하는 과정으로 BoW, TF-IDF, 단어 임베딩 등이 흔히 사용되는 기법이다.\n\n1. 훈련-시험 데이터셋 분할: 일반적으로 70-30, 80-20 또는 기타 원하는 분할 비율을 사용하여 데이터셋을 훈련과 시험 데이터셋으로 구분한다.\n\n1. 모형 선택: 적합한 통계, 머신 러닝, 딥러닝 모델을 선정한다.\n\n1. 모형 학습: 적절한 최적화 알고리즘과 손실 함수를 사용하여 훈련 데이터셋에서 선택한 모델을 학습시킨다.\n\n1. 모형 평가: 정확도, 정밀도, 리콜, F1 점수 또는 ROC 곡선 아래 영역과 같은 관련 메트릭을 사용하여 시험 데이터셋에서 학습 모형의 성능을 평가한다.\n\n1. 하이퍼파라미터 튜닝: 격자 검색 또는 무작위 검색과 같은 기술을 사용하여 모형의 하이퍼파라미터를 최적화하여 성능을 개선한다.\n\n1. 모형 배포: 모형을 학습하고 최적화한 후에는 실제 환경에서 사용할 수 있도록 실제 운영 환경에 배포하여 가치를 창출한다.\n\n1. 모니터링 및 유지 관리: 배포된 모형의 성능을 지속적으로 모니터링하고 필요에 따라 새로운 학습데이터로 업데이트하여 정확성과 효율성을 유지한다.\n:::\n\n:::::\n\n\n# 자연어 처리 작업\n\n자연어 처리 분야에서 흔히 접하는 상위 10가지 NLP으로 다음을 들 수 있다.\n\n1. 감정 분석: 긍정, 부정, 중립 등 주어진 텍스트에 표현된 감정을 파악.\n\n1. 텍스트 분류: 텍스트 데이터를 미리 정의된 클래스 또는 주제(예: 스포츠, 정치, 연예 등)로 분류.\n\n1. 개체명 인식(NER): 텍스트 내에서 사람, 조직, 위치, 날짜 등의 명명된 개체(entity)를 식별하고 분류.\n\n1. 품사(POS) 태깅: 주어진 텍스트의 단어에 문법적 레이블(예: 명사, 동사, 형용사)을 할당.\n\n1. 의존성 구문 분석: 문장 내 단어 간의 문법 구조와 관계를 식별.\n\n1. 기계 번역: 영어에서 스페인어로 또는 중국어에서 프랑스어로와 같이 한 언어에서 다른 언어로 텍스트를 번역.\n\n1. 질의 응답: 자연어로 제기된 질문을 이해하고 답변할 수 있는 시스템을 개발.\n\n1. 텍스트 요약: 주요 아이디어와 정보를 보존하면서 주어진 텍스트에 대한 간결한 요약을 생성.\n\n1. 상호참조해결(Coreference Resolution): 텍스트에서 두 개 이상의 단어나 구가 동일한 개체 또는 개념을 지칭하는 경우 식별.\n\n1. 텍스트 생성: 주어진 입력, 컨텍스트 또는 일련의 조건에 따라 일관되고 의미 있는 텍스트를 생성.\n\n자연어 처리 작업과 작업흐름을 서로 연결하게 되면 다음과 같이 개별적으로 중복되고 분리된 작업을 수행하게 되는 문제가 있다.\n\n\n\n```{mermaid}\ngraph LR\nA[\"Data Collection <br> Preprocessing\"] --> B[\"Feature Extraction <br> Model Training\"]\nB --> C[\"Model Evaluation <br> Tuning\"]\nC --> D[\"Model Deployment <br> Maintenance\"]\n\nD --> T1[1. Sentiment Analysis]\nD --> T2[2. Text Classification]\nD --> T3[3. Named Entity Recognition]\nD --> T4[4. Part-of-Speech Tagging]\nD --> T5[5. Dependency Parsing]\nD --> T6[6. Machine Translation]\nD --> T7[7. Question Answering]\nD --> T8[8. Text Summarization]\nD --> T9[9. Coreference Resolution]\nD --> T10[10. Text Generation]\n\nclass A,B,C,D nodeStyle\nclass T1,T2,T3,T4,T5,T6,T7,T8,T9,T10 taskStyle\n\nclassDef nodeStyle fill:#93c47d,stroke:#000000,stroke-width:0.7px,font-weight:bold,font-size:14px;\nclassDef taskStyle fill:#fdfd96,stroke:#000000,stroke-width:0.7px,font-weight:bold,font-size:12px;\n```\n\n\n# 비교\n\n\n```{mermaid}\ngraph TB\n\nsubgraph \"거대언어기반 NLP 작업흐름 <br>\"\ndirection TB\n  A2[Pretraining] --> B2[Fine-tuning]\n  B2 --> C2[Model Training & Evaluation]\n  C2 --> D2[Hyperparameter Tuning]\n  D2 --> E2[Deployment & Maintenance]\nend\n\nsubgraph \"전통적인 NLP 작업흐름 <br>\"\ndirection TB\n  A1[Data Collection & Preprocessing] --> B1[Feature Extraction & Model Selection]\n  B1 --> C1[Model Training & Evaluation]\n  C1 --> D1[Hyperparameter Tuning]\n  D1 --> E1[Deployment & Maintenance]\nend\n\nclass A1,B1,C1,D1,E1,A2,B2,C2,D2,E2 nodeStyle\n\nclassDef nodeStyle fill:#ffffff,stroke:#000000,stroke-width:1px,font-weight:bold,font-size:14px;\n\n```\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}