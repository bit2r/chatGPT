{
  "hash": "037b244dd2672f281825ff898c33bde0",
  "result": {
    "markdown": "---\ntitle: \"chatGPT\"\nsubtitle: \"음성인식(Whisper)\"\nauthor:\n  - name: 이광춘\n    url: https://www.linkedin.com/in/kwangchunlee/\n    affiliation: 한국 R 사용자회\n    affiliation-url: https://github.com/bit2r\ntitle-block-banner: true\n#title-block-banner: \"#562457\"\nformat:\n  html:\n    css: css/quarto.css\n    theme: flatly\n    code-fold: true\n    code-overflow: wrap\n    toc: true\n    toc-depth: 3\n    toc-title: 목차\n    number-sections: true\n    highlight-style: github    \n    self-contained: false\nfilters:\n   - lightbox\nlightbox: auto\nlink-citations: true\nknitr:\n  opts_chunk: \n    message: false\n    warning: false\n    collapse: true\n    comment: \"#>\" \n    R.options:\n      knitr.graphics.auto_pdf: true\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n\n# 음성인식 데이터셋\n\n[OpenSLR](https://www.openslr.org/index.html)은 음성 인식을 위한 학습용 말뭉치, 음성 인식 관련 소프트웨어 등 음성 및 언어 자원을 제공하고 있다.\n\n## 오디오 표본\n\n[대규모(1000시간) 영어 음성 읽기 말뭉치](https://www.openslr.org/12)에서 영어 음성 읽기 하나를 추출해서 관련 사항을 정답문과 비교해보자.\n\n\n:::::::{.column-body-outset}\n\n:::::{.columns}\n\n:::{.column}\n\n### 영어 음성 대본 {.unnumbered}\n\n[대규모(1000시간) 영어 음성 읽기 말뭉치](https://www.openslr.org/12)에 포함된 영어음성대본에서 텍스트를 추출해서 다음에 영어 음성에서 텍스트 추출을 비교한다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n# fs::dir_ls(path=\"data/LibriSpeech/dev-clean/1993/147149\")\n\ntrans_txt <- read_lines(\"data/LibriSpeech/dev-clean/1993/147149/1993-147149.trans.txt\") \n\n# trans_0011_str <- trans_txt[str_detect(trans_txt, \"0011\")]\n# trans_str <- str_extract_all(trans_0011_str, \"[a-zA-Z].+\")[[1]]\n\ntrans_0011 <- trans_txt %>% \n  enframe() %>% \n  separate(value, into = c(\"순번\", \"텍스트\"), sep = \"\\\\s\", extra = \"merge\") %>% \n  mutate(텍스트 = str_to_lower(텍스트)) %>% \n  filter(str_detect(순번, \"0011\")) %>% \n  pull(텍스트)\n```\n:::\n\n\nthen the mother lifted up her voice and wept\n\n:::\n\n:::{.column}\n\n### 영어 음성 {.unnumbered}\n\n영어 음성을 들어보자. `.flac` 파일을 `av` 패키지 `av_audio_convert()` 함수로 \n`.mp3` 혹은 `.wav` 파일 변환이 가능하다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(av)\nlibrary(embedr)\n\naudio_file <- \"data/LibriSpeech/dev-clean/1993/147149/1993-147149-0011.flac\"\n\nav::av_audio_convert(audio_file, output = \"data/whisper_before.mp3\", \n                     format = \"mp3\", sample_rate = 16000)\n#> [1] \"D:\\\\tcs\\\\chatGPT\\\\data\\\\whisper_before.mp3\"\n\nwhisper_before_mp3 <- av::read_audio_bin(\"data/whisper_before.mp3\")\n\nembedr::embed_audio(\"data/whisper_before.mp3\")\n```\n\n::: {.cell-output-display}\n```{=html}\n<audio controls> <source src='data/whisper_before.mp3' type='audio/mpeg'> Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp </audio>\n```\n:::\n:::\n\n\n:::\n\n:::::\n\n:::::::\n\n# `whisper` STT\n\n영어 음성에서 텍스트로 전환하는 작업을 STT(Speech-to-Text)라고 부르는데 \n최근 성능도 많이 좋아졌고 그중 챗GPT 인기를 얻고 있는 `whisper` API를 사용해서 \n영어 음성을 텍스트로 변환할 수도 있고, C/C++로 OpenAI의 Whisper 모델 이식한 \n[whisper.cpp](https://github.com/ggerganov/whisper.cpp) 모델을 사용하면 CPU로 \n무료로 사용가능하다.\n\n\n[`audio.whisper`](https://github.com/bnosac/audio.whisper) 패키지가 최근에 출시되어 \n이를 사용하면 수월히 R에서도 STT 작업을 수행할 수 있다.\n\n| 모형                   | 언어                        |  크기  | 필요 RAM 크기 |\n|:-----------------------|:---------------------------:|-------:|-----------:|\n| `tiny` & `tiny.en`     | Multilingual & English only | 75 MB  | 390 MB     |\n| `base` & `base.en`     | Multilingual & English only | 142 MB | 500 MB     |\n| `small` & `small.en`   | Multilingual & English only | 466 MB | 1.0 GB     |\n| `medium` & `medium.en` | Multilingual & English only | 1.5 GB | 2.6 GB     |\n| `large-v1` & `large`   | Multilingual                | 2.9 GB | 4.7 GB     |\n\n`whisper()` 입력 오디오는 16비트 `.wav` 파일형식만 가능하다. \n따라서 `av` 패키지 `av_audio_convert()` 함수로 원본 파일(`.flac`)을 `.wav` 파일로 변환한 후에 \nSTT 작업을 수행한다.\n\n:::::::{.column-body-outset}\n\n:::::{.columns}\n:::{.column}\n\n### `.flac` &rarr; `.wav` {.unnumbered}\n\n`.flac` 파일을 `.wav` 파일로 변환시킨다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(audio.whisper)\n\nav::av_audio_convert(audio_file, output = \"data/whisper_before.wav\", \n                     format = \"wav\", sample_rate = 16000)\n#> [1] \"D:\\\\tcs\\\\chatGPT\\\\data\\\\whisper_before.wav\"\n```\n:::\n\n\n:::\n\n:::{.column}\n\n### STT 결과 {.unnumbered}\n\n`base.en` 모델을 사용해서 STT로 영어음성에서 텍스트를 추출한다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- whisper(\"base.en\")\n#> whisper_init_from_file: loading model from 'D:/tcs/chatGPT/ggml-base.en.bin'\n#> whisper_model_load: loading model\n#> whisper_model_load: n_vocab       = 51864\n#> whisper_model_load: n_audio_ctx   = 1500\n#> whisper_model_load: n_audio_state = 512\n#> whisper_model_load: n_audio_head  = 8\n#> whisper_model_load: n_audio_layer = 6\n#> whisper_model_load: n_text_ctx    = 448\n#> whisper_model_load: n_text_state  = 512\n#> whisper_model_load: n_text_head   = 8\n#> whisper_model_load: n_text_layer  = 6\n#> whisper_model_load: n_mels        = 80\n#> whisper_model_load: f16           = 1\n#> whisper_model_load: type          = 2\n#> whisper_model_load: mem required  =  215.00 MB (+    6.00 MB per decoder)\n#> whisper_model_load: kv self size  =    5.25 MB\n#> whisper_model_load: kv cross size =   17.58 MB\n#> whisper_model_load: adding 1607 extra tokens\n#> whisper_model_load: model ctx     =  140.60 MB\n#> whisper_model_load: model size    =  140.54 MB\n\ntrans <- predict(model, newdata = \"data/whisper_before.wav\", language = \"en\", n_threads = 2)\n#> Processing data/whisper_before.wav (49920 samples, 3.12 sec), lang = en, translate = 0, timestamps = 0\n#> \n#> [00:00:00.000 --> 00:00:03.000]   Then the mother lifted up her voice and wept.\n\ntrans$data\n#>   segment         from           to\n#> 1       1 00:00:00.000 00:00:03.000\n#>                                             text\n#> 1  Then the mother lifted up her voice and wept.\n```\n:::\n\n\n1, 00:00:00.000, 00:00:03.000,  Then the mother lifted up her voice and wept.\n\n:::\n\n:::::\n\n:::::::\n\n# 동영상에서 오디오 추출\n\n`ffmpeg` 프로그램을 사용하면 오디오를 추출할 수 있다.\nMP4 파일에서 16비트 깊이와 16kHz 샘플링 레이트로 오디오를 추출하여야 `whisper`에 입력값으로 넣을 수 있는 `.wav` 파일이 된다.\n\n```bash\nffmpeg -i misinformation.mp4 -acodec pcm_s16le -ar 16000 -ac 2 misinformation_16.wav\n```\n\n`.mp3` 확장자를 줄 경우 오디오를 `mp3` 파일로 추출하여 파일크기를 크게 줄일 수 있다.\n\n```bash\nffmpeg -i misinformation.mp4 -acodec pcm_s16le -ar 16000 -ac 2 misinformation_16.mp3\n```\n\n# STT\n\n`2.6 GB` 영어 `medium.en` 모델은 알 수 없는 오류로 인해  `base.en` 모델을 사용하여 영어 음성에서 텍스트를 추출한다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(audio.whisper)\n\nmedium_model <- whisper(\"base.en\")\n\nmisinformation_trans <- predict(medium_model, newdata = \"data/LibriSpeech/misinformation_16.wav\", \n                                language = \"en\", n_threads = 2)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nmisinformation_trans$data %>% \n  mutate(data = glue::glue(\"{from} ==> {to} {text}\")) %>% \n  pull(data) %>% \n  write_lines(\"data/LibriSpeech/misinformation_oneline.txt\")\n```\n:::\n\n\n# SRT\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmis_srt_raw <- read_lines(\"data/LibriSpeech/misinformation_proof_reading.txt\")\n\nmis_srt_tbl <- mis_srt_raw %>% \n  enframe() %>% \n  separate(value, into = c(\"start\", \"end\"),   sep = \"\\\\s==>\\\\s\", extra = \"merge\") %>% \n  separate(end,   into = c(\"end\", \"subtitle\"), sep = \"\\\\s\", extra = \"merge\") %>% \n  mutate(subtitle = str_trim(subtitle))\n \nmis_srt_tbl %>% \n  mutate(srt = glue::glue(\"{name} \\n {start} --> {end} \\n {subtitle}\\n\\n\")) %>% \n  pull(srt) %>% \n  write_lines(\"data/LibriSpeech/misinformation_proof_reading_srt.srt\")\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}