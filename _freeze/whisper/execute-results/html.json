{
  "hash": "3eb66d92000c6d95698dbd68278f92c0",
  "result": {
    "markdown": "---\ntitle: \"chatGPT\"\nsubtitle: \"음성인식(Whisper)\"\nauthor:\n  - name: 이광춘\n    url: https://www.linkedin.com/in/kwangchunlee/\n    affiliation: 한국 R 사용자회\n    affiliation-url: https://github.com/bit2r\ntitle-block-banner: true\n#title-block-banner: \"#562457\"\nformat:\n  html:\n    css: css/quarto.css\n    theme: flatly\n    code-fold: true\n    code-overflow: wrap\n    toc: true\n    toc-depth: 3\n    toc-title: 목차\n    number-sections: true\n    highlight-style: github    \n    self-contained: false\nfilters:\n   - lightbox\nlightbox: auto\nlink-citations: true\nknitr:\n  opts_chunk: \n    message: false\n    warning: false\n    collapse: true\n    comment: \"#>\" \n    R.options:\n      knitr.graphics.auto_pdf: true\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n\n# 음성인식 데이터셋\n\n[OpenSLR](https://www.openslr.org/index.html)은 음성 인식을 위한 학습용 말뭉치, 음성 인식 관련 소프트웨어 등 음성 및 언어 자원을 제공하고 있다.\n\n## 오디오 표본\n\n[대규모(1000시간) 영어 음성 읽기 말뭉치](https://www.openslr.org/12)에서 영어 음성 읽기 하나를 추출해서 관련 사항을 정답문과 비교해보자.\n\n\n:::::::{.column-body-outset}\n\n:::::{.columns}\n\n:::{.column}\n\n### 영어 음성 대본 {.unnumbered}\n\n[대규모(1000시간) 영어 음성 읽기 말뭉치](https://www.openslr.org/12)에 포함된 영어음성대본에서 텍스트를 추출해서 다음에 영어 음성에서 텍스트 추출을 비교한다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n# fs::dir_ls(path=\"data/LibriSpeech/dev-clean/1993/147149\")\n\ntrans_txt <- read_lines(\"data/LibriSpeech/dev-clean/1993/147149/1993-147149.trans.txt\") \n\n# trans_0011_str <- trans_txt[str_detect(trans_txt, \"0011\")]\n# trans_str <- str_extract_all(trans_0011_str, \"[a-zA-Z].+\")[[1]]\n\ntrans_0011 <- trans_txt %>% \n  enframe() %>% \n  separate(value, into = c(\"순번\", \"텍스트\"), sep = \"\\\\s\", extra = \"merge\") %>% \n  mutate(텍스트 = str_to_lower(텍스트)) %>% \n  filter(str_detect(순번, \"0011\")) %>% \n  pull(텍스트)\n```\n:::\n\n\nthen the mother lifted up her voice and wept\n\n:::\n\n:::{.column}\n\n### 영어 음성 {.unnumbered}\n\n영어 음성을 들어보자. `.flac` 파일을 `av` 패키지 `av_audio_convert()` 함수로 \n`.mp3` 혹은 `.wav` 파일 변환이 가능하다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(av)\nlibrary(embedr)\n\naudio_file <- \"data/LibriSpeech/dev-clean/1993/147149/1993-147149-0011.flac\"\n\nav::av_audio_convert(audio_file, output = \"data/whisper_before.mp3\", \n                     format = \"mp3\", sample_rate = 16000)\n#> [1] \"D:\\\\tcs\\\\chatGPT\\\\data\\\\whisper_before.mp3\"\n\nwhisper_before_mp3 <- av::read_audio_bin(\"data/whisper_before.mp3\")\n\nembedr::embed_audio(\"data/whisper_before.mp3\")\n```\n\n::: {.cell-output-display}\n```{=html}\n<audio controls> <source src='data/whisper_before.mp3' type='audio/mpeg'> Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp </audio>\n```\n:::\n:::\n\n\n:::\n\n:::::\n\n:::::::\n\n# `whisper` STT\n\n영어 음성에서 텍스트로 전환하는 작업을 STT(Speech-to-Text)라고 부르는데 \n최근 성능도 많이 좋아졌고 그중 챗GPT 인기를 얻고 있는 `whisper` API를 사용해서 \n영어 음성을 텍스트로 변환할 수도 있고, C/C++로 OpenAI의 Whisper 모델 이식한 \n[whisper.cpp](https://github.com/ggerganov/whisper.cpp) 모델을 사용하면 CPU로 \n무료로 사용가능하다.\n\n\n[`audio.whisper`](https://github.com/bnosac/audio.whisper) 패키지가 최근에 출시되어 \n이를 사용하면 수월히 R에서도 STT 작업을 수행할 수 있다.\n\n| 모형                   | 언어                        |  크기  | 필요 RAM 크기 |\n|:-----------------------|:---------------------------:|-------:|-----------:|\n| `tiny` & `tiny.en`     | Multilingual & English only | 75 MB  | 390 MB     |\n| `base` & `base.en`     | Multilingual & English only | 142 MB | 500 MB     |\n| `small` & `small.en`   | Multilingual & English only | 466 MB | 1.0 GB     |\n| `medium` & `medium.en` | Multilingual & English only | 1.5 GB | 2.6 GB     |\n| `large-v1` & `large`   | Multilingual                | 2.9 GB | 4.7 GB     |\n\n`whisper()` 입력 오디오는 16비트 `.wav` 파일형식만 가능하다. \n따라서 `av` 패키지 `av_audio_convert()` 함수로 원본 파일(`.flac`)을 `.wav` 파일로 변환한 후에 \nSTT 작업을 수행한다.\n\n:::::::{.column-body-outset}\n\n:::::{.columns}\n:::{.column}\n\n### `.flac` &rarr; `.wav` {.unnumbered}\n\n`.flac` 파일을 `.wav` 파일로 변환시킨다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(audio.whisper)\n\nav::av_audio_convert(audio_file, output = \"data/whisper_before.wav\", \n                     format = \"wav\", sample_rate = 16000)\n#> [1] \"D:\\\\tcs\\\\chatGPT\\\\data\\\\whisper_before.wav\"\n```\n:::\n\n\n:::\n\n:::{.column}\n\n### STT 결과 {.unnumbered}\n\n`base.en` 모델을 사용해서 STT로 영어음성에서 텍스트를 추출한다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- whisper(\"base.en\")\n#> whisper_init_from_file: loading model from 'D:/tcs/chatGPT/ggml-base.en.bin'\n#> whisper_model_load: loading model\n#> whisper_model_load: n_vocab       = 51864\n#> whisper_model_load: n_audio_ctx   = 1500\n#> whisper_model_load: n_audio_state = 512\n#> whisper_model_load: n_audio_head  = 8\n#> whisper_model_load: n_audio_layer = 6\n#> whisper_model_load: n_text_ctx    = 448\n#> whisper_model_load: n_text_state  = 512\n#> whisper_model_load: n_text_head   = 8\n#> whisper_model_load: n_text_layer  = 6\n#> whisper_model_load: n_mels        = 80\n#> whisper_model_load: f16           = 1\n#> whisper_model_load: type          = 2\n#> whisper_model_load: mem required  =  215.00 MB (+    6.00 MB per decoder)\n#> whisper_model_load: kv self size  =    5.25 MB\n#> whisper_model_load: kv cross size =   17.58 MB\n#> whisper_model_load: adding 1607 extra tokens\n#> whisper_model_load: model ctx     =  140.60 MB\n#> whisper_model_load: model size    =  140.54 MB\n\ntrans <- predict(model, newdata = \"data/whisper_before.wav\", language = \"en\", n_threads = 2)\n#> Processing data/whisper_before.wav (49920 samples, 3.12 sec), lang = en, translate = 0, timestamps = 0\n#> \n#> [00:00:00.000 --> 00:00:03.000]   Then the mother lifted up her voice and wept.\n\ntrans$data\n#>   segment         from           to\n#> 1       1 00:00:00.000 00:00:03.000\n#>                                             text\n#> 1  Then the mother lifted up her voice and wept.\n```\n:::\n\n\n1, 00:00:00.000, 00:00:03.000,  Then the mother lifted up her voice and wept.\n\n:::\n\n:::::\n\n:::::::\n\n# 챗GPT와 오정보\n\n[[한국 R 컨퍼런스 - Julia Silge Keynote번역 (2021-11-17)](https://statkclee.github.io/deep-learning/rconf-keynote.html)]{.aside}\n\n2023년 서울 R 미트업에서 \"챗GPT와 오정보(ChatGPT and Misinformation)\"를 주제로\n워싱턴 대학 제빈 웨스트 교수님을 모시고 특강을 진행했다. \n한정된 예산으로 직접 모시지는 못하고 줌(Zoom) 녹화로 대신했다.\n\n약 1시간 분량의 녹화분량 중 일부 불필요한 부분은 [오픈샷 비디오 편집기(OpenShot Video Editor)](https://ko.wikipedia.org/wiki/%EC%98%A4%ED%94%88%EC%83%B7)를 가지고 잘라낸다.\n\n결국 유튜브 동영상으로 올려 한글 자막을 입히는 것이 최종 목적이기 때문에 \n`.mp4` 파일에서 이미지 정보 대신 오디오 정보만 `.wav`, `.mp3` 파일로 추출해서 뽑아낸다. \n\nSTT(Speech-to-Text) 영어 음성을 영어 텍스트로 전사(transcribe)해야 하는 작업이 \n필요하기 때문에 다양한 모델이 있지만 성능이 좋다고 인정받는 OpenAI Whisper 를 \n사용한다. \n\n[OpenAI Whisper API](https://platform.openai.com/docs/models/whisper)는 OpenAI의 API를 통해 사용할 수 있는 음성-텍스트 변환 모델이며 Whisper의 [오픈 소스 버전은 Github](https://github.com/openai/whisper)에서 사용할 수 있다. 오픈 소스 버전의 Whisper와 OpenAI의 API를 통해 제공되는 버전은 차이가 없지만, OpenAI의 Whisper API를 사용할 경우, 시스템 전반에 걸친 일련의 최적화를 통해 OpenAI는 12월부터 ChatGPT의 비용을 90% 저렴하게 이용할 수 있으며,개발자는 API에서 오픈 소스 Whisper 대형-v2 모델을 사용하여 훨씬 빠르고 비용 효율적인 결과를 얻을 수 있다.\n\n\n![](images/whisper_srt.jpg)\n\n## 동영상에서 오디오 추출\n\n`ffmpeg` 프로그램을 사용하면 오디오를 추출할 수 있다.\nMP4 파일에서 16비트 깊이와 16kHz 샘플링 레이트로 오디오를 추출하여야 `whisper`에 입력값으로 넣을 수 있는 `.wav` 파일이 된다. 두가지 조건(16비트 깊이와 16kHz 샘플링)이 충족되지 않을 경우 Whisper에서 처리할 수 없다는 오류가 나온다.\n\n:::::{.columns}\n:::{.column}\n\n### `.mp4`에서 `.wav` 추출  {.unnumbered}\n\n```bash\nffmpeg -i misinformation.mp4 -acodec pcm_s16le -ar 16000 -ac 2 misinformation_16.wav\n```\n\n:::\n:::{.column}\n\n### `.mp4`에서 `.mp3` 추출 {.unnumbered}\n\n`.mp3` 확장자를 줄 경우 오디오를 `mp3` 파일로 추출하여 파일크기를 크게 줄일 수 있다.\n\n```bash\nffmpeg -i misinformation.mp4 -acodec pcm_s16le -ar 16000 -ac 2 misinformation.mp3\n```\n\n:::\n\n:::::\n\n전체 `.mp3` 파일이 너무 길어 `ffmpeg`를 사용해서 30분부터 30초만 추출하는 코드를 작성해서 들어보자.\n\n```bash\nffmpeg -i misinformation.mp3 -ss 00:30:30 -t 00:00:30 -vn -codec:a libmp3lame -qscale:a 2 misinformation_short.mp3\n```\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nembedr::embed_audio(\"data/LibriSpeech/misinformation_short.mp3\")\n```\n\n::: {.cell-output-display}\n```{=html}\n<audio controls> <source src='data/LibriSpeech/misinformation_short.mp3' type='audio/mpeg'> Your browser does not support the audio tag;  for browser support, please see:  https://www.w3schools.com/tags/tag_audio.asp </audio>\n```\n:::\n:::\n\n\n\n\n## STT\n\n16비트 깊이와 16kHz 샘플링 조건을 갖춘 `.wav` 파일이 준비되면 다음 단계로 \n음성-텍스트 변환 Whisper 모델을 선정하여 텍스트 전사 작업을 수행한다.\n윈도우 10 에서 `2.6 GB` 영어 `medium.en` 모델은 알 수 없는 오류로 인해 `base.en` 모델을 사용하여 영어 음성에서 텍스트를 추출했다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(audio.whisper)\n\nmedium_model <- whisper(\"base.en\")\n\nmisinformation_trans <- predict(medium_model, newdata = \"data/LibriSpeech/misinformation_16.wav\", \n                                language = \"en\", n_threads = 2)\n```\n:::\n\n\nWhisper 모델을 통해 나온 음성-텍스트 변환 결과를 로컬 파일에 저장하여 점검한다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nmisinformation_trans$data %>% \n  mutate(data = glue::glue(\"{from} ==> {to} {text}\")) %>% \n  pull(data) %>% \n  write_lines(\"data/LibriSpeech/misinformation_oneline.txt\")\n```\n:::\n\n\n## SRT\n\n몇번의 시행착오를 거쳐 순번, 시작시각, 종료시각, 텍스트로 구성된 파일을 `.srt` 파일 형태로 \n변환하여 영어 자막작업을 마무리한다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmis_srt_raw <- read_lines(\"data/LibriSpeech/misinformation_proof_reading.txt\")\n\nmis_srt_tbl <- mis_srt_raw %>% \n  enframe() %>% \n  separate(value, into = c(\"start\", \"end\"),   sep = \"\\\\s==>\\\\s\", extra = \"merge\") %>% \n  separate(end,   into = c(\"end\", \"subtitle\"), sep = \"\\\\s\", extra = \"merge\") %>% \n  mutate(subtitle = str_trim(subtitle))\n \nmis_srt_tbl %>% \n  mutate(srt = glue::glue(\"{name} \\n {start} --> {end} \\n {subtitle}\\n\\n\")) %>% \n  pull(srt) %>% \n  write_lines(\"data/LibriSpeech/misinformation_proof_reading_srt.srt\")\n```\n:::\n\n\n유튜브 동영상 영문 자막으로 사용될 `.srt` 자막 파일을 불러읽어와서 최종 작업결과를 살펴본다.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nmis_srt <- read_lines(\"data/LibriSpeech/misinformation.srt\")\n\nmis_srt %>% \n  head(20) \n#>  [1] \"1 \"                                                                                          \n#>  [2] \"00:00:00.000 --> 00:00:27.520 \"                                                              \n#>  [3] \"Welcome everybody to our Seoul R Online seminar where we explore various ideas and\"          \n#>  [4] \"\"                                                                                            \n#>  [5] \"2 \"                                                                                          \n#>  [6] \"00:00:27.520 --> 00:00:33.360 \"                                                              \n#>  [7] \"practices of data science. We are really happy today to have Jevin West from the University\" \n#>  [8] \"\"                                                                                            \n#>  [9] \"3 \"                                                                                          \n#> [10] \"00:00:33.360 --> 00:00:43.040 \"                                                              \n#> [11] \"of Washington with us to talk about ChatGPT and misinformation. I'm sorry, I'll begin again.\"\n#> [12] \"\"                                                                                            \n#> [13] \"4 \"                                                                                          \n#> [14] \"00:00:43.040 --> 00:00:46.480 \"                                                              \n#> [15] \"Okay, no problem. No, I do this a lot of times.\"                                             \n#> [16] \"\"                                                                                            \n#> [17] \"5 \"                                                                                          \n#> [18] \"00:00:46.480 --> 00:00:53.680 \"                                                              \n#> [19] \"Welcome everybody to our Seoul R Online seminar. We explore various ideas and\"               \n#> [20] \"\"\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(magick)\n\nmis_mp4 <- image_read_video(\"data/LibriSpeech/misinformation.mp4\",  fps = 1)\n```\n:::\n\n\n![](images/misinformation.png)\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}