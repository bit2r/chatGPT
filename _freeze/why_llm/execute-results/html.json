{
  "hash": "36247acd424e9ffd027484a661c08e42",
  "result": {
    "markdown": "---\ntitle: \"chatGPT\"\nsubtitle: \"왜 거대언어모형인가?\"\nauthor:\n  - name: 이광춘\n    url: https://www.linkedin.com/in/kwangchunlee/\n    affiliation: 한국 R 사용자회\n    affiliation-url: https://github.com/bit2r\ntitle-block-banner: true\n#title-block-banner: \"#562457\"\nformat:\n  html:\n    css: css/quarto.css\n    theme: flatly\n    code-fold: false\n    toc: true\n    toc-depth: 3\n    toc-title: 목차\n    number-sections: true\n    highlight-style: github    \n    self-contained: false\nfilters:\n   - lightbox\nlightbox: auto\nknitr:\n  opts_chunk: \n    message: false\n    warning: false\n    collapse: true\n    comment: \"#>\" \n    R.options:\n      knitr.graphics.auto_pdf: true\neditor_options: \n  chunk_output_type: console\nbibliography: bibliography.bib\nlink-citations: yes\ncsl: apa-single-spaced.csl\n---\n\n\n\n![](images/tech_giant.png)\n\n# 들어가며\n\n**GPT-3(Generative Pre-trained Transformer 3)**와 같은 거대 언어 모델은 인간과 유사한 언어를 처리하고 생성할 수 있기 때문에 이 시대에 중요한 역할을 한다. 이를 통해 자연어 처리, 챗봇, 언어 번역, 콘텐츠 제작, 코딩 등 다양한 애플리케이션에 수많은 가능성을 열었다는 평가를 받고 있다.\n\n거대 언어 모델이 중요한 몇 가지 이유를 꼽으면 다음과 같다.\n\n- **자연어 처리(Natural language processing)**: GPT-3와 같은 거대 언어 모델은 대량의 텍스트 데이터를 처리할 수 있고 언어의 문맥과 의미를 이해할 수 있어 감정 분석, 언어 번역, 텍스트 분류와 같은 자연어 처리 작업을 수행할 수 있다.\n\n- **챗봇(Chatbot)**: 거대 언어 모델을 사용하여 자연어 쿼리를 이해하고 응답할 수 있는 대화형 대리인(챗봇)를 만들 수 있다. 이러한 챗봇은 고객 지원, 가상 비서 및 기타 다양한 애플리케이션에서 사용할 수 있다.\n\n- **언어 번역(Language translation)**: 거대 언어 모델은 여러 언어에 대해 학습할 수 있으며 고품질 언어 번역을 수행한다. 이는 관광, 전자상거래, 국제 무역 등 다양한 산업에서 유용하게 사용될 수 있다.\n\n- **콘텐츠 생성(Content creation)**: 거대 언어 모델은 기사, 요약, 시 등 사람과 유사한 텍스트 콘텐츠를 생성할 수 있다. 이는 저널리즘, 콘텐츠 제작, 광고 등 다양한 산업에서 활용할 수 있다.\n\n- **코딩(Coding)**: GPT-3는 소프트웨어 개발 및 자동화에 광범위한 영향을 미칠 수 있는 컴퓨터 코드를 생성할 수 있는 능력을 입증했다.\n\n요약하면, 거대 언어 모델은 우리가 기계와 상호작용하고 작업을 수행하는 방식을 혁신할 수 있는 잠재력을 가지고 있어 우리 시대에 중요한 기술이 될 것임은 자명하다.\n\n\n:::{.panel-tabset}\n\n## 2019년 \n\n![[@sanh2019distilbert]](images/LLM_2019.png)\n\n## 2021년\n\n![[Efficient Natural Language Processing](https://hanlab.mit.edu/projects/efficientnlp_old/)](images/LLM_2021.png)\n\n## 2022년 \n\n![[langcon 2023 by 신정규](https://songys.github.io/2023Langcon/)(images/LLM_langcon2023.png)\n\n\n:::\n\n\n\n\n# 모형크기\n\nquestion-answering tasks (open-domain closed-book variant), cloze and sentence-completion tasks, Winograd-style tasks, in-context reading comprehension tasks, common-sense reasoning tasks, SuperGLUE tasks, and natural language inference tasks가 포함된 총 29개 작업 중 28개 영역에서 PaLM 540B가 이전 거대 언어모형 GLaM, GPT-3, Megatron-Turing NLG, Gopher, Chinchilla, LaMDA 을 가볍게 능가했다.\n\n[[Sharan Narang and Aakanksha Chowdhery (APRIL 04, 2022), \"Pathways Language Model (PaLM): Scaling to 540 Billion Parameters for Breakthrough Performance\", Software Engineers, Google Research](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)]{.aside}\n\n:::{.panel-tabset}\n\n## LLM 진화\n\n![](images/LLM_tree.gif)\n\n## 80억 패러미터\n\n![](images/LLM_tree_8_billion.png)\n\n## 400억 \n\n![](images/LLM_tree_40_billion.png)\n\n## 640억 \n\n![](images/LLM_tree_62_billion.png)\n\n\n## 5,400억 \n\n![](images/LLM_tree_540_billion.png)\n\n## 성능\n\n![](images/LLM_tree_performance.png)\n\n:::\n\n# 거대언어모형 성능\n\n자연어 처리(NLP) 및 머신 러닝 분야의 여러 발전으로 인해 GPT-3와 같은 대규모 언어 모델의 성능이 이전 모델보다 향상되었다. 주요 원인으로 다음을 꼽을 수 있다.\n\n- **규모(Scale)**: 대규모 언어 모델은 방대한 양의 텍스트 데이터로 학습되어 언어의 더 많은 뉘앙스를 포착하고 문맥을 더 잘 이해할 수 있다. GPT-3는 45테라바이트가 넘는 텍스트 데이터셋으로 학습되었다고 알려져 지금까지 사전 학습된 언어 모델 중 가장 큰 규모를 갖고 있다.\n\n- **아키텍처(Architecture)**: GPT-3는 병렬 처리가 가능한 트랜스포머(Transformer) 기반 아키텍처를 사용하여 학습 시간을 단축하고 성능을 향상시켰다.\n\n- **사전 학습(Pre-training)**: 대규모 언어 모델은 방대한 양의 텍스트 데이터로 사전 학습되어 다양한 작업에 적용할 수 있는 일반적인 언어 패턴과 관계를 학습할 수 있다. GPT-3는 비지도 학습을 사용하여 사전 학습되므로 특정 작업을 염두에 두지 않고 원시 텍스트 데이터에서 학습했다.\n\n- **미세 조정(Fine-tuning)**: 언어 번역이나 텍스트 분류와 같은 특정 작업을 위해 대규모 언어 모델을 미세 조정(Fine-tuning) 작업을 수행한다. 이 과정에는 해당 작업에 특화된 소규모 데이터셋로 모델을 추가 학습시켜 성능을 더욱 향상시킨다.\n\n- **전이 학습(Transfer learning)**: 대규모 언어 모델은 한 작업에서 학습한 지식을 다른 작업으로 전이시킬 수 있다. 즉, 언어 번역과 같은 한 작업에서 학습된 모델을 더 작은 데이터셋을 사용하여 감정 분석과 같은 다른 작업에 맞게 추가 학습작업(Fine-tuning)을 시킬 수 있다.\n\n요약하면, 대규모 언어 모델의 성능은 규모, 아키텍처, 사전 학습, 미세 조정 및 전이 학습의 발전으로 인해 이전 모델보다 더 우수하다. 이러한 발전 덕분에 대규모 언어 모델은 다양한 언어 작업에서 최첨단 성능을 달성할 수 있게 되어 자연어 처리 및 머신 러닝 분야에서 강력한 도구가 된 것이다.\n\n\n![[@wei2022emergent]](images/LLM_few_shot.png)\n\n\n# 수학\n\n[@lewkowycz2022solving]{.aside}\n\n:::{.panel-tabset}\n\n## 미네르바 LM\n\n![](images/math_minerva.png)\n\n## 손으로 풀기\n\n![](images/LLM_hand.jpg){width=50%}\n\n\n## 시각화\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\ngiven_line <- function(x)  10 + 4 * x\nsolve_line <- function(x) -10 + 4 *x\n\nggplot() +\n  geom_point(aes(x = 5, y = 10), size = 3) +\n  geom_function(fun = given_line, color = \"blue\", size = 1.5) +\n  geom_function(fun = solve_line, color = \"red\", size = 1.5, alpha = 0.5) +\n  theme_classic() +\n  scale_x_continuous(limits = c(-7, 7), breaks = seq(-7, 7, 1)) +\n  scale_y_continuous(limits = c(-20, 20), breaks = seq(-20, 20, 1)) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) \n```\n\n::: {.cell-output-display}\n![](why_llm_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n## Sympy 해법\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sympy import *\n\nx, y, b = symbols('x y b')\n\ngiven_eq = y - (4*x + 10)\n\nparallel_eq = y - (4*x + b)\n\nintercept_eq = parallel_eq.subs([(x, 5), (y, 10)])\n\nsolveset(Eq(intercept_eq, 0), b)\n#> {-10}\n```\n:::\n\n\n:::\n\n# 다양한 사례 (PaLM)\n\n5,400 억 패러미터를 장착한 Pathways Language Model (PaLM)의 성능을 실감해보자.\n\n:::{.panel-tabset}\n\n## 다양한 기능\n\n![](images/PaLM_overview.gif)\n\n## 추론\n\n추론(Reasoning)\n\n![](images/PaLM_reasoning.png)\n\n\n## 코딩\n\n코딩(Code Generation)\n\n![](images/PaLM_coding.gif)\n\n:::\n\n\n# 개발비\n\n[[Estimating 🌴PaLM's training cost](https://blog.heim.xyz/palm-training-cost/)]{.aside}\n\n언어 모형 개발은 2010년 이후 개발비용이 급격히 증가하고 있으며 그 추세는 상상을 초월한다.\n\n:::{.panel-tabset}\n\n## Our World in Data\n\n<iframe src=\"https://ourworldindata.org/grapher/artificial-intelligence-training-computation?yScale=linear&time=2017-06-12..2022-07-01\" loading=\"lazy\" style=\"width: 100%; height: 600px; border: 0px none;\"></iframe>\n\n## Lennart Heim\n\n<iframe src=\"https://ourworldindata.org/grapher/artificial-intelligence-training-computation?yScale=linear&time=earliest..latest&country=~PaLM+%28540B%29\" loading=\"lazy\" style=\"width: 100%; height: 600px; border: 0px none;\"></iframe>\n\n:::\n\n# 생성모형의 부작용\n\n생성 AI를 통해 인간이 생성한 데이터와 기계가 생성한 데이터가 무작위로 섞인 \n지금까지 경험하지 못한 세상이 출현하고 있다. \n즉, 생성 AI 모형에서 이미지, 텍스트, 동영상 등 무수히 많은 데이터가 인터넷에 공개 및 공유될 것이며\n기계학습 및 딥러닝 생성모형는 결국 실제 데이터와 기계가 생성한 데이터를 입력값으로 \n인공지능 모형을 생성하게 된다.\n하지만 이런 경우 과연 AI 모형은 어떤 특성을 갖게 될 것인가? \n데이터 증강(Data Augmentation)처럼 더 좋은 성능을 갖는 AI 모형이 될 것이가 아니면 그 반대의 모습을 가지게 될 것인가? 논문[@hataya2022will]에서는 부정적인 효과도 있다고 주장하고 있다.\n\n\n::: {.panel-tabset}\n\n## 현재 상황\n\n![](images/training_data_corrupt.png){height=300px, width=150px}\n\n\n\n## 기계오염된 데이터\n\n![기계생성 데이터 사용하여 나온 결과물](images/corrupt_images.png)\n\n\n:::\n\n\n",
    "supporting": [
      "why_llm_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}