---
title: "chatGPT"
subtitle: "BERT(**B**idirectional **E**ncoder **R**epresentation from **T**ransformer)"
author:
  - name: 이광춘
    url: https://www.linkedin.com/in/kwangchunlee/
    affiliation: 한국 R 사용자회
    affiliation-url: https://github.com/bit2r
title-block-banner: true
#title-block-banner: "#562457"
format:
  html:
    css: css/quarto.css
    theme: flatly
    code-fold: false
    code-overflow: wrap
    toc: true
    toc-depth: 3
    toc-title: 목차
    number-sections: true
    highlight-style: pygments 
    self-contained: false
filters:
   - lightbox
   - include-code-files   
lightbox: auto
knitr:
  opts_chunk: 
    message: false
    warning: false
    collapse: true
    comment: "#>" 
    R.options:
      knitr.graphics.auto_pdf: true
editor_options: 
  chunk_output_type: console
bibliography: bibliography.bib
link-citations: yes
csl: apa-single-spaced.csl
---

# Context-Free and Context-based model

- Context-free models: Word2Vec, Glove, FastText
- Context-embedding models(transformer based models): BERT, ELMO, Universal Sentence Encoder

Context-free 모형은 단순하고 효율적이지만 텍스트의 뉴앙스를 비롯하여 의미를 잡아내는데 다소 미흡할 수 있다.
반면에 Context-based model은 강력하고 유연하지만 컴퓨팅 자원을 많이 사용하고 더 복잡하다.


![](images/BERT_workflow.png)

# 질의응답

## 파이썬 코드

BERT 논문 [https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)의 초록(Abstract)에서 
질의를 하고 관련 내용을 뽑아내는 코드를 다음과 같이 작성한다. [@ravichandiran2021getting]

```{.python include="code/BERT/BERT_QnA.py"}
```

BERT 임베딩 모형을 사용해서 질문과 응답을 파이썬 코드로 작성하고 나서 그 결과값을 R에서 바록 읽어 후처리 하도록 한다.

```{r}
#| eval: true
library(reticulate)
library(tidyverse)

reticulate::source_python("code/BERT/BERT_QnA.py")
```

## 질의응답 설정

BERT를 사용해서 질문과 응답을 준비한다.

:::: {.columns}

::: {.column width="30%"}

```{r}
py$question
```
:::

::: {.column width="70%"}

```{r}
py$abstract
```
:::

::::

## 질의응답 결과

```{r}
# str_c(py$tokens[py$start_index$tolist()+1:py$end_index$tolist()+1], collapse = " ")
py$answer
```

# 감성분석

## 파이썬 코드

IMDB 영화평점 텍스트에 담긴 감성분석을 BERT를 사용해서 수행한다.


```{.python include="code/BERT/BERT_sentiment.py"}
```

## 감성분석 결과

```{r}
senti_raw <- read_csv('https://gist.githubusercontent.com/Mukilan-Krishnakumar/e998ecf27d11b84fe6225db11c239bc6/raw/74dbac2b992235e555df9a0a4e4d7271680e7e45/imdb_movie_reviews.csv')

reticulate::source_python("code/BERT/BERT_sentiment.py")

senti_tbl <- senti_raw %>% 
  rename(label = sentiment) %>% 
  bind_cols(py$df %>% select(sentiment))


senti_tbl %>% 
  count(label, sentiment) %>% 
  ggplot(aes(x = sentiment, y = n, fill = label)) +
    geom_col(width = 0.3, alpha = 0.7) +
    scale_fill_manual(values = c("red", "green")) +
    labs(title = "IMDB 영화 평점 데이터셋 감성분석 결과",
         x = "감성점수: 부정(1) --- 긍정(5)",
         y = "영화평점 부여건수",
         fill = "긍부정") +
    theme_minimal() +
    theme(legend.position = "top")
```


## 후속 분석

평점 4점으로 예측된 영화 평점 중 긍부정 3개 리뷰를 뽑아 직접 살펴보자.

```{r}
library(reactable)

senti_tbl %>% 
  filter(sentiment == 4) %>% 
  group_by(label) %>% 
  slice_sample(n = 3) %>% 
  reactable::reactable(
      columns = list(
        text = colDef(width = 700),
        label = colDef(width = 50),
        sentiment = colDef(width = 50)
  ),
  fullWidth = TRUE
  )
```


# BERT 실용화

BERT가 좋은 성능을 보이는 것은 맞지만 너무 크기가 크기 때문에 실무적으로 사용하기에는 
제약이 많다. 몇가지 기술이 개발되어 BERT를 사용하기 좋게 만드는 방법을 알아보자.

## 접근방법

- Quantization and Pruning
- DistilBERT: Knowledge Distillation
- ALBERT: A Lite BERT

[[Samuel Sučík (August 8th, 2019), "Compressing BERT for faster prediction", RASA Blog](https://rasa.com/blog/compressing-bert-for-faster-prediction-2/)]{.aside}

:::{layout-ncol=3}

![Quantization](images/BERT_quantization.png)

![Pruning](images/BERT_pruning.png)

![Pruning](images/BERT_distillation.png)

:::


## 성능비교

DistilBERT, A Lite BERT(ALBERT) 변형된 BERT 모형을 논문에 제시된 NLP 작업별 성능과 크기와 속도를 
BERT-base 모형과 비교해보자. 

:::{.panel-tabset}

### DistilBERT [@sanh2019distilbert]

![](images/BERT_distillBERT.png)

### ALBERT [@lan2019albert]

![](images/BERT_ALBERT.png)


:::




