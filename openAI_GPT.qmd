---
title: "chatGPT"
subtitle: "OpenAI GPT"
description: |
  LangChain은 AI 기능 및 애플리케이션을 보다 쉽게 구축할 수 있도록 하는 데 중점을 둔 인기 있는 오픈 소스 라이브러리로, 특히 GPT 및 기타 언어 모델을 통합하는 데 중점을 두고 있다.
author:
  - name: 이광춘
    url: https://www.linkedin.com/in/kwangchunlee/
    affiliation: 한국 R 사용자회
    affiliation-url: https://github.com/bit2r
title-block-banner: true
#title-block-banner: "#562457"
format:
  html:
    css: css/quarto.css
    theme: flatly
    code-fold: false
    toc: true
    toc-depth: 3
    toc-title: 목차
    number-sections: true
    highlight-style: github    
    self-contained: false
filters:
   - lightbox
lightbox: auto
link-citations: yes
knitr:
  opts_chunk: 
    message: false
    warning: false
    collapse: true
    comment: "#>" 
    R.options:
      knitr.graphics.auto_pdf: true
editor_options: 
  chunk_output_type: console
editor: 
  markdown: 
    wrap: 72
---

# ChatGPT

**ChatGPT**는 간단히 말해 생성형 사전 학습된 트랜스포머(Generative Pre-trained Transformer)의 약자로, OpenAI의 GPT-3 거대 언어 모델 제품군에 기반한 챗봇으로 지도학습과 강화학습기법을 적용하여
미세조정(fine-tuned)된 제품이자 서비스다. OpenAI GPT-3 모형은 크게 세가지가 있다.

- GPT-3
- Codex
- 콘텐츠 필터 모델

## GPT-3

GPT-3은 자연어 처리 및 생성을 담당하는 모델로 인간의 언어 즉,
자연어처럼 보이는 텍스트를 이해할 수 있다.
한걸음 더 들어가면 속도와 성능에 따라 4가지 모델(A, B, C, D)로 구분된다.

- text-davinci-003
- text-curie-001
- text-babbage-001
- text-ada-001

성능기준으로 보면 다음과 같이 정렬할 수 있는데 비용도 그에 따라 높아진다는 의미도 함축한다.

`text-davinci-003` > `text-curie-001` > `text-babbage-001` > `text-ada-001`

따라서, OpenAI는 다빈치 모델(`text-davinci-003`)을 통해 원하는 결과를 얻은 후에 
다른 모델을 사용해 볼 것을 권장하는데 이유는 
훨씬 저렴한 비용으로 많은 수의 유사한 작업을 수행할 수 있기 때문이다.

### `text-ada-001`

2,048개의 토큰 및 2019년 10월까지의 데이터 학습하여 이후 모형과 비교하여
정확도나 성능에서 다소 밀리는 모습이지만 최적화를 통해 매우 빠르고 비용이 가장 저렴하다.

### `text-babbage-001`

2,048개의 토큰과 2019년 10월까지의 데이터 학습되었고 간단한 분류와 의미론적 분류에 효과적이다.

### `text-curie-001`

최대 2048개의 토큰을 지원하며 `text-davinci-003` 다음으로 뛰어난 성능을 보이는 GPT-3 모델이다.
2019년 10월까지의 데이터로 학습되었기 때문에 `text-davinci-003`보다 정확도가 떨어지지만,
번역, 복잡한 분류, 텍스트 분석 및 요약에 좋은 성능을 보이고 있어 
`text-davinci-003`와 비교하여 가성비가 높다고 평가되고 있다.

### `text-davinci-003`

2021년 9월까지의 데이터로 훈련되었기 때문에 최신 정보를 제공하지 못한다는 한계는 있지만, 
앞선 GPT-3 모형과 비교하여 더 높은 품질을 제공한다.
장점 중 하나는 최대 4,000개 토큰까지 요청할 수 있다는 점이 이전 모형과 큰 차별점이 된다.

## 코덱스(Codex)

코덱스는 프로그래밍 코드 이해 및 생성을 위한 것으로 `code-davinci-002`와 `code-cushman-001`가 있다. 또한, 코덱스는 GitHub Copilot을 구동하는 모델이기도 하다.
파이썬, 자바스크립트, 고, 펄, PHP, 루비, 스위프트, 타입스크립트, SQL, 셸 등 12개 이상의 프로그래밍 언어를 지원할 뿐만 아니라 
자연어로 표현된 주석(comment)를 이해하고 사용자를 대신하여 요청된 작업을 수행할 수 있다.

### `code-cushman-001`

복잡한 작업을 수행하는 데 있어서는 `code-davinci-002`가 더 강력하지만,
많은 코드 생성 작업을 수행할 수 있고 `code-davinci-002` 보다 더 빠르고 저렴하다는 장점이 있다.

### `code-davinci-002`

자연어를 코드로 번역하는 데 탁월할 뿐만 아니라 코드를 자동 완성할 뿐만 아니라 보충 요소 삽입도 지원한다. 최대 8,000개의 토큰을 처리할 수 있으며 2021년 6월까지의 데이터로 학습되었다.


## 콘텐츠 필터

민감한 콘텐츠 제거하기 위한 필터 모형이다.
민감하거나 안전하지 않을 수 있는 API 생성 텍스트를 감지할 수 있다. 
사용자가 사용할 AI 응용프로그램을 개발할 경우, 필터를 사용하여 모델이 부적절한 콘텐츠를 반환하는지 감지할 수 있다. 이 필터는 텍스트를 다음 3가지 범주로 나눈다.

- 안전(safe)
- 민감(sensitive)
- 안전하지 않음(unsafe)









